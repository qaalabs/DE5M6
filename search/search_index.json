{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Data Engineering Product Lifecycle","text":""},{"location":"#module-4-planning-a-data-engineering-product","title":"Module 4: Planning a Data Engineering Product","text":"<p>Plan</p> <ul> <li>What problem are we solving and how will we design the solution?</li> <li>Defining business needs, architecting data solutions, and creating comprehensive blueprints for success.</li> </ul>"},{"location":"#module-5-data-engineering-product-development","title":"Module 5: Data Engineering Product Development","text":"<p>Build</p> <ul> <li>How do we bring our design to life?</li> <li>Transforming plans into functional data pipelines, implementing storage solutions, and creating robust processing workflows.</li> </ul>"},{"location":"#module-6-data-operations","title":"Module 6: Data Operations","text":"<p>Monitor</p> <ul> <li>How do we ensure lasting value?</li> <li>Deploying, monitoring, and evolving data products to deliver continuous business impact through changing requirements.</li> </ul>"},{"location":"404/","title":"Page Not Found","text":"<p>Sorry, the page you're looking for does not exist.</p> <ul> <li>Check the URL for typos</li> <li>Go back to the home page</li> </ul>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#links","title":"Links","text":"<ul> <li>Learner reactions ~ Ticks &amp; Crosses</li> </ul>"},{"location":"resources/#qa-platform-labs","title":"QA Platform Labs","text":"<ul> <li>Microsoft Fabric Playground</li> </ul>"},{"location":"trainer/","title":"Trainer Notes","text":""},{"location":"trainer/#day-1-monitoring-performance","title":"Day 1 - Monitoring &amp; Performance","text":""},{"location":"trainer/#session-1","title":"Session 1","text":"<ul> <li>Discussion: System failure impact</li> <li>Why monitor?</li> <li>Investigation: Status page analysis</li> <li>Report-Back: Status page ~ Report back</li> </ul>"},{"location":"trainer/#session-2","title":"Session 2","text":"<ul> <li>Introduction to Microsoft Fabric</li> <li>Practice: Lab 1.1 ~ 01 Create Fabric Lakehouse</li> <li>Discussion: Debrief about the lab</li> <li>Practice: Lab 1.2 ~ 05 Dataflows Gen2</li> </ul>"},{"location":"trainer/#session-3","title":"Session 3","text":"<ul> <li>Practice: Lab 1.3 ~ 18 Monitor Hub</li> <li>Discussion: Debrief about the lab</li> <li>Practice: Lab 1.4 ~ 06c Monitor Warehouse</li> <li>Discussion: Monitoring tools</li> </ul>"},{"location":"trainer/#session-4","title":"Session 4","text":"<ul> <li>Investigation: Cloud monitoring comparison</li> <li>Report-Back: Cloud monitoring ~ Report back</li> <li>Activity: Create monitoring strategy</li> <li>Report-Back: Share monitoring strategies</li> </ul>"},{"location":"trainer/#day-2-incident-response","title":"Day 2 - Incident Response","text":""},{"location":"trainer/#session-1_1","title":"Session 1","text":"<ul> <li>Practice: Lab 2.1 ~ 04 Ingest Pipeline</li> <li>Discussion: When a system broke at work</li> <li>Practice: Lab 04 ~ Break it systematically</li> </ul>"},{"location":"trainer/#session-2_1","title":"Session 2","text":"<ul> <li>Practice: Lab 04 ~ More complex breaks</li> <li>Discussion: Share findings</li> <li>Demo: Lab ~ 11 Data Activator</li> <li>Discussion: AI in Data Engineering</li> </ul>"},{"location":"trainer/#session-3_1","title":"Session 3","text":"<ul> <li>Investigation: Fire Drill preparation</li> <li>Fire drill technical briefing</li> <li>\ud83d\udd25 Fire Drill Round 1</li> <li>Discussion: Fire drill debrief</li> </ul>"},{"location":"trainer/#session-4_1","title":"Session 4","text":"<ul> <li>\ud83d\udd25 Fire Drill Round 2</li> <li>Discussion: Fire drill debrief</li> <li>Discussion: Workplace connections</li> </ul>"},{"location":"trainer/#day-3-quality-governance","title":"Day 3 - Quality &amp; Governance","text":""},{"location":"trainer/#session-1_2","title":"Session 1","text":"<ul> <li>Discussion: What does good data mean to you?</li> <li>Quality Data - why it matters</li> <li>Investigation: DMBOK's 6 data quality dimensions</li> <li>DMBOK report back</li> </ul>"},{"location":"trainer/#session-2_2","title":"Session 2","text":"<ul> <li>Practice: Lab 3.1 ~ 03b Medallion Architecture</li> <li>Discussion: Quality patterns analysis</li> <li>Investigation: Quality tools</li> <li>Quality tools ~ report back</li> </ul>"},{"location":"trainer/#session-3_2","title":"Session 3","text":"<ul> <li>Practice: Lab 3.2 ~ 19 Secure Data Access</li> <li>Discussion: Security vs Accessibility trade-offs</li> <li>Investigation: Governance Approaches Investigation</li> </ul>"},{"location":"trainer/#session-4_2","title":"Session 4","text":"<ul> <li>Practice: Lab 3.3 ~ 21 Deployment Pipelines</li> <li>Discussion: Deployment governance</li> <li>Workshop: Governance integration</li> </ul>"},{"location":"trainer/#day-4-improvement-value","title":"Day 4 - Improvement &amp; Value","text":""},{"location":"trainer/#session-1_3","title":"Session 1","text":"<ul> <li>Instructions &amp; Overview</li> <li>Groups: Planning for Sprint 1</li> <li>Groups: Sprint Block 1</li> </ul>"},{"location":"trainer/#session-2_3","title":"Session 2","text":"<ul> <li>Stand-Up 1</li> <li>Groups: Planning for Sprint 2</li> <li>Groups: Sprint Block 2</li> </ul>"},{"location":"trainer/#session-3_3","title":"Session 3","text":"<ul> <li>Stand-Up 2</li> <li>Groups: Planning for Sprint 3</li> <li>Groups: Sprint Block 3</li> </ul>"},{"location":"trainer/#session-4_3","title":"Session 4","text":"<ul> <li>Sprint Review in groups</li> <li>Team Demos</li> <li>Sprint Retrospective</li> </ul>"},{"location":"agenda/day1/","title":"Day 1 - Monitoring &amp; Performance","text":""},{"location":"agenda/day1/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome: Welcome to Day 1 of DE5 Module 6 (10 mins)</li> <li><code>09:40</code> VM Setup (20 mins)</li> <li><code>10:00</code> Discussion: System failure impact (10 mins)</li> <li><code>10:10</code> Why monitor? (10 mins)</li> <li><code>10:20</code> Investigation: Status page analysis (10 mins)</li> <li><code>10:30</code> Report-Back: Status page ~ Report back (10 mins)</li> </ul>"},{"location":"agenda/day1/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day1/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Introduction to Microsoft Fabric (10 mins)</li> <li><code>11:10</code> Practice: Lab 1.1 ~ 01 Create Fabric Lakehouse (30 mins)</li> <li><code>11:40</code> Discussion: Debrief about the lab (10 mins)</li> <li><code>11:50</code> Practice: Lab 1.2 ~ 05 Dataflows Gen2 (30 mins)</li> </ul>"},{"location":"agenda/day1/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day1/#session-3","title":"Session 3","text":"<ul> <li><code>13:20</code> Practice: Lab 1.3 ~ 18 Monitor Hub (30 mins)</li> <li><code>13:50</code> Discussion: Debrief about the lab (10 mins)</li> <li><code>14:00</code> Practice: Lab 1.4 ~ 06c Monitor Warehouse (20 mins)</li> <li><code>14:20</code> Discussion: Monitoring tools (10 mins)</li> </ul>"},{"location":"agenda/day1/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day1/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> Investigation: Cloud monitoring comparison (20 mins)</li> <li><code>15:10</code> Report-Back: Cloud monitoring ~ Report back (10 mins)</li> <li><code>15:20</code> Activity: Create monitoring strategy (20 mins)</li> <li><code>15:40</code> Report-Back: Share monitoring strategies (10 mins)</li> </ul>"},{"location":"agenda/day1/#wrap-up","title":"Wrap up","text":""},{"location":"agenda/day2/","title":"Day 2 - Incident Response","text":""},{"location":"agenda/day2/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome: Welcome to Day 2 of DE5 Module 6 (10 mins)</li> <li><code>09:40</code> Practice: Lab 2.1 ~ 04 Ingest Pipeline (30 mins)</li> <li><code>10:10</code> Discussion: When a system broke at work (10 mins)</li> <li><code>10:20</code> Practice: Lab 04 ~ Break it systematically (20 mins)</li> </ul>"},{"location":"agenda/day2/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day2/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Practice: Lab 04 ~ More complex breaks (20 mins)</li> <li><code>11:20</code> Discussion: Share findings (10 mins)</li> <li><code>11:30</code> Demo: Lab ~ 11 Data Activator (10 mins)</li> <li><code>11:40</code> Discussion: AI in Data Engineering (30 mins)</li> </ul>"},{"location":"agenda/day2/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day2/#session-3","title":"Session 3","text":"<ul> <li><code>13:10</code> Investigation: Fire Drill preparation (20 mins)</li> <li><code>13:30</code> Fire drill technical briefing (10 mins)</li> <li><code>13:40</code> \ud83d\udd25 Fire Drill Round 1 (30 mins)</li> <li><code>14:10</code> Discussion: Fire drill debrief (20 mins)</li> </ul>"},{"location":"agenda/day2/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day2/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> \ud83d\udd25 Fire Drill Round 2 (30 mins)</li> <li><code>15:20</code> Discussion: Fire drill debrief (10 mins)</li> <li><code>15:30</code> Discussion: Workplace connections (20 mins)</li> </ul>"},{"location":"agenda/day2/#wrap-up","title":"Wrap up","text":""},{"location":"agenda/day3/","title":"Day 3 - Quality &amp; Governance","text":""},{"location":"agenda/day3/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Discussion: What does good data mean to you? (10 mins)</li> <li><code>09:40</code> Quality Data - why it matters (10 mins)</li> <li><code>09:50</code> Investigation: DMBOK's 6 data quality dimensions (40 mins)</li> <li><code>10:30</code> DMBOK report back (10 mins)</li> </ul>"},{"location":"agenda/day3/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day3/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Practice: Lab 3.1 ~ 03b Medallion Architecture (40 mins)</li> <li><code>11:40</code> Discussion: Quality patterns analysis (20 mins)</li> <li><code>12:00</code> Investigation: Quality tools (20 mins)</li> <li><code>12:20</code> Quality tools ~ report back (10 mins)</li> </ul>"},{"location":"agenda/day3/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day3/#session-3","title":"Session 3","text":"<ul> <li><code>13:30</code> Practice: Lab 3.2 ~ 19 Secure Data Access (40 mins)</li> <li><code>14:10</code> Discussion: Security vs Accessibility trade-offs (10 mins)</li> <li><code>14:20</code> Investigation: Governance Approaches Investigation (10 mins)</li> </ul>"},{"location":"agenda/day3/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day3/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> Practice: Lab 3.3 ~ 21 Deployment Pipelines (20 mins)</li> <li><code>15:10</code> Discussion: Deployment governance (20 mins)</li> <li><code>15:30</code> Workshop: Governance integration (20 mins)</li> </ul>"},{"location":"agenda/day3/#wrap-up","title":"Wrap up","text":""},{"location":"agenda/day4/","title":"Day 4 - Improvement &amp; Value","text":""},{"location":"agenda/day4/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome: Welcome &amp; Introduction to Day 4 (10 mins)</li> <li><code>09:40</code> Instructions &amp; Overview (10 mins)</li> <li><code>09:50</code> Groups: Planning for Sprint 1 (10 mins)</li> <li><code>10:00</code> Groups: Sprint Block 1 (40 mins)</li> </ul>"},{"location":"agenda/day4/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day4/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Stand-Up 1 (10 mins)</li> <li><code>11:10</code> Groups: Planning for Sprint 2 (10 mins)</li> <li><code>11:20</code> Groups: Sprint Block 2 (50 mins)</li> </ul>"},{"location":"agenda/day4/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day4/#session-3","title":"Session 3","text":"<ul> <li><code>13:10</code> Stand-Up 2 (10 mins)</li> <li><code>13:20</code> Groups: Planning for Sprint 3 (10 mins)</li> <li><code>13:30</code> Groups: Sprint Block 3 (50 mins)</li> </ul>"},{"location":"agenda/day4/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day4/#session-4","title":"Session 4","text":"<ul> <li><code>14:40</code> Sprint Review in groups (10 mins)</li> <li><code>14:50</code> Team Demos (30 mins)</li> <li><code>15:20</code> Sprint Retrospective (20 mins)</li> </ul>"},{"location":"agenda/day4/#course-evaluation","title":"\ud83d\udc4d Course Evaluation","text":""},{"location":"agenda/day4/#wrap-up","title":"Wrap up","text":""},{"location":"day1/cloud-monitoring-comparison/","title":"Cloud Monitoring Comparison","text":""},{"location":"day1/cloud-monitoring-comparison/#overview","title":"Overview","text":"<p>This investigation activity allows learners to explore how major cloud providers approach data monitoring, comparing different tools and philosophies. This builds on their hands-on Fabric experience and prepares them for creating their own monitoring strategy.</p>"},{"location":"day1/cloud-monitoring-comparison/#platform-assignments","title":"Platform Assignments","text":"<p>Assign each group one primary platform to research:</p>"},{"location":"day1/cloud-monitoring-comparison/#group-1-aws-monitoring","title":"Group 1: AWS Monitoring","text":"<ul> <li>CloudWatch (metrics, logs, dashboards)</li> <li>AWS X-Ray (distributed tracing)</li> <li>AWS Data Pipeline monitoring</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#group-2-google-cloud-monitoring","title":"Group 2: Google Cloud Monitoring","text":"<ul> <li>Cloud Monitoring (formerly Stackdriver)</li> <li>Cloud Logging</li> <li>Data pipeline monitoring in Cloud Composer/Dataflow</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#group-3-azure-monitoring-beyond-fabric","title":"Group 3: Azure Monitoring (beyond Fabric)","text":"<ul> <li>Azure Monitor</li> <li>Application Insights  </li> <li>Data Factory monitoring vs Fabric monitoring</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#group-4-fabric-deep-dive-if-enough-groups","title":"Group 4: Fabric Deep Dive (if enough groups)","text":"<ul> <li>Advanced Fabric monitoring features</li> <li>Integration with Azure Monitor</li> <li>Comparison with other Azure data services</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#core-monitoring-capabilities","title":"Core Monitoring Capabilities","text":""},{"location":"day1/cloud-monitoring-comparison/#1-what-can-you-monitor","title":"1. What can you monitor?","text":"<ul> <li>Data pipeline performance?</li> <li>Data quality metrics?</li> <li>Infrastructure metrics?</li> <li>Cost/resource usage?</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#2-how-do-you-set-up-monitoring","title":"2. How do you set up monitoring?","text":"<ul> <li>Automatic vs manual configuration?</li> <li>Built-in dashboards vs custom creation?</li> <li>Ease of setup for data pipelines?</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#3-alerting-and-notifications","title":"3. Alerting and Notifications","text":"<ul> <li>What triggers can you set?</li> <li>How are alerts delivered? (email, SMS, Slack, etc.)</li> <li>Can you set up automated responses?</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#data-specific-questions","title":"Data-Specific Questions","text":""},{"location":"day1/cloud-monitoring-comparison/#1-data-pipeline-monitoring","title":"1. Data Pipeline Monitoring","text":"<ul> <li>Can you track data freshness/latency?</li> <li>Schema change detection?</li> <li>Data volume monitoring?</li> <li>Failed job notifications?</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#2-integration-and-ecosystem","title":"2. Integration and Ecosystem","text":"<ul> <li>How well does it integrate with other tools?</li> <li>APIs for custom monitoring?</li> <li>Third-party tool support?</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#3-cost-and-complexity","title":"3. Cost and Complexity","text":"<ul> <li>What's the pricing model?</li> <li>How complex is it to maintain?</li> <li>Required expertise level?</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#research-strategy-tips","title":"Research Strategy Tips","text":"<ul> <li>Look for documentation, tutorials, and real-world examples</li> <li>Focus on data/analytics monitoring, not just general infrastructure</li> <li>Take screenshots of interesting dashboards or features</li> <li>Note what seems easy vs complex to implement</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#report-back-session","title":"Report Back Session","text":""},{"location":"day1/cloud-monitoring-comparison/#presentation-format","title":"Presentation Format","text":"<p>Each group has 90 seconds to share their key findings using this structure:</p> <p>30 seconds - Platform Overview:</p> <ul> <li>\"The main monitoring tool for platform X is...\"</li> <li>\"The best feature for data monitoring is...\"</li> </ul> <p>30 seconds - Strengths:</p> <ul> <li>\"What this platform does really well is...\"</li> <li>\"The standout capability is...\"</li> </ul> <p>30 seconds - Challenges/Gaps:</p> <ul> <li>\"The biggest limitation we found is...\"</li> <li>\"You'd struggle with this platform if...\"</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#key-questions-for-discussion","title":"Key Questions for Discussion","text":"<p>After all groups present, facilitate quick discussion:</p> <ul> <li>\"Which approach seems most similar to what you've used in your workplace?\"</li> <li>\"What surprised you about the differences between platforms?\"</li> <li>\"If you had to choose one for a new project, what would drive your decision?\"</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#expected-outcomes","title":"Expected Outcomes","text":"<p>By the end of this activity, learners should:</p> <ol> <li>Understand that different platforms have different monitoring philosophies</li> <li>Recognize common patterns across cloud monitoring tools  </li> <li>Have concrete examples of monitoring capabilities beyond what they've seen</li> <li>Be prepared to make informed choices about monitoring approaches</li> </ol>"},{"location":"day1/cloud-monitoring-report-back/","title":"Cloud Monitoring ~ Report Back","text":"<p>This investigation activity allows you to explore how major cloud providers approach data monitoring, comparing different tools and philosophies.</p>"},{"location":"day1/cloud-monitoring-report-back/#core-monitoring-capabilities","title":"Core Monitoring Capabilities","text":""},{"location":"day1/cloud-monitoring-report-back/#1-what-can-you-monitor","title":"1. What can you monitor?","text":"<ul> <li>Data pipeline performance?</li> <li>Data quality metrics?</li> <li>Infrastructure metrics?</li> <li>Cost/resource usage?</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#2-how-do-you-set-up-monitoring","title":"2. How do you set up monitoring?","text":"<ul> <li>Automatic vs manual configuration?</li> <li>Built-in dashboards vs custom creation?</li> <li>Ease of setup for data pipelines?</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#3-alerting-and-notifications","title":"3. Alerting and Notifications","text":"<ul> <li>What triggers can you set?</li> <li>How are alerts delivered? (email, SMS, Slack, etc.)</li> <li>Can you set up automated responses?</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#data-specific-questions","title":"Data-Specific Questions","text":""},{"location":"day1/cloud-monitoring-report-back/#1-data-pipeline-monitoring","title":"1. Data Pipeline Monitoring","text":"<ul> <li>Can you track data freshness/latency?</li> <li>Schema change detection?</li> <li>Data volume monitoring?</li> <li>Failed job notifications?</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#2-integration-and-ecosystem","title":"2. Integration and Ecosystem","text":"<ul> <li>How well does it integrate with other tools?</li> <li>APIs for custom monitoring?</li> <li>Third-party tool support?</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#3-cost-and-complexity","title":"3. Cost and Complexity","text":"<ul> <li>What's the pricing model?</li> <li>How complex is it to maintain?</li> <li>Required expertise level?</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#research-strategy-tips","title":"Research Strategy Tips","text":"<ul> <li>Look for documentation, tutorials, and real-world examples</li> <li>Focus on data/analytics monitoring, not just general infrastructure</li> <li>Take screenshots of interesting dashboards or features</li> <li>Note what seems easy vs complex to implement</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#report-back-session","title":"Report Back Session","text":""},{"location":"day1/cloud-monitoring-report-back/#platform-overview","title":"Platform Overview","text":"<ul> <li>\"The main monitoring tool for platform X is...\"</li> <li>\"The best feature for data monitoring is...\"</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#strengths","title":"Strengths","text":"<ul> <li>\"What this platform does really well is...\"</li> <li>\"The standout capability is...\"</li> </ul>"},{"location":"day1/cloud-monitoring-report-back/#challengesgaps","title":"Challenges/Gaps","text":"<ul> <li>\"The biggest limitation we found is...\"</li> <li>\"You'd struggle with this platform if...\"</li> </ul>"},{"location":"day1/monitoring-strategy-creation/","title":"Day 1: Monitoring Strategy Creation","text":""},{"location":"day1/monitoring-strategy-creation/#overview","title":"Overview","text":"<p>This production activity synthesizes everything learners have experienced today - the labs, status page analysis, and cloud platform research - into a comprehensive monitoring strategy. Working in pairs, they create a tangible monitoring framework that could be applied in real-world scenarios.</p>"},{"location":"day1/monitoring-strategy-creation/#session-structure","title":"Session Structure","text":""},{"location":"day1/monitoring-strategy-creation/#setup-instructions-3-minutes","title":"Setup Instructions (3 minutes)","text":""},{"location":"day1/monitoring-strategy-creation/#scenario-assignment","title":"Scenario Assignment","text":"<p>Give each pair a realistic scenario to make their strategy concrete:</p> <p>Scenario A: E-commerce Data Platform</p> <ul> <li>Real-time customer behaviour tracking</li> <li>Daily sales reporting pipelines  </li> <li>Inventory management data feeds</li> <li>Customer impact: Revenue loss if data is stale/incorrect</li> </ul> <p>Scenario B: Healthcare Analytics</p> <ul> <li>Patient data processing pipelines</li> <li>Clinical reporting systems</li> <li>Research data aggregation</li> <li>Customer impact: Patient safety and regulatory compliance</li> </ul> <p>Scenario C: Financial Services</p> <ul> <li>Transaction processing monitoring</li> <li>Risk calculation pipelines</li> <li>Regulatory reporting systems</li> <li>Customer impact: Financial losses and regulatory penalties</li> </ul> <p>Scenario D: Manufacturing IoT</p> <ul> <li>Sensor data ingestion from factory floor</li> <li>Predictive maintenance pipelines</li> <li>Quality control data processing</li> <li>Customer impact: Production delays and safety risks</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#strategy-development-phase-20-minutes","title":"Strategy Development Phase (20 minutes)","text":""},{"location":"day1/monitoring-strategy-creation/#framework-template","title":"Framework Template:","text":"<p>Provide pairs with this structured template to complete:</p>"},{"location":"day1/monitoring-strategy-creation/#1-what-we-monitor-5-minutes","title":"1. What We Monitor (5 minutes)","text":"<p>Define the key monitoring areas:</p> <p>Data Pipeline Health:</p> <ul> <li> Pipeline execution status (success/failure)</li> <li> Processing time/latency</li> <li> Data volume trends</li> <li> Error rates and types</li> </ul> <p>Data Quality:</p> <ul> <li> Completeness (missing records)</li> <li> Accuracy (data validation failures)  </li> <li> Freshness (data age/staleness)</li> <li> Schema changes/drift</li> </ul> <p>Infrastructure &amp; Performance:</p> <ul> <li> Resource utilization (CPU, memory, storage)</li> <li> Cost monitoring</li> <li> Scalability metrics</li> <li> Security events</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#2-how-we-monitor-5-minutes","title":"2. How We Monitor (5 minutes)","text":"<p>Choose tools and methods:</p> <p>Monitoring Tools:</p> <ul> <li>Primary platform: ________________</li> <li>Dashboard tools: ________________</li> <li>Alerting method: ________________</li> <li>Log aggregation: ________________</li> </ul> <p>Monitoring Approach:</p> <ul> <li> Real-time monitoring</li> <li> Batch/scheduled checks</li> <li> Trend analysis</li> <li> Predictive monitoring</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#3-when-we-alert-5-minutes","title":"3. When We Alert (5 minutes)","text":"<p>Define alert thresholds and escalation:</p> <p>Critical Alerts (immediate response required):</p> <ul> <li>Trigger: _________________________</li> <li>Notify: __________________________</li> <li>Response time: ___________________</li> </ul> <p>Warning Alerts (monitor closely):</p> <ul> <li>Trigger: _________________________</li> <li>Notify: __________________________</li> <li>Response time: ___________________</li> </ul> <p>Information Alerts (awareness only):</p> <ul> <li>Trigger: _________________________</li> <li>Notify: __________________________</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#4-who-responds-3-minutes","title":"4. Who Responds (3 minutes)","text":"<p>Define roles and responsibilities:</p> <p>First Response Team:</p> <ul> <li>Role: ____________________________</li> <li>Responsibilities: _________________</li> </ul> <p>Escalation Path:</p> <ul> <li>Level 1: _________________________</li> <li>Level 2: _________________________</li> <li>Level 3: _________________________</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#5-communication-plan-2-minutes","title":"5. Communication Plan (2 minutes)","text":"<p>How you communicate during incidents:</p> <p>Internal Communication:</p> <ul> <li>Method: __________________________</li> <li>Frequency: _______________________</li> </ul> <p>External Communication:</p> <ul> <li>Stakeholders: ____________________</li> <li>Method: __________________________</li> <li>Message template: ________________</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#strategy-sharing-feedback-7-minutes","title":"Strategy Sharing &amp; Feedback (7 minutes)","text":""},{"location":"day1/monitoring-strategy-creation/#rapid-strategy-showcase-5-minutes","title":"Rapid Strategy Showcase (5 minutes):","text":"<ul> <li>Each pair screen shares for 60 seconds</li> <li>Focus on \"one key decision we made and why\"</li> <li>Other pairs post ONE insight in chat</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#group-reflection-2-minutes","title":"Group Reflection (2 minutes):","text":"<ul> <li>Facilitator reads chat highlights</li> <li>\"What common themes do you notice?\"</li> <li>\"What surprised you about different approaches?\"---</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#expected-outputs","title":"Expected Outputs:","text":"<p>Each pair should produce:</p> <ol> <li>Completed monitoring strategy template</li> <li>Understanding of monitoring complexity</li> <li>Appreciation for different monitoring approaches</li> <li>Foundation for tomorrow's incident response activities</li> </ol>"},{"location":"day1/share-monitoring-strategies/","title":"Day 1: Wrap-up","text":""},{"location":"day1/share-monitoring-strategies/#strategy-sharing-feedback-7-minutes","title":"Strategy Sharing &amp; Feedback (7 minutes)","text":""},{"location":"day1/share-monitoring-strategies/#rapid-strategy-showcase-5-minutes","title":"Rapid Strategy Showcase (5 minutes)","text":"<ul> <li>Each pair screen shares for 60 seconds</li> <li>Focus on \"one key decision we made and why\"</li> <li>Other pairs post ONE insight in chat</li> </ul>"},{"location":"day1/share-monitoring-strategies/#group-reflection-2-minutes","title":"Group Reflection (2 minutes)","text":"<ul> <li>Facilitator reads chat highlights</li> <li>\"What common themes do you notice?\"</li> <li>\"What surprised you about different approaches?\"</li> </ul>"},{"location":"day1/status-page-analysis/","title":"Status Page Analysis","text":"<p>Learning Type: Investigation Format: Pairs research with report back</p>"},{"location":"day1/status-page-analysis/#setup-instructions-2-minutes","title":"Setup Instructions (2 minutes)","text":"<ol> <li>Form pairs</li> <li>Assign each pair 1-2 status pages from the list below</li> <li>Research task: 10 minutes investigation + 3 minutes prep for sharing</li> </ol>"},{"location":"day1/status-page-analysis/#status-pages-to-research","title":"Status Pages to Research","text":""},{"location":"day1/status-page-analysis/#comprehensive","title":"Comprehensive","text":"<ul> <li>AWS: https://health.aws.amazon.com/health/status</li> <li>Microsoft Azure: https://status.azure.com/</li> <li>Google Cloud: https://status.cloud.google.com/</li> <li>Zoom: https://status.zoom.us/</li> </ul>"},{"location":"day1/status-page-analysis/#less-detail","title":"Less detail","text":"<ul> <li>GitHub: https://www.githubstatus.com/</li> <li>Slack: https://status.slack.com/</li> <li>Stripe: https://status.stripe.com/</li> </ul>"},{"location":"day1/status-page-analysis/#investigation-questions-for-pairs","title":"Investigation Questions for Pairs","text":"<p>Provide each pair with these guiding questions:</p> <p>Current Status:</p> <ul> <li>What's the overall system health right now?</li> <li>How is information organised? (by service, region, etc.)</li> </ul> <p>Historical Incidents:</p> <ul> <li>Look at 1-2 recent incidents - what information is provided?</li> <li>How detailed are the updates?</li> <li>How frequently were updates posted?</li> </ul> <p>Communication Style:</p> <ul> <li>How technical is the language?</li> <li>What do they tell users about impact?</li> <li>Do they explain what they're doing to fix it?</li> </ul> <p>What Works Well:</p> <ul> <li>What would be helpful if you were a customer?</li> <li>What builds confidence that they're handling it?</li> </ul>"},{"location":"day1/status-page-analysis/#report-back-session-3-minutes","title":"Report Back Session (3 minutes)","text":"<p>Each pair shares one key insight in 30 seconds:</p> <ul> <li>\"The most interesting thing we noticed was...\"</li> <li>\"If we were designing a status page, we'd make sure to...\"</li> </ul>"},{"location":"day1/status-page-analysis/#facilitator-notes","title":"Facilitator Notes","text":"<ul> <li>If internet issues: Have screenshots of status pages prepared as backup</li> <li>Time management: Keep sharing tight - use a timer</li> <li>Follow-up questions: \"How does this connect to monitoring?\" \"What would happen without these status pages?\"</li> </ul>"},{"location":"day1/status-page-report-back/","title":"Status Page Analysis ~ Report Back","text":""},{"location":"day1/status-page-report-back/#investigation-questions","title":"Investigation Questions","text":""},{"location":"day1/status-page-report-back/#current-status","title":"Current Status","text":"<ul> <li>What is the overall system health right now?</li> <li>How is information organised? (by service, region, etc.)</li> </ul>"},{"location":"day1/status-page-report-back/#historical-incidents","title":"Historical Incidents","text":"<ul> <li>Look at 1-2 recent incidents - what information is provided?</li> <li>How detailed are the updates?</li> <li>How frequently were updates posted?</li> </ul>"},{"location":"day1/status-page-report-back/#communication-style","title":"Communication Style","text":"<ul> <li>How technical is the language?</li> <li>What do they tell users about impact?</li> <li>Do they explain what they're doing to fix it?</li> </ul>"},{"location":"day1/status-page-report-back/#what-works-well","title":"What Works Well","text":"<ul> <li>What would be helpful if you were a customer?</li> <li>What builds confidence that they're handling it?</li> </ul>"},{"location":"day1/status-page-report-back/#report-back-session","title":"Report Back Session","text":"<p>Each pair shares a key insight:</p> <ul> <li>\"The most interesting thing we noticed was...\"</li> <li>\"If we were designing a status page, we'd make sure to...\"</li> </ul>"},{"location":"day1/system-failure/","title":"System Failure Stories","text":"<ul> <li>Learning Type: Discussion  </li> <li>Format: Group discussion with individual sharing</li> </ul>"},{"location":"day1/system-failure/#facilitator-instructions","title":"Facilitator Instructions","text":""},{"location":"day1/system-failure/#opening-question","title":"Opening Question","text":"<ul> <li>Think of a time when a system failure impacted you personally</li> <li>Could be at work, online shopping, banking, social media.</li> <li>What happened?</li> </ul>"},{"location":"day1/system-failure/#individual-reflection","title":"Individual Reflection","text":"<ul> <li>Give learners time to think of their example</li> <li>Encourage them to consider: What broke? How did they find out? What was the impact?</li> </ul>"},{"location":"day1/system-failure/#sharing-round","title":"Sharing Round","text":"<p>Each learner shares their story (1-2 minutes each)</p> <p>As facilitator, listen for common themes:</p> <ul> <li>How they discovered the problem</li> <li>Lack of communication about the issue</li> <li>Business/personal impact</li> <li>Resolution time</li> </ul>"},{"location":"day1/system-failure/#pattern-recognition","title":"Pattern Recognition","text":"<ul> <li>Summarise common themes you heard</li> <li>What patterns do you notice in these stories?</li> </ul>"},{"location":"day1/system-failure/#key-questions-to-guide-discussion","title":"Key Questions to Guide Discussion","text":"<ul> <li>How did you first realise something was wrong?</li> <li>Did anyone tell you what was happening?</li> <li>How long did it take to resolve?</li> <li>What would have made the experience better?</li> </ul>"},{"location":"day1/system-failure/#expected-outcomes","title":"Expected Outcomes","text":"<ul> <li>Learners connect personally with system reliability challenges</li> <li>Common themes emerge: detection, communication, resolution</li> <li>Natural lead-in to why monitoring matters</li> </ul>"},{"location":"day1/why-monitor/","title":"Why Monitor?","text":"<p>This session establishes the foundational understanding of why monitoring matters in data operations by connecting personal experiences with business impact and examining real-world communication during system failures.</p> <ul> <li>Learning Type: Acquisition  </li> <li>Format: Trainer presentation with interactive elements</li> </ul>"},{"location":"day1/why-monitor/#key-points-to-cover","title":"Key Points to Cover","text":""},{"location":"day1/why-monitor/#from-personal-pain-to-business-impact","title":"From Personal Pain to Business Impact","text":"<ul> <li>Personal frustration = lost customers/revenue for businesses</li> <li>System downtime costs: Amazon loses \u00a32.8M per minute of downtime</li> <li>Reputation damage can last longer than the outage</li> </ul>"},{"location":"day1/why-monitor/#the-monitoring-pyramid","title":"The Monitoring Pyramid","text":"<pre><code>Business Impact     (Customer complaints, lost revenue)\n        \u2191\nService Impact      (Application slow/down, data unavailable)\n        \u2191\nTechnical Issues    (CPU high, disk full, network timeout)\n        \u2191\nMonitoring Data     (Metrics, logs, alerts)\n</code></pre>"},{"location":"day1/why-monitor/#key-monitoring-questions","title":"Key Monitoring Questions","text":"<ol> <li>Is it working? (Availability monitoring)</li> <li>Is it working well? (Performance monitoring)  </li> <li>Will it keep working? (Predictive monitoring)</li> <li>When it breaks, how do we know? (Alerting)</li> <li>When it breaks, what do we tell people? (Communication)</li> </ol>"},{"location":"day1/why-monitor/#interactive-elements","title":"Interactive Elements","text":"<ul> <li>Ask: \"Which level of the pyramid do you think most organisations focus on?\"</li> <li>Quick poll: \"Hands up if your workplace monitors at each level\"</li> </ul>"},{"location":"day1/why-monitor/#transition-to-next-session","title":"Transition to Next Session","text":"<p>Bridge to Lab 5 (2 minutes):</p> <ul> <li>\"We've talked about why monitoring matters and how companies communicate when things go wrong\"</li> <li>\"Now let's start building something we can monitor\"</li> <li>\"Our first lab will introduce you to Microsoft Fabric and create a simple data process\"</li> </ul>"},{"location":"day1/why-monitor/#key-takeaways","title":"Key Takeaways","text":"<p>By the end of this session, learners should understand:</p> <ol> <li>System failures have real business and personal impact</li> <li>Monitoring helps detect, communicate, and resolve issues faster</li> <li>Good communication during incidents builds trust</li> <li>We're going to learn operational skills to handle these challenges</li> </ol>"},{"location":"day2/breaking-things/","title":"Day 2: Breaking Things","text":""},{"location":"day2/breaking-things/#overview","title":"Overview","text":"<p>This hands-on session introduces failures into working data pipelines. You will practice controlled troubleshooting while observing how MS Fabric responds to different types of failures.</p>"},{"location":"day2/breaking-things/#part-1-establish-working-baseline","title":"Part 1: Establish Working Baseline","text":""},{"location":"day2/breaking-things/#setup-instructions","title":"Setup Instructions","text":"<p>Technical Setup:</p> <ul> <li>This morning you did Lab 2.1 - Ingest Pipeline Data</li> <li>Verify that the data flows through successfully</li> </ul>"},{"location":"day2/breaking-things/#part-2-systematic-breaking-learning","title":"Part 2: Systematic Breaking &amp; Learning","text":""},{"location":"day2/breaking-things/#break-cycle-process","title":"Break Cycle Process","text":"<p>For each break type, follow this 4-step cycle:</p> <ol> <li>Break   - introduce the failure</li> <li>Observe - run pipeline, note error messages</li> <li>Discuss - what does the error tell us?</li> <li>Fix     - restore to working state</li> </ol>"},{"location":"day2/breaking-things/#break-1-file-not-found","title":"Break 1: File Not Found","text":"<p>Break Instructions:</p> <ul> <li>Navigate to your source data file</li> <li>Rename it - add <code>_broken</code> to the filename</li> <li>Now try to run your pipeline</li> </ul> <p>Observation Points:</p> <ul> <li>How quickly can you see the error?</li> <li>What does Fabric's error message say?</li> <li>Does the error message help you identify the problem?</li> <li>Is it clear what needs to be fixed?</li> </ul> <p>Discussion:</p> <p>Fix &amp; Verify:</p> <ul> <li>Rename the file back to original name</li> <li>Run pipeline again to confirm it works</li> </ul>"},{"location":"day2/breaking-things/#break-2-schema-mismatch","title":"Break 2: Schema Mismatch","text":"<p>Break Instructions:</p> <ul> <li>Open your source CSV file</li> <li>Delete one of the column headers (not the data, just the header)</li> <li>Save the file and run your pipeline</li> </ul> <p>Observation Points:</p> <ul> <li>Does the error message clearly indicate schema issues?</li> <li>How long does it take to identify the problem?</li> <li>Is the difference clear between this and the file error?</li> </ul> <p>Fix &amp; Verify:</p> <ul> <li>Add the column header back</li> <li>Verify pipeline works again</li> </ul>"},{"location":"day2/breaking-things/#break-3-data-quality-issues","title":"Break 3: Data Quality Issues","text":"<p>Break Instructions:</p> <ul> <li>In your CSV, find a numeric column</li> <li>Change some numbers to text (like 'ERROR' or 'N/A')</li> <li>Save and run pipeline</li> </ul> <p>Observation Points:</p> <ul> <li>Does Fabric handle data type mismatches gracefully?</li> <li>What happens to the bad data - does it get skipped or cause total failure?</li> </ul> <p>Fix &amp; Verify:</p> <ul> <li>Fix the data values back to numbers</li> <li>Confirm everything works</li> </ul>"},{"location":"day2/complex-breaking/","title":"Day 2: Advanced Breaking Scenarios","text":""},{"location":"day2/complex-breaking/#overview","title":"Overview","text":"<p>This session introduces more complex failure scenarios that mirror real-world production issues.</p>"},{"location":"day2/complex-breaking/#scenario-1-performance-issues","title":"Scenario 1: Performance Issues","text":""},{"location":"day2/complex-breaking/#setup","title":"Setup","text":"<p>Context: \"Large files and complex processing can cause timeouts and resource issues\"</p> <p>Breaking Method:</p> <p>Find or create a much larger dataset than used in the previous lab</p> <p>Options:</p> <ul> <li>Duplicate your existing file multiple times to create a large CSV</li> <li>Use online sample datasets (sales data, sensor data, etc.)</li> <li>Create synthetic data with repeated rows</li> </ul> <p>Target: File should be significantly larger (aim for 10x+ size)</p>"},{"location":"day2/complex-breaking/#execution-observation","title":"Execution &amp; Observation","text":"<p>Break Process:</p> <ol> <li>Replace your pipeline's source with the large file</li> <li>Run the pipeline and monitor performance</li> <li>Observe behavior: Does it slow down? Timeout? Complete successfully?</li> <li>Check resource usage if visible in Fabric interface</li> </ol> <p>Key Observation Points:</p> <ul> <li>How long does processing take compared to small files?</li> <li>Does Fabric show any warnings or performance indicators?</li> <li>Are there any timeout errors or resource constraint messages?</li> <li>How does the monitoring respond to slower processing?</li> </ul>"},{"location":"day2/complex-breaking/#documentation","title":"Documentation","text":"<p>Quick Notes: Write down what you observed for later discussion</p>"},{"location":"day2/complex-breaking/#scenario-2-data-corruption-scenarios","title":"Scenario 2: Data Corruption Scenarios","text":""},{"location":"day2/complex-breaking/#setup_1","title":"Setup","text":"<p>Context: \"Real data often contains mixed quality - some good records, some problematic ones\"</p> <p>Breaking Method Options (choose 1-2 to try):</p> <p>Option A: Mixed Data Types</p> <ul> <li>In a numeric column, replace some values with text (\"ERROR\", \"NULL\", \"N/A\")</li> <li>Leave other values as valid numbers</li> <li>See if Fabric processes partially or fails completely</li> </ul> <p>Option B: Encoding Issues</p> <ul> <li>Add special characters or emojis to text fields</li> <li>Try different character encodings if possible</li> <li>See how Fabric handles non-standard characters</li> </ul> <p>Option C: Incomplete Records</p> <ul> <li>Delete some values in the middle of rows (creating gaps)</li> <li>Remove entire columns from some rows</li> <li>Create inconsistent row lengths</li> </ul>"},{"location":"day2/complex-breaking/#execution-observation_1","title":"Execution &amp; Observation","text":"<p>Break Process:</p> <ol> <li>Choose one corruption type and modify your data file</li> <li>Run the pipeline and carefully observe results</li> <li>Check output data - did bad records get processed, skipped, or cause failure?</li> <li>Try a second corruption type if time allows</li> </ol> <p>Key Observation Points:</p> <ul> <li>Does the pipeline fail completely or continue with warnings?</li> <li>How does Fabric handle the corrupted data?</li> <li>What error messages or warnings appear?</li> <li>Can you identify which specific records caused problems?</li> <li>How would a business user know there was an issue?</li> </ul>"},{"location":"day2/complex-breaking/#documentation_1","title":"Documentation","text":"<p>Record findings: What happened with each corruption type?</p>"},{"location":"day2/complex-breaking/#scenario-3-resource-constraints-timing-issues","title":"Scenario 3: Resource Constraints &amp; Timing Issues","text":""},{"location":"day2/complex-breaking/#setup_2","title":"Setup","text":"<p>Context: \"Production systems often have multiple processes competing for resources\"</p> <p>Breaking Method Options:</p> <p>Option A: Multiple Simultaneous Pipelines</p> <ul> <li>Create duplicate copies of your pipeline</li> <li>Run multiple instances at the same time</li> <li>See how Fabric handles concurrent resource usage</li> </ul> <p>Option B: Complex Transformations</p> <ul> <li>Add complex calculations or data transformations to your pipeline</li> <li>Create joins between multiple large datasets if possible</li> <li>See if processing becomes resource-intensive</li> </ul> <p>Option C: Rapid Successive Runs</p> <ul> <li>Run the same pipeline repeatedly in quick succession</li> <li>Don't wait for completion before starting the next run</li> <li>Observe queuing, conflicts, or resource competition</li> </ul>"},{"location":"day2/complex-breaking/#execution-observation_2","title":"Execution &amp; Observation","text":"<p>Break Process:</p> <ol> <li>Choose one resource constraint scenario</li> <li>Execute your chosen approach</li> <li>Monitor system behavior and any queue/resource indicators</li> <li>Try to overwhelm the system (within reasonable limits)</li> </ol> <p>Key Observation Points:</p> <ul> <li>How does Fabric manage multiple concurrent operations?</li> <li>Are there visible queue indicators or resource monitors?</li> <li>Do operations slow down, queue up, or fail?</li> <li>How long before the system recovers to normal performance?</li> </ul>"},{"location":"day2/complex-breaking/#documentation_2","title":"Documentation","text":"<p>Note observations: System behavior under load</p>"},{"location":"day2/fire-drill-briefing/","title":"Day 2: Fire Drill Briefing","text":""},{"location":"day2/fire-drill-briefing/#trainer-instructions","title":"Trainer Instructions","text":""},{"location":"day2/fire-drill-briefing/#opening","title":"Opening","text":"<p>Dramatic tone: \"Right, preparation time is over. We have a LIVE INCIDENT situation developing with our data platform. You are now the incident response teams for our organization.\"</p> <p>Set the scene: \"It's a typical workday, you're focused on your regular tasks, when suddenly...\"</p>"},{"location":"day2/fire-drill-briefing/#rules-logistics","title":"Rules &amp; Logistics","text":"<p>Team Coordination:</p> <ul> <li>Your team has a war room - your breakout room for internal discussion</li> <li>You can plan, debate, and coordinate freely in your team room</li> <li>BUT - all official communication must go through the chat app</li> </ul> <p>Chat Communication Rules:</p> <ul> <li>Use your Chat channel for incident updates to stakeholders</li> <li>This is how you communicate with customers, executives, and other teams</li> <li>Write professionally - these messages will be reviewed during debrief</li> <li>First action should be incident announcement to the channel</li> </ul> <p>Timing:</p> <ul> <li>You have 25 minutes to handle this incident</li> <li>I'll be monitoring your communications</li> <li>You may recive additional updates or pressure via as the situation develops</li> </ul>"},{"location":"day2/fire-drill-briefing/#scenario-distribution-1-minute","title":"Scenario Distribution (1 minute)","text":"<p>Hand out scenario cards:</p> <ul> <li>Read your scenario and understand the situation</li> <li>Then immediately post your first incident update to the chat app</li> <li>Remember: stakeholders are waiting to hear from you</li> </ul> <p>Guidelines:</p> <ul> <li>Don't watch the other teams</li> <li>Focus on what your team needs to do!</li> </ul>"},{"location":"day2/fire-drill-briefing/#success-criteria","title":"Success Criteria","text":"<p>What we're looking for:</p> <ul> <li>Clear, professional communication via the chat app</li> <li>Systematic approach to incident response</li> <li>Good coordination within your team</li> <li>Balancing technical fixes with stakeholder communication</li> </ul> <p>Important:</p> <ul> <li>There's no single 'right' answer - we're practicing process and communication</li> <li>Real incidents are messy and uncertain - embrace that</li> <li>Use what you learned from the major incident examples</li> </ul>"},{"location":"day2/fire-drill-briefing/#facilitator-actions-during-briefing","title":"Facilitator Actions During Briefing","text":""},{"location":"day2/fire-drill-briefing/#before-starting","title":"Before Starting:","text":"<ul> <li>Ensure breakout rooms are ready for each team</li> <li>Confirm chat app is set up and all learners can access</li> <li>Have scenario cards ready to distribute</li> <li>Set 25-minute timer visible to all</li> </ul>"},{"location":"day2/fire-drill-briefing/#during-briefing","title":"During Briefing:","text":"<ul> <li>Maintain dramatic energy - this is a real incident simulation</li> <li>Check for questions but keep momentum high</li> <li>Emphasize chat communication - this is key learning objective</li> <li>Watch for any technical issues with breakout rooms or the chat app</li> </ul>"},{"location":"day2/fire-drill-briefing/#after-briefing","title":"After Briefing:","text":"<ul> <li>Send teams to breakout rooms immediately after scenario distribution</li> <li>Monitor the chat app for their first posts</li> <li>Prepare to send escalation messages during the 25-minute period</li> </ul>"},{"location":"day2/fire-drill-briefing/#optional-pressure-elements","title":"Optional Pressure Elements","text":""},{"location":"day2/fire-drill-briefing/#during-the-25-minute-drill-you-can-add-realism-by-sending-these-messages","title":"During the 25-minute drill, you can add realism by sending these messages:","text":"<p>5 minutes in:</p> <p>\"Customer calls are increasing - service desk getting overwhelmed\"</p> <p>10 minutes in:</p> <p>\"Social media mentions of our service issues are trending\"</p> <p>15 minutes in:</p> <p>\"CEO's office is asking for status update\"</p> <p>20 minutes in:</p> <p>\"Major client just called - they're considering switching providers\"</p> <p>Use sparingly - only if teams seem to be handling the basic scenario well.</p>"},{"location":"day2/fire-drill-briefing/#what-to-watch-for","title":"What to Watch For","text":""},{"location":"day2/fire-drill-briefing/#during-the-drill","title":"During the Drill","text":"<p>Chat Communication Quality:</p> <ul> <li>Are first posts professional and informative?</li> <li>Do they provide regular updates?</li> <li>Are messages clear for non-technical audiences?</li> </ul> <p>Team Coordination:</p> <ul> <li>Are they using breakout rooms effectively?</li> <li>Is there clear role separation?</li> <li>Are decisions being made efficiently?</li> </ul> <p>Incident Response Process:</p> <ul> <li>Do they follow systematic approach?</li> <li>Are they balancing technical fixes with communication?</li> <li>How do they handle uncertainty?</li> </ul>"},{"location":"day2/fire-drill-briefing/#common-issues-to-note","title":"Common Issues to Note:","text":"<p>Communication Problems:</p> <ul> <li>Too technical for general audience</li> <li>Infrequent updates or radio silence</li> <li>Unclear or confusing messaging</li> </ul> <p>Coordination Issues:</p> <ul> <li>All talking at once, no clear leadership</li> <li>Forgetting to update the chat app while focused on technical discussion</li> <li>Paralysis when facing uncertain information</li> </ul> <p>These become great debrief discussion points!</p>"},{"location":"day2/fire-drill-briefing/#wrap-up-at-25-minutes","title":"Wrap-up at 25 Minutes","text":""},{"location":"day2/fire-drill-briefing/#end-the-drill","title":"End the Drill:","text":"<p>Announcement:</p> <p>Time! Step back from your incident response. Well done everyone.</p> <p>Immediate Actions:</p> <ul> <li>Bring all teams back to main room</li> <li>Keep chat app channels open for review</li> <li>Prepare for 10-minute quick debrief</li> </ul>"},{"location":"day2/fire-drill-briefing/#transition","title":"Transition","text":"<p>Before we break, let's do a quick review of your communication and initial responses. Then we'll take a break before the next escalation phase.</p>"},{"location":"day2/fire-drill-briefing/#success-indicators","title":"Success Indicators","text":"<p>Good Simulation if:</p> <ul> <li>Teams immediately start coordinating in breakout rooms</li> <li>Chat shows professional incident updates within first 5 minutes</li> <li>Teams balance technical discussion with communication needs</li> <li>Energy level is high and focused</li> </ul> <p>Adjust if:</p> <ul> <li>Teams seem confused about process (provide clarification)</li> <li>Chat communication is sparse (remind about stakeholder updates)</li> <li>Teams are too stressed (reduce pressure elements)</li> <li>Teams finish too quickly (add complexity via chat app updates)</li> </ul>"},{"location":"day2/fire-drill-groups/","title":"Fire Drill Groups","text":"<ul> <li>Group 1 ~ Blaze </li> <li>Group 2 ~ Ember </li> </ul>"},{"location":"day2/group-blaze/","title":"Incident Response Scenario","text":""},{"location":"day2/group-blaze/#group-1-blaze","title":"Group 1 ~ BLAZE","text":""},{"location":"day2/group-blaze/#the-communications-app","title":"The Communications App","text":"<p>Here is the link the communications app for your group:</p> <ul> <li>https://drill.ingwane.org/BLAZE</li> </ul>"},{"location":"day2/group-blaze/#notes","title":"Notes","text":"<ul> <li>One, or more, or all of you can enter a message in the app</li> <li>Make sure you update the from with a name and title</li> <li>The To is optional but is recommended</li> </ul>"},{"location":"day2/group-blaze/#here-are-links-to-the-status-pages","title":"Here are links to the Status Pages","text":"<ul> <li>Status mailbox: https://ingwanelabs.github.io/status/mailbox/</li> <li>Status update: https://ingwanelabs.github.io/status/message-202521320/</li> </ul>"},{"location":"day2/group-blaze/#rules-logistics","title":"Rules &amp; Logistics","text":""},{"location":"day2/group-blaze/#team-coordination","title":"Team Coordination","text":"<ul> <li>Your team has a breakout room for internal discussion</li> <li>You can plan, debate, and coordinate freely in your team room</li> <li>BUT - all official communication must go through the chat app</li> </ul>"},{"location":"day2/group-blaze/#chat-communication-rules","title":"Chat Communication Rules","text":"<ul> <li>Use your chat app for incident updates to stakeholders</li> <li>This is how you communicate with customers, executives, and other teams</li> <li>Write professionally - these messages will be reviewed during debrief</li> <li>First action should be incident announcement to the channel</li> </ul>"},{"location":"day2/group-blaze/#timing","title":"Timing","text":"<ul> <li>You have 25 minutes to handle this incident</li> </ul> <p>At the end we will debrief by looking at all the messages you sent!</p>"},{"location":"day2/group-blaze/#suggested-personas","title":"Suggested Personas","text":"<ul> <li>Incident Commander: Coordinates overall response, makes key decisions</li> <li>Technical Lead: Investigates root cause, determines fix options  </li> <li>Communications Lead: Handles stakeholder updates and messaging</li> <li>Business Impact Analyst: Assesses downstream effects</li> </ul>"},{"location":"day2/group-ember/","title":"Incident Response Scenario","text":""},{"location":"day2/group-ember/#group-2-ember","title":"Group 2 ~ EMBER","text":""},{"location":"day2/group-ember/#the-communications-app","title":"The Communications App","text":"<p>Here is the link the communications app for your group:</p> <ul> <li>https://drill.ingwane.org/EMBER</li> </ul>"},{"location":"day2/group-ember/#notes","title":"Notes","text":"<ul> <li>One, or more, or all of you can enter a message in the app</li> <li>Make sure you update the from with a name and title</li> <li>The To is optional but is recommended</li> </ul>"},{"location":"day2/group-ember/#here-are-links-to-the-status-pages","title":"Here are links to the Status Pages","text":"<ul> <li>Status mailbox: https://ingwanelabs.github.io/status/mailbox/</li> <li>Status update: https://ingwanelabs.github.io/status/message-202521320/</li> </ul>"},{"location":"day2/group-ember/#rules-logistics","title":"Rules &amp; Logistics","text":""},{"location":"day2/group-ember/#team-coordination","title":"Team Coordination","text":"<ul> <li>Your team has a breakout room for internal discussion</li> <li>You can plan, debate, and coordinate freely in your team room</li> <li>BUT - all official communication must go through the chat app</li> </ul>"},{"location":"day2/group-ember/#chat-communication-rules","title":"Chat Communication Rules","text":"<ul> <li>Use your chat app for incident updates to stakeholders</li> <li>This is how you communicate with customers, executives, and other teams</li> <li>Write professionally - these messages will be reviewed during debrief</li> <li>First action should be incident announcement to the channel</li> </ul>"},{"location":"day2/group-ember/#timing","title":"Timing","text":"<ul> <li>You have 25 minutes to handle this incident</li> </ul> <p>At the end we will debrief by looking at all the messages you sent!</p>"},{"location":"day2/group-ember/#suggested-personas","title":"Suggested Personas","text":"<ul> <li>Incident Commander: Coordinates overall response, makes key decisions</li> <li>Technical Lead: Investigates root cause, determines fix options  </li> <li>Communications Lead: Handles stakeholder updates and messaging</li> <li>Business Impact Analyst: Assesses downstream effects</li> </ul>"},{"location":"day2/incident-response-prep/","title":"Day 2: Fire Drill Preparation","text":""},{"location":"day2/incident-response-prep/#whats-coming-next","title":"What's Coming Next","text":"<p>In the next session, you'll participate in an incident response \"fire drill\" simulating a data platform failure. You'll work in teams to coordinate technical responses and stakeholder communication under time pressure.</p> <p>But before we do that, it may be useful to read up on some real incidents that happened.</p>"},{"location":"day2/incident-response-prep/#your-preparation-task","title":"Your Preparation Task","text":""},{"location":"day2/incident-response-prep/#real-incident-examples","title":"Real Incident Examples","text":"<p>Scan these major incidents to understand what works (and what doesn't) in incident response:</p> <p>Cloudflare Global DNS Outage - June 21, 2022</p> <ul> <li>27-minute global DNS failure affecting millions of websites</li> <li>Link: https://blog.cloudflare.com/cloudflare-outage-on-june-21-2022/</li> <li>Key lesson: Speed vs. accuracy in technical communication</li> </ul> <p>AWS us-east-1 Outage - December 7, 2021 </p> <ul> <li>5+ hour power-related outage affecting thousands of services</li> <li>Search: \"AWS us-east-1 outage December 2021 post-mortem\"</li> <li>Key lesson: Cascading failures and dependency management</li> </ul> <p>GitHub Database Incident - October 21, 2018</p> <ul> <li>24+ hour database cluster failure during maintenance</li> <li>Search: \"GitHub October 2018 incident post-mortem database\"</li> <li>Key lesson: Incident escalation and technical decision-making</li> </ul> <p>Meta/Facebook Global Outage - October 4, 2021</p> <ul> <li>6+ hour BGP configuration error taking down all Meta services</li> <li>Search: \"Facebook outage October 2021 BGP routing\"</li> <li>Key lesson: When monitoring systems fail too</li> </ul>"},{"location":"day2/incident-response-prep/#extract-key-principles","title":"Extract Key Principles","text":"<p>Focus your research on these questions</p> <p>Detection &amp; Assessment:</p> <ul> <li>How quickly did they recognize the problem?</li> <li>What information did they gather before acting?</li> <li>How did they assess business impact?</li> </ul> <p>Technical Response:</p> <ul> <li>Did they implement quick fixes or wait for proper solutions?</li> <li>How did they coordinate multiple teams?</li> <li>What tools and processes helped/hindered them?</li> </ul> <p>Communication Strategy:</p> <ul> <li>When did they first communicate publicly?</li> <li>How detailed were their updates?</li> <li>How did they handle uncertainty in their messaging?</li> <li>What different messages did they send to different audiences?</li> </ul> <p>Coordination &amp; Decision-Making:</p> <ul> <li>Who made key decisions?</li> <li>How did they balance speed vs. accuracy?</li> <li>What escalation triggers did they use?</li> </ul>"},{"location":"day2/incident-response-prep/#research-strategy-tips","title":"Research Strategy Tips","text":"<p>Don't try to read everything! Focus on:</p> <ul> <li>Executive summaries and key timeline points</li> <li>Communication examples - actual status updates they published</li> <li>Lessons learned sections in post-mortems</li> <li>Decision points - when they chose one approach over another</li> </ul> <p>Look for patterns:</p> <ul> <li>What's common across different incidents?</li> <li>What approaches consistently work or fail?</li> <li>How do companies handle uncertainty in their communication?</li> </ul> <p>The goal isn't deep expertise - it's rather preparation for practical application!</p>"},{"location":"day2/scenario1-debrief/","title":"Scenario 1 ~ Debrief","text":""},{"location":"day2/scenario1-debrief/#scenario-1-debrief-elements-you-can-use","title":"Scenario 1 Debrief Elements You Can Use:","text":"<p>From the Scenario Card:</p> <ul> <li>Customer Dashboard Failure - stale data since 6 AM</li> <li>Multiple stakeholder pressure - Sales Director, Customer Success, IT Director</li> <li>Time pressure - Board meeting at 10 AM (45 minutes)</li> <li>Technical context - Fabric pipeline failed, vendor data source changes</li> </ul> <p>Observable Team Behaviors:</p> <ul> <li>First response approach - technical investigation vs. immediate communication</li> <li>Stakeholder prioritization - who did they contact first?</li> <li>Communication timing - how quickly did they update your status site?</li> <li>Status update quality - professional vs. technical language</li> </ul> <p>Optional Pressure Elements You Added:</p> <ul> <li>Customer service call volume doubling</li> <li>Premium client Acme Corp threatening contract review</li> <li>Sales Director urgency about board meeting</li> <li>IT Director asking for technical details</li> </ul>"},{"location":"day2/scenario1-debrief/#debrief-questions-that-work","title":"Debrief Questions That Work:","text":"<p>Decision Comparison:</p> <ul> <li>\"Team A focused on customer communication first, Team B went straight to technical investigation - what are the pros/cons?\"</li> <li>\"Which approach would work better in your workplace?\"</li> </ul> <p>Communication Analysis:</p> <ul> <li>\"Which status updates would have reassured you as a customer?\"</li> <li>\"How did teams balance technical details with business language?\"</li> </ul> <p>Pressure Response:</p> <ul> <li>\"How did the escalating pressure messages affect your decision-making?\"</li> <li>\"When did you feel most/least confident in your approach?\"</li> </ul>"},{"location":"day2/scenario2-debrief/","title":"Scenario 2 ~ Debrief","text":""},{"location":"day2/scenario2-debrief/#scenario-2-debrief-elements-you-can-use","title":"Scenario 2 Debrief Elements You Can Use:","text":"<p>From the Escalation Card:</p> <ul> <li>Partial success complexity - pipeline running but 40% data still missing</li> <li>CEO involvement - board meeting happened with incomplete data</li> <li>Multiple new stakeholders - Legal, Data Protection Officer, Marketing Director</li> <li>External pressures - Media inquiry, competitor actions, formal client complaints</li> <li>Complex decisions - technical vs. workaround, legal/compliance implications</li> </ul> <p>CEO Briefing Observations:</p> <ul> <li>Communication effectiveness - which teams translated technical issues clearly?</li> <li>Executive presence - how did teams handle direct CEO questioning?</li> <li>Decision confidence - who gave firm commitments vs. hedged their answers?</li> <li>Resource requests - what support did teams ask for?</li> </ul> <p>Cross-Team Comparison:</p> <ul> <li>Risk assessment approaches - legal, competitive, technical priorities</li> <li>Communication strategies - public vs. internal messaging decisions</li> <li>Timeline commitments - conservative vs. aggressive promises</li> <li>Escalation handling - how teams managed multiple senior stakeholders</li> </ul>"},{"location":"day2/scenario2-debrief/#rich-debrief-questions","title":"Rich Debrief Questions:","text":"<p>CEO Briefing Analysis:</p> <ul> <li>\"Which team's CEO briefing felt most/least confident?\"</li> <li>\"How did teams balance honesty about problems with leadership expectations?\"</li> <li>\"What made some technical explanations clearer than others?\"</li> </ul> <p>Decision-Making Under Pressure:</p> <ul> <li>\"How did teams handle the legal/compliance questions?\"</li> <li>\"Which risk assessments seemed most comprehensive?\"</li> <li>\"How did competitive pressure affect your communication decisions?\"</li> </ul> <p>Stakeholder Juggling:</p> <ul> <li>\"How did teams prioritize between CEO, Legal, Marketing, and technical needs?\"</li> <li>\"Which external pressures (media, competitors) influenced your approach most?\"</li> </ul>"},{"location":"day2/system-broke/","title":"When a system broke at work","text":""},{"location":"day2/system-broke/#overview","title":"Overview","text":"<p>This session transitions from Day 1's \"when things work\" to Day 2's \"when things break\" by collecting workplace failure experiences and establishing the foundational concepts of incident vs problem management. This creates the context for today's hands-on breaking and responding activities.</p>"},{"location":"day2/system-broke/#session-structure","title":"Session Structure","text":""},{"location":"day2/system-broke/#1-discussion-system-failure-stories","title":"1. Discussion: System Failure Stories","text":"<p>Learning Type: Discussion Format: Structured sharing with experience capture</p>"},{"location":"day2/system-broke/#sharing-structure","title":"Sharing Structure","text":"<p>Individual Reflection Think of a workplace example:</p> <ul> <li>System failure, data pipeline break, application outage</li> <li>Focus on the organizational response, not just the technical issue</li> </ul> <p>Round-Robin Sharing Each learner shares briefly (45-60 seconds each):</p> <ul> <li>What broke? (brief technical context)</li> <li>How did people find out? (detection method)</li> <li>Who got involved? (response team/escalation)</li> <li>How long to fix? (resolution time)</li> <li>What was learned? (if anything)</li> </ul>"},{"location":"day2/system-broke/#2-acquisition-incident-vs-problem-framework-8-minutes","title":"2. Acquisition: Incident vs Problem Framework (8 minutes)","text":"<p>Learning Type: Acquisition Format: Interactive presentation with real examples</p>"},{"location":"day2/system-broke/#core-definitions-3-minutes","title":"Core Definitions (3 minutes):","text":"<p>Incident:</p> <ul> <li>Definition: Unplanned interruption or reduction in quality of service</li> <li>Focus: Restore service as quickly as possible</li> <li>Timeframe: Minutes to hours</li> <li>Example: \"Data pipeline failed at 2am, sales dashboard showing yesterday's data\"</li> </ul> <p>Problem:</p> <ul> <li>Definition: Root cause of one or more incidents</li> <li>Focus: Prevent incidents from recurring</li> <li>Timeframe: Days to weeks</li> <li>Example: \"Why do our pipelines keep failing when the source system releases schema changes?\"</li> </ul>"},{"location":"day2/system-broke/#the-response-hierarchy-3-minutes","title":"The Response Hierarchy (3 minutes):","text":"<pre><code>INCIDENT MANAGEMENT           PROBLEM MANAGEMENT\n\u2193                            \u2193\nDetect \u2192 Respond \u2192 Restore   Analyze \u2192 Identify \u2192 Prevent\n\u2193                            \u2193\n\"Make it work now\"           \"Stop it happening again\"\n</code></pre> <p>Key Insight: You often need to do both simultaneously</p> <ul> <li>Immediate team: Fix the incident (restore service)</li> <li>Analysis team: Investigate the problem (prevent recurrence)</li> </ul>"},{"location":"day2/system-broke/#real-world-application-2-minutes","title":"Real-World Application (2 minutes):","text":"<p>Scenario Walkthrough:</p> <p>\"A data pipeline feeding the customer dashboard fails at 6am. Customers start calling at 8am saying they can't see their orders.\"</p>"},{"location":"day2/welcome-context/","title":"Day 2: Welcome &amp; Context Setting","text":""},{"location":"day2/welcome-context/#overview","title":"Overview","text":"<p>This session transitions from Day 1's \"when things work\" to Day 2's \"when things break\" by collecting workplace failure experiences and establishing the foundational concepts of incident vs problem management. This creates the context for today's hands-on breaking and responding activities.</p>"},{"location":"day2/welcome-context/#session-structure","title":"Session Structure","text":""},{"location":"day2/welcome-context/#1-discussion-system-failure-stories-workplace-edition-12-minutes","title":"1. Discussion: System Failure Stories - Workplace Edition (12 minutes)","text":"<p>Learning Type: Discussion Format: Structured sharing with experience capture</p>"},{"location":"day2/welcome-context/#sharing-structure-8-minutes","title":"Sharing Structure (8 minutes):","text":"<p>Individual Reflection (2 minutes): Think of a workplace example:</p> <ul> <li>System failure, data pipeline break, application outage</li> <li>Focus on the organizational response, not just the technical issue</li> </ul> <p>Round-Robin Sharing (6 minutes): Each learner shares briefly (45-60 seconds each):</p> <ul> <li>What broke? (brief technical context)</li> <li>How did people find out? (detection method)</li> <li>Who got involved? (response team/escalation)</li> <li>How long to fix? (resolution time)</li> <li>What was learned? (if anything)</li> </ul>"},{"location":"day2/welcome-context/#2-acquisition-incident-vs-problem-framework-8-minutes","title":"2. Acquisition: Incident vs Problem Framework (8 minutes)","text":"<p>Learning Type: Acquisition Format: Interactive presentation with real examples</p>"},{"location":"day2/welcome-context/#core-definitions-3-minutes","title":"Core Definitions (3 minutes):","text":"<p>Incident:</p> <ul> <li>Definition: Unplanned interruption or reduction in quality of service</li> <li>Focus: Restore service as quickly as possible</li> <li>Timeframe: Minutes to hours</li> <li>Example: \"Data pipeline failed at 2am, sales dashboard showing yesterday's data\"</li> </ul> <p>Problem:</p> <ul> <li>Definition: Root cause of one or more incidents</li> <li>Focus: Prevent incidents from recurring</li> <li>Timeframe: Days to weeks</li> <li>Example: \"Why do our pipelines keep failing when the source system releases schema changes?\"</li> </ul>"},{"location":"day2/welcome-context/#the-response-hierarchy-3-minutes","title":"The Response Hierarchy (3 minutes):","text":"<pre><code>INCIDENT MANAGEMENT           PROBLEM MANAGEMENT\n\u2193                            \u2193\nDetect \u2192 Respond \u2192 Restore   Analyze \u2192 Identify \u2192 Prevent\n\u2193                            \u2193\n\"Make it work now\"           \"Stop it happening again\"\n</code></pre> <p>Key Insight: You often need to do both simultaneously</p> <ul> <li>Immediate team: Fix the incident (restore service)</li> <li>Analysis team: Investigate the problem (prevent recurrence)</li> </ul>"},{"location":"day2/welcome-context/#real-world-application-2-minutes","title":"Real-World Application (2 minutes):","text":"<p>Scenario Walkthrough:</p> <p>\"A data pipeline feeding the customer dashboard fails at 6am. Customers start calling at 8am saying they can't see their orders.\"</p>"},{"location":"day3/governance/","title":"Day 3: Security &amp; Access Governance","text":""},{"location":"day3/governance/#overview","title":"Overview","text":"<p>This afternoon session shifts from data quality to data governance, focusing on security and access control. Through hands-on Lab 19 and structured discussion, learners explore how to balance data accessibility with security requirements, understanding governance as an operational discipline that enables rather than restricts business value.</p>"},{"location":"day3/governance/#session-structure","title":"Session Structure","text":""},{"location":"day3/governance/#phase-1-secure-data-access-lab-45-minutes","title":"Phase 1: Secure Data Access Lab (45 minutes)","text":""},{"location":"day3/governance/#lab-19-secure-data-access-in-microsoft-fabric-45-minutes","title":"Lab 19: Secure Data Access in Microsoft Fabric (45 minutes)","text":"<p>Microsoft Learn Lab: Secure data access in Microsoft Fabric</p> <p>Key Learning Focus for Today:</p> <p>While completing the lab, learners should pay attention to:</p> <ul> <li>Workspace permissions: Who can access what at the workspace level?</li> <li>Item-level security: Granular control over individual data assets</li> <li>OneLake data access roles: File-level security and data lake permissions</li> <li>Row-level security: Controlling access to specific data records</li> </ul> <p>Governance Mindset During Lab:</p> <p>As learners work through security configurations, encourage them to think:</p> <ul> <li>Business justification: Why would you restrict this access?</li> <li>User experience: How do security controls affect data consumers?</li> <li>Operational impact: What happens when security is too restrictive vs. too permissive?</li> </ul> <p>Facilitator Notes:</p> <ul> <li>Emphasize business context over technical configuration</li> <li>Connect to real scenarios: \"When would you use row-level security?\"</li> <li>Highlight trade-offs: \"How does security affect data accessibility?\"</li> <li>Time management: Focus on understanding concepts over perfect setup</li> </ul>"},{"location":"day3/governance/#phase-2-security-vs-accessibility-analysis-20-minutes","title":"Phase 2: Security vs. Accessibility Analysis (20 minutes)","text":""},{"location":"day3/governance/#governance-trade-offs-discussion-20-minutes","title":"Governance Trade-offs Discussion (20 minutes)","text":"<p>Setup (2 minutes):</p> <p>Facilitator Frame:</p> <ul> <li>\"You've just implemented multiple layers of data security\"</li> <li>\"Good governance requires balancing security with business needs\"</li> <li>\"Let's analyze the trade-offs and decision frameworks you just experienced\"</li> </ul>"},{"location":"day3/governance/#structured-analysis-15-minutes","title":"Structured Analysis (15 minutes):","text":"<p>Individual Reflection (5 minutes): Give learners this framework based on their lab experience:</p> <pre><code>SECURITY &amp; ACCESSIBILITY TRADE-OFFS\n\nWORKSPACE-LEVEL SECURITY:\n\u25a1 What business scenarios require workspace restrictions?\n\u25a1 How would this affect collaborative analytics work?\n\u25a1 What problems could overly restrictive workspace access cause?\n\nITEM-LEVEL SECURITY:\n\u25a1 When would you restrict access to specific datasets?\n\u25a1 How does this impact self-service analytics capabilities?\n\u25a1 What governance challenges arise with granular permissions?\n\nROW-LEVEL SECURITY:\n\u25a1 What business cases justify filtering data by user?\n\u25a1 How does this affect performance and user experience?\n\u25a1 What alternatives exist to row-level restrictions?\n\nGOVERNANCE DECISION FRAMEWORK:\n\u25a1 How do you decide what level of security to apply?\n\u25a1 Who should make these access control decisions?\n\u25a1 How do you balance compliance requirements with business agility?\n</code></pre> <p>Pair Discussion (5 minutes):</p> <ul> <li>Form pairs (mix from different lab experiences if possible)</li> <li>Share perspectives on security trade-offs observed</li> <li>Focus question: \"What's the biggest governance challenge you identified?\"</li> </ul> <p>Group Discussion (5 minutes): Facilitator-led synthesis:</p> <p>Key Questions:</p> <ul> <li>\"Where did you see the biggest tension between security and accessibility?\"</li> <li>\"How would these security controls affect data scientists or analysts in your organization?\"</li> <li>\"What governance decisions require business stakeholder input vs. technical judgment?\"</li> </ul> <p>Expected Insights to Highlight:</p> <ul> <li>Over-restriction kills innovation - too much security prevents valuable analysis</li> <li>Under-restriction creates compliance risk - inadequate security exposes sensitive data</li> <li>User experience matters - complex security creates workarounds and shadow IT</li> <li>Governance is about enabling business value safely, not just preventing access</li> </ul>"},{"location":"day3/governance/#bridge-to-governance-principles-3-minutes","title":"Bridge to Governance Principles (3 minutes):","text":"<p>Facilitator:</p> <ul> <li>\"Security controls are just one part of data governance\"</li> <li>\"Good governance creates frameworks for making these trade-off decisions consistently\"</li> <li>\"Next we'll investigate governance approaches that balance multiple competing priorities\"</li> </ul>"},{"location":"day3/governance/#phase-3-governance-approaches-investigation-10-minutes","title":"Phase 3: Governance Approaches Investigation (10 minutes)","text":""},{"location":"day3/governance/#governance-framework-research-10-minutes","title":"Governance Framework Research (10 minutes)","text":"<p>Setup (1 minute): Research Focus: \"Investigate how organizations structure governance to make consistent, business-aligned decisions about data access, quality, and usage\"</p> <p>Team Formation:</p> <ul> <li>Pairs or small groups (depending on class size)</li> <li>Each group investigates a different governance approach</li> </ul>"},{"location":"day3/governance/#governance-approaches-to-research","title":"Governance Approaches to Research:","text":"<p>Group 1: Data Mesh Governance</p> <ul> <li>Concept: Decentralized governance with domain ownership</li> <li>Research focus: How do domain teams balance autonomy with consistency?</li> <li>Key question: What governance decisions stay centralized vs. distributed?</li> </ul> <p>Group 2: Centralized Data Governance</p> <ul> <li>Concept: Enterprise-wide standards and controls</li> <li>Research focus: How do central teams ensure compliance while enabling innovation?</li> <li>Key question: What are the benefits and challenges of centralized control?</li> </ul> <p>Group 3: Federated Data Governance</p> <ul> <li>Concept: Shared governance across business units</li> <li>Research focus: How do organizations coordinate governance across multiple stakeholders?</li> <li>Key question: How do you resolve conflicts between different business needs?</li> </ul> <p>Group 4: Agile Data Governance</p> <ul> <li>Concept: Iterative, flexible governance that evolves with business needs</li> <li>Research focus: How do you maintain control while adapting quickly?</li> <li>Key question: What governance elements can be lightweight vs. must be rigorous?</li> </ul>"},{"location":"day3/governance/#research-questions-8-minutes","title":"Research Questions (8 minutes):","text":"<p>For each governance approach, investigate:</p> <p>Decision-Making Structure:</p> <ul> <li>Who makes governance decisions (access, quality standards, retention)?</li> <li>How are business priorities balanced with technical constraints?</li> <li>What escalation paths exist for governance conflicts?</li> </ul> <p>Implementation Practical:</p> <ul> <li>How would this approach handle the security scenarios from Lab 19?</li> <li>What tools and processes support this governance model?</li> <li>How does this approach scale with organizational growth?</li> </ul> <p>Business Alignment:</p> <ul> <li>How does this governance approach enable vs. restrict business value?</li> <li>What cultural changes are required for successful implementation?</li> <li>How do you measure governance effectiveness?</li> </ul>"},{"location":"day3/governance/#quick-sharing-1-minute","title":"Quick Sharing (1 minute):","text":"<p>Rapid insights: Each group shares one key finding (15 seconds each):</p> <ul> <li>\"This governance approach works best when...\"</li> <li>\"The biggest challenge with this model is...\"</li> <li>\"This would solve the problem of...\"</li> </ul>"},{"location":"day3/governance/#transition-to-session-4","title":"Transition to Session 4","text":""},{"location":"day3/governance/#bridge-to-governance-framework-design-1-minute","title":"Bridge to Governance Framework Design (1 minute):","text":"<p>Facilitator:</p> <ul> <li>\"You've seen different governance philosophies and experienced security trade-offs hands-on\"</li> <li>\"After the break, we'll design a practical governance framework\"</li> <li>\"We'll focus on creating decision-making processes that balance competing priorities\"</li> </ul>"},{"location":"day3/governance/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/governance/#pre-session-preparation","title":"Pre-Session Preparation","text":"<ul> <li>Complete Lab 19 yourself to understand security configuration options</li> <li>Research governance models briefly to guide groups if needed</li> <li>Prepare real-world examples of governance trade-offs to reference</li> </ul>"},{"location":"day3/governance/#managing-the-lab-phase","title":"Managing the Lab Phase","text":"<p>Emphasize Business Context:</p> <ul> <li>Connect security to scenarios: \"When would a sales team need row-level security?\"</li> <li>Highlight user impact: \"How would these restrictions affect a data analyst's daily work?\"</li> <li>Discuss implementation costs: \"What's the operational overhead of maintaining these permissions?\"</li> </ul> <p>Common Lab Challenges:</p> <ul> <li>Technical complexity: Focus on understanding concepts over perfect configuration</li> <li>Permission errors: Use as learning opportunities about governance hierarchy</li> <li>Time management: Ensure core security concepts are understood, skip advanced features if needed</li> </ul>"},{"location":"day3/governance/#facilitating-trade-offs-discussion","title":"Facilitating Trade-offs Discussion","text":"<p>Drawing Out Business Thinking:</p> <ul> <li>Use specific examples: \"In the row-level security setup, what business rule determined the filtering?\"</li> <li>Challenge assumptions: \"Is restricting access always the answer to data security?\"</li> <li>Connect to workplace: \"How do these decisions get made in your organization?\"</li> </ul> <p>Managing Governance Research:</p> <ul> <li>Keep research practical: Focus on decision-making processes, not theoretical frameworks</li> <li>Guide struggling groups: Suggest looking for case studies or implementation examples</li> <li>Time box tightly: 8 minutes is quick research - aim for key insights, not comprehensive analysis</li> </ul>"},{"location":"day3/governance/#expected-learning-outcomes","title":"Expected Learning Outcomes","text":"<p>Governance Mindset:</p> <ul> <li>Security as business enabler - not just risk prevention</li> <li>Trade-off thinking - balancing competing priorities systematically</li> <li>Stakeholder coordination - governance involves business and technical perspectives</li> </ul> <p>Practical Skills:</p> <ul> <li>Security implementation - hands-on experience with access controls</li> <li>Decision frameworks - understanding different approaches to governance choices</li> <li>Business communication - translating technical security to business impact</li> </ul>"},{"location":"day3/governance/#common-discussion-themes","title":"Common Discussion Themes","text":"<p>Security Complexity:</p> <ul> <li>Difficulty determining appropriate security levels</li> <li>User experience degradation with complex permissions</li> <li>Operational overhead of maintaining granular controls</li> </ul> <p>Governance Challenges:</p> <ul> <li>Balancing centralized control with business agility</li> <li>Coordinating decisions across multiple stakeholders</li> <li>Measuring effectiveness of governance processes</li> </ul> <p>Business Alignment:</p> <ul> <li>Ensuring governance supports rather than hinders business goals</li> <li>Managing conflicts between security and accessibility</li> <li>Evolving governance as business needs change</li> </ul>"},{"location":"day3/governance/#connection-to-ksbs","title":"Connection to KSBs","text":"<ul> <li>S13: Use data systems securely to meet requirements and in line with organizational procedures</li> <li>K10: Concepts of data governance, including regulatory requirements, data privacy, security</li> <li>S22: Develop collaborative relationships with stakeholders (business vs. technical needs)</li> <li>K13: Implications of security, scalability, compliance regarding local, remote or distributed solutions</li> <li>B2: Works collaboratively with stakeholders to achieve common goals</li> </ul>"},{"location":"day3/governance/#time-management-tips","title":"Time Management Tips","text":"<ul> <li>Lab phase: Monitor progress, assist with conceptual understanding over technical details</li> <li>Discussion phase: Use structured reflection to maximize learning from lab experience</li> <li>Research phase: Encourage rapid insight gathering over comprehensive research</li> <li>Keep transitions crisp: Each phase builds toward practical governance framework design</li> </ul>"},{"location":"day3/medallion-architecture/","title":"Day 3: Medallion Architecture &amp; Quality Patterns","text":""},{"location":"day3/medallion-architecture/#overview","title":"Overview","text":"<p>This session combines hands-on experience with medallion architecture (Lab 3b) and structured analysis of how quality patterns are implemented in data architecture. Learners see how the DMBOK quality dimensions they studied earlier are addressed through bronze, silver, and gold layer design, then investigate modern quality tools and approaches.</p>"},{"location":"day3/medallion-architecture/#session-structure","title":"Session Structure","text":""},{"location":"day3/medallion-architecture/#phase-1-medallion-architecture-lab-45-minutes","title":"Phase 1: Medallion Architecture Lab (45 minutes)","text":""},{"location":"day3/medallion-architecture/#lab-3b-create-a-medallion-architecture-45-minutes","title":"Lab 3b: Create a Medallion Architecture (45 minutes)","text":"<p>Microsoft Learn Lab: Create a medallion architecture in a Fabric lakehouse</p> <p>Key Learning Focus for Today:</p> <p>While completing the lab, learners should pay attention to:</p> <ul> <li>Bronze layer: Raw data ingestion - what quality issues exist here?</li> <li>Silver layer: Data cleaning and validation - what quality improvements happen?</li> <li>Gold layer: Business-ready data - what quality standards are enforced?</li> </ul> <p>Facilitator Notes:</p> <ul> <li>Circulate during lab to highlight quality aspects as learners work</li> <li>Point out quality transformations: \"Notice how the silver layer handles missing values\"</li> <li>Connect to DMBOK: \"This validation step addresses which quality dimension?\"</li> <li>Time management: Keep lab moving - focus on completion over perfection</li> </ul>"},{"location":"day3/medallion-architecture/#phase-2-quality-patterns-analysis-20-minutes","title":"Phase 2: Quality Patterns Analysis (20 minutes)","text":""},{"location":"day3/medallion-architecture/#medallion-quality-mapping-discussion-20-minutes","title":"Medallion Quality Mapping Discussion (20 minutes)","text":"<p>Setup (2 minutes):</p> <p>Facilitator Frame:</p> <ul> <li>\"You've just built a medallion architecture hands-on\"</li> <li>\"Now let's analyze how this pattern addresses the DMBOK quality dimensions you studied earlier\"</li> <li>\"We'll map specific quality improvements to each layer\"</li> </ul>"},{"location":"day3/medallion-architecture/#structured-quality-analysis-15-minutes","title":"Structured Quality Analysis (15 minutes):","text":"<p>Individual Reflection (5 minutes): Give learners this framework to complete based on their lab experience:</p> <pre><code>MEDALLION QUALITY MAPPING\n\nBRONZE LAYER - Raw Data:\n\u25a1 What quality issues did you observe in the raw data?\n\u25a1 Which DMBOK dimensions had problems here?\n\u25a1 What was preserved \"as-is\" and why?\n\nSILVER LAYER - Cleaned Data:  \n\u25a1 What quality transformations happened in silver?\n\u25a1 Which DMBOK dimensions were improved?\n\u25a1 What validation rules were applied?\n\nGOLD LAYER - Business-Ready Data:\n\u25a1 What additional quality standards were enforced?\n\u25a1 Which DMBOK dimensions reached \"production quality\"?\n\u25a1 What business rules ensured fitness for purpose?\n\nQUALITY PATTERNS OBSERVED:\n\u25a1 How does this layered approach handle different quality requirements?\n\u25a1 What are the trade-offs between layers?\n\u25a1 Where would you add additional quality checks?\n</code></pre> <p>Pair Discussion (5 minutes):</p> <ul> <li>Share findings with a partner (different from morning DMBOK pairs)</li> <li>Compare observations about quality transformations</li> <li>Identify one quality improvement they found most effective</li> </ul> <p>Group Discussion (5 minutes): Facilitator-led analysis:</p> <p>Key Questions:</p> <ul> <li>\"Which DMBOK dimensions were most improved by the silver layer transformations?\"</li> <li>\"What quality trade-offs did you notice between preserving raw data and cleaning it?\"</li> <li>\"Where would you add additional quality checks if this was a production system?\"</li> </ul> <p>Expected Insights to Highlight:</p> <ul> <li>Bronze preserves completeness but may sacrifice accuracy</li> <li>Silver improves accuracy and consistency through validation and standardization</li> <li>Gold ensures fitness for purpose through business rule application</li> <li>Different layers serve different quality needs - exploration vs. reporting vs. analytics</li> </ul>"},{"location":"day3/medallion-architecture/#bridge-to-quality-tools-3-minutes","title":"Bridge to Quality Tools (3 minutes):","text":"<p>Facilitator:</p> <ul> <li>\"The medallion pattern gives you architecture for quality improvement\"</li> <li>\"But you need tools to implement and monitor these quality standards\"</li> <li>\"Next we'll investigate modern data quality tools that can automate what you just did manually\"</li> </ul>"},{"location":"day3/medallion-architecture/#phase-3-quality-tools-investigation-10-minutes","title":"Phase 3: Quality Tools Investigation (10 minutes)","text":""},{"location":"day3/medallion-architecture/#modern-data-quality-tools-research-10-minutes","title":"Modern Data Quality Tools Research (10 minutes)","text":"<p>Setup (1 minute):</p> <p>Research Focus:</p> <p>\"Investigate tools that could automate the quality checks you just implemented in the medallion architecture\"</p> <p>Team Formation:</p> <ul> <li>Pairs or individuals (depending on class size)</li> <li>Each pair/person takes a different tool category</li> </ul>"},{"location":"day3/medallion-architecture/#tool-categories-to-research","title":"Tool Categories to Research:","text":"<p>Category 1: Data Quality Frameworks</p> <ul> <li>Great Expectations: Python-based data validation framework</li> <li>dbt tests: Built-in and custom data quality tests</li> <li>Focus: How do these tools implement quality checks similar to your silver layer transformations?</li> </ul> <p>Category 2: Cloud-Native Quality Tools</p> <ul> <li>Azure Data Quality (in Fabric/Synapse): Built-in quality monitoring</li> <li>AWS Glue DataBrew: Visual data quality profiling</li> <li>Focus: How do cloud platforms handle quality monitoring and alerting?</li> </ul> <p>Category 3: Quality Monitoring &amp; Observability</p> <ul> <li>Monte Carlo: Data observability and quality monitoring</li> <li>Datadog Data Streams: Real-time quality monitoring</li> <li>Focus: How do these tools detect quality issues automatically?</li> </ul> <p>Category 4: Open Source Quality Solutions</p> <ul> <li>Apache Griffin: Data quality platform</li> <li>DataHub: Data discovery with quality insights</li> <li>Focus: How do open source tools provide cost-effective quality solutions?</li> </ul>"},{"location":"day3/medallion-architecture/#research-questions-8-minutes","title":"Research Questions (8 minutes):","text":"<p>For each tool category, investigate:</p> <p>Implementation Approach:</p> <ul> <li>How would this tool integrate with a medallion architecture?</li> <li>What quality checks could be automated?</li> <li>How does it compare to manual validation?</li> </ul> <p>Quality Dimensions Covered:</p> <ul> <li>Which DMBOK dimensions does this tool address best?</li> <li>What types of quality issues would it catch/miss?</li> <li>How does it handle quality monitoring vs. quality improvement?</li> </ul> <p>Practical Considerations:</p> <ul> <li>What skills/resources needed for implementation?</li> <li>How does it fit with Microsoft Fabric/Azure ecosystem?</li> <li>What would be the first quality check you'd implement?</li> </ul>"},{"location":"day3/medallion-architecture/#quick-sharing-1-minute","title":"Quick Sharing (1 minute):","text":"<p>Rapid insights: Each pair/person shares one key finding (15 seconds each):</p> <ul> <li>\"The most interesting capability we found was...\"</li> <li>\"This tool would solve the problem of...\"</li> <li>\"The biggest implementation challenge would be...\"</li> </ul>"},{"location":"day3/medallion-architecture/#transition-to-afternoon","title":"Transition to Afternoon","text":""},{"location":"day3/medallion-architecture/#bridge-to-security-governance-1-minute","title":"Bridge to Security &amp; Governance (1 minute):","text":"<p>Facilitator:</p> <ul> <li>\"This morning you've focused on data quality - ensuring data meets standards\"</li> <li>\"This afternoon we'll explore governance - ensuring the right people can access the right data safely\"</li> <li>\"Quality and governance work together - high-quality data that's not properly secured and governed can't deliver business value\"</li> </ul>"},{"location":"day3/medallion-architecture/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/medallion-architecture/#pre-session-preparation","title":"Pre-Session Preparation:","text":"<ul> <li>Complete Lab 3b yourself to understand quality transformation points</li> <li>Research quality tools briefly to guide learners if they struggle</li> <li>Prepare quality examples from the lab to highlight during discussion</li> </ul>"},{"location":"day3/medallion-architecture/#managing-the-lab-phase","title":"Managing the Lab Phase:","text":"<p>Keep Quality Focus:</p> <ul> <li>Point out quality improvements as they happen: \"Notice how this addresses accuracy\"</li> <li>Connect to DMBOK: Reference morning's framework throughout</li> <li>Time management: Ensure lab completion within 45 minutes</li> </ul> <p>Common Lab Issues:</p> <ul> <li>If learners struggle with technical steps: Focus on understanding the quality patterns rather than perfect execution</li> <li>If lab runs long: Guide them to complete bronze and silver layers, skip gold if necessary</li> <li>If learners finish early: Have them analyze additional quality improvement opportunities</li> </ul>"},{"location":"day3/medallion-architecture/#facilitating-quality-discussion","title":"Facilitating Quality Discussion:","text":"<p>Drawing Out Insights:</p> <ul> <li>Use specific examples from the lab: \"What happened to the invalid dates in silver layer?\"</li> <li>Connect to workplace: \"How does this compare to quality processes in your organization?\"</li> <li>Highlight trade-offs: \"Why keep raw data in bronze if it has quality issues?\"</li> </ul> <p>Managing Tools Research:</p> <ul> <li>Keep research focused on practical implementation rather than deep technical details</li> <li>Guide struggling researchers: Suggest looking for case studies or getting-started guides</li> <li>Connect to medallion: \"How would this tool fit into the architecture you just built?\"</li> </ul>"},{"location":"day3/medallion-architecture/#expected-learning-outcomes","title":"Expected Learning Outcomes","text":"<p>Practical Understanding:</p> <ul> <li>Quality patterns in action - seeing DMBOK dimensions addressed through architecture</li> <li>Tool awareness - knowing what modern quality solutions exist</li> <li>Implementation thinking - connecting theory to practical application</li> </ul> <p>Quality Mindset:</p> <ul> <li>Layered quality approach - different standards for different purposes</li> <li>Automation possibilities - tools can implement quality checks at scale</li> <li>Integration considerations - quality tools work within broader data architecture</li> </ul>"},{"location":"day3/medallion-architecture/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K4: Frameworks for data quality (DMBOK applied to architecture)</li> <li>S26: Identify data quality metrics and track them (quality tools investigation)</li> <li>S6: Systematically clean, validate, and describe data at ETL stages (medallion lab)</li> <li>S25: Assess gaps in existing tools and technologies (quality tools research)</li> <li>B3: Quality focus that promotes continuous improvement (throughout session)</li> </ul>"},{"location":"day3/medallion-architecture/#time-management-tips","title":"Time Management Tips:","text":"<ul> <li>Lab phase: Use timer, keep groups moving together</li> <li>Discussion phase: Limit individual reflection to 5 minutes max</li> <li>Research phase: Encourage \"good enough\" research over perfect analysis</li> <li>Sharing: Use strict time limits for rapid knowledge sharing</li> </ul>"},{"location":"day3/quality-foundation/","title":"Day 3: Welcome &amp; Quality Foundation","text":""},{"location":"day3/quality-foundation/#overview","title":"Overview","text":"<p>This opening session transitions from Day 2's \"when systems break\" to Day 3's \"when systems work but data doesn't meet standards.\" Through discussion and concept introduction, learners explore what quality means in operational contexts and why data quality is a different type of operational challenge requiring similar systematic approaches.</p>"},{"location":"day3/quality-foundation/#session-structure","title":"Session Structure","text":""},{"location":"day3/quality-foundation/#discussion-what-does-good-data-mean","title":"Discussion: What Does \"Good Data\" Mean?","text":""},{"location":"day3/quality-foundation/#opening-frame","title":"Opening Frame","text":"<p>Facilitator Introduction:</p> <ul> <li>\"Yesterday we focused on incident response - when technical systems break\"</li> <li>\"Today we're exploring a different operational challenge: when systems work fine, but the data doesn't meet our standards\"</li> <li>\"This is about quality and governance - making sure data is 'good enough' for its intended use\"</li> </ul>"},{"location":"day3/quality-foundation/#individual-reflection","title":"Individual Reflection","text":"<p>Question for learners: \"Think about data you use in your work - what makes data 'good' versus 'bad'? What frustrates you most about poor quality data?\"</p> <p>Reflection prompts:</p> <ul> <li>Think of a time when you couldn't trust data you were given</li> <li>What made you lose confidence in a report or dashboard?</li> <li>When have you had to spend time cleaning or fixing data before you could use it?</li> </ul>"},{"location":"day3/quality-foundation/#group-sharing","title":"Group Sharing","text":"<p>Structured sharing:</p> <p>Each learner shares one example (30-45 seconds):</p> <ul> <li>\"Bad data for me is when...\"</li> <li>\"I lose confidence in data when...\"</li> <li>\"The most frustrating data quality issue I face is...\"</li> </ul> <p>Facilitator role:</p> <ul> <li>Listen for patterns and capture themes on screen/flip chart</li> <li>Group similar responses: accuracy issues, completeness problems, timeliness concerns</li> <li>Note the business impact - not just technical problems but work impact</li> </ul>"},{"location":"day3/quality-foundation/#pattern-recognition","title":"Pattern Recognition","text":"<p>Facilitator synthesis:</p> <ul> <li>\"I'm hearing themes around accuracy, completeness, timing...\"</li> <li>\"Notice how quality problems affect your ability to do your job\"</li> <li>\"These aren't just technical issues - they have real business consequences\"</li> </ul> <p>Expected themes to emerge:</p> <ul> <li>Accuracy: Wrong numbers, outdated information</li> <li>Completeness: Missing records, partial data</li> <li>Consistency: Same thing measured differently across systems</li> <li>Timeliness: Data arriving too late to be useful</li> <li>Relevance: Right data but not quite what's needed</li> <li>Trust: Uncertainty about whether data can be relied upon</li> </ul>"},{"location":"day3/quality-foundation/#acquisition-data-quality-as-operational-challenge-8-minutes","title":"Acquisition: Data Quality as Operational Challenge (8 minutes)","text":""},{"location":"day3/quality-foundation/#reframe-quality-as-operations-3-minutes","title":"Reframe Quality as Operations (3 minutes):","text":"<p>Connect to Yesterday:</p> <ul> <li>\"Yesterday you handled incidents - systems failing completely\"</li> <li>\"Today's challenge is more subtle: systems work, but output isn't meeting standards\"</li> <li>\"This requires the same systematic thinking, but different detection and response approaches\"</li> </ul> <p>Quality vs. Incidents:</p> <pre><code>SYSTEM INCIDENTS          vs.    DATA QUALITY ISSUES\n\u2193                                \u2193\nPipeline stops working           Pipeline produces unreliable data\nClear failure signal           Subtle degradation signal\nImmediate impact               Gradual impact\nTechnical fix needed           Process/standards fix needed\n</code></pre>"},{"location":"day3/quality-foundation/#why-quality-matters-operationally-3-minutes","title":"Why Quality Matters Operationally (3 minutes):","text":"<p>Business Impact Examples:</p> <ul> <li>Decision-making: Executives making strategic decisions based on inaccurate data</li> <li>Customer experience: Personalization systems using outdated customer preferences</li> <li>Compliance: Regulatory reports with incomplete or inconsistent data</li> <li>Efficiency: Teams spending time validating data instead of analyzing it</li> </ul> <p>The Quality Challenge:</p> <ul> <li>Detection is harder: Systems appear to be working fine</li> <li>Standards are subjective: \"Good enough\" varies by use case</li> <li>Prevention requires governance: Processes, policies, and culture change</li> <li>Impact is often delayed: Problems discovered weeks or months later</li> </ul>"},{"location":"day3/quality-foundation/#operational-quality-dimensions","title":"Operational Quality Dimensions","text":"<p>Quick introduction to key concepts:</p> <ul> <li>Fitness for purpose: Data quality depends on how it will be used</li> <li>Quality standards: Need clear, measurable criteria</li> <li>Quality monitoring: Ongoing measurement, not one-time assessment</li> <li>Quality improvement: Systematic processes for addressing issues</li> </ul> <p>Preview today's approach:</p> <ul> <li>\"We'll define quality standards using proven frameworks\"</li> <li>\"See how to implement quality patterns in data architectures\"</li> <li>\"Practice balancing quality requirements with practical constraints\"</li> </ul>"},{"location":"day3/quality-foundation/#transition-to-next-session","title":"Transition to Next Session","text":""},{"location":"day3/quality-foundation/#bridge-to-dmbok-investigation","title":"Bridge to DMBOK Investigation","text":"<p>Facilitator:</p> <ul> <li>\"You've identified quality challenges from your experience\"</li> <li>\"Now we'll explore a professional framework for thinking about data quality\"</li> <li>\"The DMBOK (Data Management Body of Knowledge) provides six specific dimensions for measuring and improving data quality\"</li> <li>\"This will give you systematic language for the quality issues you've experienced\"</li> </ul>"},{"location":"day3/quality-foundation/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/quality-foundation/#managing-the-discussion","title":"Managing the Discussion","text":"<p>If learners struggle to think of examples:</p> <ul> <li>\"Think about spreadsheets you've received that had problems\"</li> <li>\"Consider dashboards or reports where you questioned the numbers\"</li> <li>\"What about customer data that seemed outdated or wrong?\"</li> </ul> <p>If examples get too technical:</p> <ul> <li>\"Focus on the impact on your work, not the technical cause\"</li> <li>\"How did the quality problem affect your decision-making?\"</li> </ul> <p>If discussion goes long:</p> <ul> <li>Capture themes quickly and move to acquisition</li> <li>\"I'm hearing great examples - let's put framework around these experiences\"</li> </ul>"},{"location":"day3/quality-foundation/#common-quality-issues-to-expect","title":"Common Quality Issues to Expect:","text":"<p>Accuracy Problems:</p> <ul> <li>Wrong customer contact information</li> <li>Incorrect financial calculations</li> <li>Outdated product pricing</li> </ul> <p>Completeness Issues:</p> <ul> <li>Missing customer records</li> <li>Partial transaction data</li> <li>Incomplete survey responses</li> </ul> <p>Consistency Problems:</p> <ul> <li>Different systems showing different customer names</li> <li>Conflicting sales figures across reports</li> <li>Varying date formats</li> </ul> <p>Timeliness Issues:</p> <ul> <li>Reports showing yesterday's data for real-time decisions</li> <li>Batch processing delays affecting morning meetings</li> <li>Historical data needed but not available</li> </ul>"},{"location":"day3/quality-foundation/#key-insights-to-highlight","title":"Key Insights to Highlight:","text":"<p>Quality is Contextual:</p> <ul> <li>Data that's \"good enough\" for one purpose may be inadequate for another</li> <li>Quality requirements change based on business criticality</li> <li>Perfect data quality is often neither necessary nor cost-effective</li> </ul> <p>Quality is Operational:</p> <ul> <li>Requires ongoing monitoring, not just initial assessment</li> <li>Needs clear standards and measurement processes</li> <li>Benefits from the same systematic thinking as incident response</li> </ul> <p>Quality is Organizational:</p> <ul> <li>Often involves multiple teams and systems</li> <li>Requires governance processes and clear accountability</li> <li>Culture and processes matter as much as technology</li> </ul>"},{"location":"day3/quality-foundation/#expected-outcomes","title":"Expected Outcomes:","text":"<p>By the end of this session, learners should:</p> <ol> <li>Connect personal experience with formal data quality concepts</li> <li>Understand quality as operational challenge requiring systematic approaches</li> <li>Appreciate the business impact of data quality issues</li> <li>Be prepared to explore structured frameworks for defining and measuring quality</li> </ol>"},{"location":"day3/quality-foundation/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K4: Frameworks for data quality covering dimensions such as accuracy, completeness, consistency</li> <li>S26: Identify data quality metrics and track them to ensure quality, accuracy and reliability</li> <li>K5: The inherent risks of data such as incomplete data, ethical data sources</li> <li>B3: Quality focus that promotes continuous improvement</li> </ul>"},{"location":"day3/quality-patterns/","title":"Day 3: Quality Patterns","text":""},{"location":"day3/quality-patterns/#overview","title":"Overview","text":"<p>This session combines hands-on experience with medallion architecture (Lab 3b) and structured analysis of how quality patterns are implemented in data architecture. Learners see how the DMBOK quality dimensions they studied earlier are addressed through bronze, silver, and gold layer design, then investigate modern quality tools and approaches.</p> <p>Key Learning Focus for Today:</p> <p>While completing the lab, learners should pay attention to:</p> <ul> <li>Bronze layer: Raw data ingestion - what quality issues exist here?</li> <li>Silver layer: Data cleaning and validation - what quality improvements happen?</li> <li>Gold layer: Business-ready data - what quality standards are enforced?</li> </ul> <p>Facilitator Notes:</p> <ul> <li>Circulate during lab to highlight quality aspects as learners work</li> <li>Point out quality transformations: \"Notice how the silver layer handles missing values\"</li> <li>Connect to DMBOK: \"This validation step addresses which quality dimension?\"</li> <li>Time management: Keep lab moving - focus on completion over perfection</li> </ul>"},{"location":"day3/quality-patterns/#phase-2-quality-patterns-analysis-20-minutes","title":"Phase 2: Quality Patterns Analysis (20 minutes)","text":""},{"location":"day3/quality-patterns/#medallion-quality-mapping-discussion-20-minutes","title":"Medallion Quality Mapping Discussion (20 minutes)","text":"<p>Setup (2 minutes):</p> <p>Facilitator Frame:</p> <ul> <li>\"You've just built a medallion architecture hands-on\"</li> <li>\"Now let's analyze how this pattern addresses the DMBOK quality dimensions you studied earlier\"</li> <li>\"We'll map specific quality improvements to each layer\"</li> </ul>"},{"location":"day3/quality-patterns/#structured-quality-analysis-15-minutes","title":"Structured Quality Analysis (15 minutes):","text":"<p>Individual Reflection (5 minutes): Give learners this framework to complete based on their lab experience:</p> <pre><code>MEDALLION QUALITY MAPPING\n\nBRONZE LAYER - Raw Data:\n\u25a1 What quality issues did you observe in the raw data?\n\u25a1 Which DMBOK dimensions had problems here?\n\u25a1 What was preserved \"as-is\" and why?\n\nSILVER LAYER - Cleaned Data:  \n\u25a1 What quality transformations happened in silver?\n\u25a1 Which DMBOK dimensions were improved?\n\u25a1 What validation rules were applied?\n\nGOLD LAYER - Business-Ready Data:\n\u25a1 What additional quality standards were enforced?\n\u25a1 Which DMBOK dimensions reached \"production quality\"?\n\u25a1 What business rules ensured fitness for purpose?\n\nQUALITY PATTERNS OBSERVED:\n\u25a1 How does this layered approach handle different quality requirements?\n\u25a1 What are the trade-offs between layers?\n\u25a1 Where would you add additional quality checks?\n</code></pre> <p>Pair Discussion (5 minutes): - Share findings with a partner (different from morning DMBOK pairs) - Compare observations about quality transformations - Identify one quality improvement they found most effective</p> <p>Group Discussion (5 minutes): Facilitator-led analysis:</p> <p>Key Questions: - \"Which DMBOK dimensions were most improved by the silver layer transformations?\" - \"What quality trade-offs did you notice between preserving raw data and cleaning it?\" - \"Where would you add additional quality checks if this was a production system?\"</p> <p>Expected Insights to Highlight: - Bronze preserves completeness but may sacrifice accuracy - Silver improves accuracy and consistency through validation and standardization - Gold ensures fitness for purpose through business rule application - Different layers serve different quality needs - exploration vs. reporting vs. analytics</p>"},{"location":"day3/quality-patterns/#bridge-to-quality-tools-3-minutes","title":"Bridge to Quality Tools (3 minutes):","text":"<p>Facilitator: - \"The medallion pattern gives you architecture for quality improvement\" - \"But you need tools to implement and monitor these quality standards\" - \"Next we'll investigate modern data quality tools that can automate what you just did manually\"</p>"},{"location":"day3/quality-tools-trainer/","title":"Quality Tools Investigation","text":""},{"location":"day3/quality-tools-trainer/#modern-data-quality-tools-research","title":"Modern Data Quality Tools Research","text":"<p>Research Focus:</p> <p>\"Investigate tools that could automate the quality checks you just implemented in the medallion architecture\"</p> <p>Team Formation:</p> <ul> <li>Pairs or individuals (depending on class size)</li> <li>Each pair/person takes a different tool category</li> </ul>"},{"location":"day3/quality-tools-trainer/#tool-categories-to-research","title":"Tool Categories to Research","text":""},{"location":"day3/quality-tools-trainer/#group-1-data-quality-frameworks","title":"Group 1: Data Quality Frameworks","text":"<ul> <li>Great Expectations: Python-based data validation framework</li> <li>dbt tests: Built-in and custom data quality tests</li> <li>Focus: How do these tools implement quality checks similar to your silver layer transformations?</li> </ul>"},{"location":"day3/quality-tools-trainer/#group-2-cloud-native-quality-tools","title":"Group 2: Cloud-Native Quality Tools","text":"<ul> <li>Azure Data Quality (in Fabric/Synapse): Built-in quality monitoring</li> <li>AWS Glue DataBrew: Visual data quality profiling</li> <li>Focus: How do cloud platforms handle quality monitoring and alerting?</li> </ul>"},{"location":"day3/quality-tools-trainer/#group-3-quality-monitoring-observability","title":"Group 3: Quality Monitoring &amp; Observability","text":"<ul> <li>Monte Carlo: Data observability and quality monitoring</li> <li>Datadog Data Streams: Real-time quality monitoring</li> <li>Focus: How do these tools detect quality issues automatically?</li> </ul>"},{"location":"day3/quality-tools-trainer/#group-4-open-source-quality-solutions","title":"Group 4: Open Source Quality Solutions","text":"<ul> <li>Apache Griffin: Data quality platform</li> <li>DataHub: Data discovery with quality insights</li> <li>Focus: How do open source tools provide cost-effective quality solutions?</li> </ul>"},{"location":"day3/quality-tools-trainer/#research-questions","title":"Research Questions","text":"<p>For each tool category, investigate:</p>"},{"location":"day3/quality-tools-trainer/#implementation-approach","title":"Implementation Approach","text":"<ul> <li>How would this tool integrate with a medallion architecture?</li> <li>What quality checks could be automated?</li> <li>How does it compare to manual validation?</li> </ul>"},{"location":"day3/quality-tools-trainer/#quality-dimensions-covered","title":"Quality Dimensions Covered","text":"<ul> <li>Which DMBOK dimensions does this tool address best?</li> <li>What types of quality issues would it catch/miss?</li> <li>How does it handle quality monitoring vs. quality improvement?</li> </ul>"},{"location":"day3/quality-tools-trainer/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>What skills/resources needed for implementation?</li> <li>How does it fit with Microsoft Fabric/Azure ecosystem?</li> <li>What would be the first quality check you'd implement?</li> </ul>"},{"location":"day3/quality-tools-trainer/#report-back","title":"Report Back","text":"<p>Rapid insights: Each pair/person shares one key finding (15 seconds each):</p> <ul> <li>\"The most interesting capability we found was...\"</li> <li>\"This tool would solve the problem of...\"</li> <li>\"The biggest implementation challenge would be...\"</li> </ul>"},{"location":"day3/quality-tools-trainer/#transition-to-afternoon","title":"Transition to Afternoon","text":""},{"location":"day3/quality-tools-trainer/#bridge-to-security-governance","title":"Bridge to Security &amp; Governance","text":"<p>Facilitator:</p> <ul> <li>\"This morning you've focused on data quality - ensuring data meets standards\"</li> <li>\"This afternoon we'll explore governance - ensuring the right people can access the right data safely\"</li> <li>\"Quality and governance work together - high-quality data that's not properly secured and governed can't deliver business value\"</li> </ul>"},{"location":"day3/quality-tools-trainer/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/quality-tools-trainer/#expected-learning-outcomes","title":"Expected Learning Outcomes","text":"<p>Practical Understanding:</p> <ul> <li>Quality patterns in action - seeing DMBOK dimensions addressed through architecture</li> <li>Tool awareness - knowing what modern quality solutions exist</li> <li>Implementation thinking - connecting theory to practical application</li> </ul> <p>Quality Mindset:</p> <ul> <li>Layered quality approach - different standards for different purposes</li> <li>Automation possibilities - tools can implement quality checks at scale</li> <li>Integration considerations - quality tools work within broader data architecture</li> </ul>"},{"location":"day3/quality-tools/","title":"Quality Tools Investigation","text":""},{"location":"day3/quality-tools/#tool-categories-to-research","title":"Tool Categories to Research","text":""},{"location":"day3/quality-tools/#group-1-data-quality-frameworks","title":"Group 1: Data Quality Frameworks","text":"<ul> <li>Great Expectations: Python-based data validation framework</li> <li>dbt tests: Built-in and custom data quality tests</li> <li>Focus: How do these tools implement quality checks similar to your silver layer transformations?</li> </ul>"},{"location":"day3/quality-tools/#group-2-cloud-native-quality-tools","title":"Group 2: Cloud-Native Quality Tools","text":"<ul> <li>Azure Data Quality (in Fabric/Synapse): Built-in quality monitoring</li> <li>AWS Glue DataBrew: Visual data quality profiling</li> <li>Focus: How do cloud platforms handle quality monitoring and alerting?</li> </ul>"},{"location":"day3/quality-tools/#group-3-quality-monitoring-observability","title":"Group 3: Quality Monitoring &amp; Observability","text":"<ul> <li>Monte Carlo: Data observability and quality monitoring</li> <li>Datadog Data Streams: Real-time quality monitoring</li> <li>Focus: How do these tools detect quality issues automatically?</li> </ul>"},{"location":"day3/quality-tools/#group-4-open-source-quality-solutions","title":"Group 4: Open Source Quality Solutions","text":"<ul> <li>Apache Griffin: Data quality platform</li> <li>DataHub: Data discovery with quality insights</li> <li>Focus: How do open source tools provide cost-effective quality solutions?</li> </ul>"},{"location":"day3/quality-tools/#research-guidelines","title":"Research Guidelines","text":"<p>For each tool category, investigate:</p>"},{"location":"day3/quality-tools/#implementation-approach","title":"Implementation Approach","text":"<ul> <li>How would this tool integrate with a medallion architecture?</li> <li>What quality checks could be automated?</li> <li>How does it compare to manual validation?</li> </ul>"},{"location":"day3/quality-tools/#quality-dimensions-covered","title":"Quality Dimensions Covered","text":"<ul> <li>Which DMBOK dimensions does this tool address best?</li> <li>What types of quality issues would it catch/miss?</li> <li>How does it handle quality monitoring vs. quality improvement?</li> </ul>"},{"location":"day3/quality-tools/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>What skills/resources needed for implementation?</li> <li>How does it fit with Microsoft Fabric/Azure ecosystem?</li> <li>What would be the first quality check you'd implement?</li> </ul>"},{"location":"day3/quality-tools/#teach-back","title":"Teach Back","text":"<ul> <li>Each group presents what they have found.</li> </ul>"},{"location":"day4/overview/","title":"Day 4 ~ Instructions &amp; Overview","text":""},{"location":"day4/overview/#improvement-value","title":"Improvement &amp; Value.","text":"<p>On day 4, you will work in teams to simulate a real-world DataOps project using agile methods. Your mission is to improve a data pipeline over three focused sprints - each targeting a different operational goal.</p> <p>By the end of the day, you'll have:</p> <ul> <li>Planned and executed a small Scrum project</li> <li>Made a pipeline observable, resilient, and governed</li> <li>Practiced teamwork, monitoring, and incident response</li> <li>Presented your work and reflected on what you've learned</li> </ul>"},{"location":"day4/overview/#fyi-a-complete-pipeline-example-is-available-in-a-github-repo","title":"FYI ~ A complete pipeline example is available in a GitHub repo","text":"<ul> <li>To view the Jupyter Notebook in html format - Click here</li> <li>To view the Jupyter Notebook in the GitHub repo - Click here</li> <li>For instructions on how to run the notebook in your VM - Click here</li> </ul>"},{"location":"day4/overview/#how-the-day-works","title":"How the Day Works","text":"<p>We'll use a mini Scrum structure with three timeboxed sprints:</p> Sprint Theme Focus 1 Make it Observable Logging, monitoring, visibility 2 Make it Resilient Error handling, alerts, recoverability 3 Make it Governed Validation, versioning, improvements <p>Each sprint includes:</p> <ul> <li>Planning: Quickly decide what to do and who will do it</li> <li>Execution: Work as a team to build and improve</li> <li>Stand-Up: Share progress, blockers, next steps</li> </ul> <p>And at the end of the 3 sprints:</p> <ul> <li>Review: Demo your pipeline and explain decisions</li> <li>Retrospective: Reflect on what went well and what to improve</li> </ul>"},{"location":"day4/overview/#working-in-teams","title":"Working in Teams","text":"<ul> <li>You'll work in groups of 3-5 learners.</li> <li>Just like in Scrum ~ there are no allocated roles.</li> <li>You can work as a team, or individually.</li> <li>You can build on the work of others, within a sprint, and also across sprints.</li> </ul>"},{"location":"day4/overview/#your-challenge","title":"Your Challenge","text":"<p>You are a DataOps team responsible for improving a deployed data pipeline to ensure it's observable, resilient, and governed.</p> <p>You will in sprints to:</p> <ul> <li>Add meaningful logs and visibility</li> <li>Simulate and respond to failures</li> <li>Build quality and governance into your process</li> </ul> <p>You can use complete pipeline (see above), or you can create one from scratch.</p>"},{"location":"day4/overview/#what-youll-present","title":"What You'll Present","text":"<p>At the end of the day, your team will:</p> <ul> <li>Demo your working pipeline</li> <li>Explain how it evolved through the sprints</li> <li>Reflect on challenges and improvements</li> </ul>"},{"location":"day4/overview/#tools-youll-use","title":"Tools You'll Use","text":"<p>You can use any tools that you have availalble. Including:</p> <ul> <li>Microsoft Fabric / Notebooks / Pipelines</li> <li>GitHub for collaboration (optional)</li> <li>Trello or physical boards for task tracking</li> <li>Logs, markdown files, or screenshots to support your demo</li> </ul> <p>Just remember that the MS Fabric lab is 3 hours. So download any notebooks before lunch!</p>"},{"location":"day4/overview/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Start small \u2014 get a working version early, then improve it</li> <li>Work iteratively \u2014 you don't have to perfect everything in Sprint 1</li> <li>Communicate often \u2014 check in with your team regularly</li> <li>Be prepared to demo and explain your work clearly</li> </ul>"},{"location":"day4/sprint1/","title":"Sprint 1 ~ Make it Observable (40 minutes)","text":"<p>Choose 1-3 items that interest you</p>"},{"location":"day4/sprint1/#quick-wins-1-2-points","title":"Quick Wins (1-2 points)","text":""},{"location":"day4/sprint1/#dashboard-design-2-points","title":"Dashboard Design (2 points)","text":"<p>As a Data Engineer, I want to design a monitoring dashboard so that stakeholders can see pipeline health at a glance.</p> <p>Key Tasks:</p> <ul> <li>Design dashboard layout and key metrics</li> <li>Identify target audience and their needs</li> <li>Create wireframes or mockups</li> <li>Plan implementation approach</li> </ul> <p>Skills: design, visualisation</p>"},{"location":"day4/sprint1/#data-flow-visualisation-2-points","title":"Data Flow Visualisation (2 points)","text":"<p>As a Data Engineer, I want to visualise our ETL data flow so that team members understand the pipeline structure.</p> <p>Key Tasks:</p> <ul> <li>Map current ETL process flow</li> <li>Create visual diagram of data movement</li> <li>Identify potential bottlenecks or failure points</li> <li>Document data sources and destinations</li> </ul> <p>Skills: design, planning</p>"},{"location":"day4/sprint1/#production-readiness-assessment-2-points","title":"Production Readiness Assessment (2 points)","text":"<p>As a Data Engineer, I want to assess what's missing for production deployment so that we can prioritise improvements.</p> <p>Key Tasks:</p> <ul> <li>Review current pipeline against production checklist</li> <li>Identify gaps in monitoring, security, and reliability</li> <li>Prioritise missing components</li> <li>Create improvement roadmap</li> </ul> <p>Skills: planning, analysis</p>"},{"location":"day4/sprint1/#change-management-documentation-1-point","title":"Change Management Documentation (1 point)","text":"<p>As a Data Steward, I want to create a change log for all improvements so we can track what's been changed in the pipeline.</p> <p>Key Tasks:</p> <ul> <li>Document current state of the notebook</li> <li>Create template for tracking future changes</li> <li>Establish change approval process</li> <li>Include version control recommendations</li> </ul> <p>Skills: planning, documentation</p>"},{"location":"day4/sprint1/#hands-on-tasks-2-3-points","title":"Hands-On Tasks (2-3 points)","text":""},{"location":"day4/sprint1/#configuration-externalisation-3-points","title":"Configuration Externalisation (3 points)","text":"<p>As a Developer, I want to parameterise the data source and output file so that others can reuse it.</p> <p>Key Tasks:</p> <ul> <li>Move all hardcoded values to external configuration</li> <li>Create config files for different environments</li> <li>Update code to read from configuration files</li> <li>Document configuration parameters and their purpose</li> </ul> <p>Skills: python, configuration</p>"},{"location":"day4/sprint1/#basic-python-logging-implementation-2-points","title":"Basic Python Logging Implementation (2 points)","text":"<p>As a Data Engineer, I want structured logging in our ETL pipeline so that I can track execution progress and identify issues.</p> <p>Key Tasks:</p> <ul> <li>Add Python logging configuration to the pipeline</li> <li>Log start/end times for each major pipeline stage</li> <li>Include record counts and processing statistics in logs</li> <li>Save logs to timestamped files for historical analysis</li> </ul> <p>Skills: python, logging</p>"},{"location":"day4/sprint1/#database-connection-health-checks-2-points","title":"Database Connection Health Checks (2 points)","text":"<p>As a Data Engineer, I want to monitor database connectivity so that I can detect connection issues before they cause pipeline failures.</p> <p>Key Tasks:</p> <ul> <li>Create function to test SQL Server connection status</li> <li>Log connection attempt results with timestamps</li> <li>Add timeout handling for connection checks</li> <li>Test and document connection recovery procedures</li> </ul> <p>Skills: python, sql server, error handling</p>"},{"location":"day4/sprint1/#monitoring-strategy-design-3-points","title":"Monitoring Strategy Design (3 points)","text":"<p>As a Data Operations Manager, I want a comprehensive monitoring strategy so that our team can effectively observe pipeline health.</p> <p>Key Tasks:</p> <ul> <li>Define what metrics to monitor and why</li> <li>Design alerting thresholds and escalation procedures</li> <li>Plan monitoring infrastructure and tools</li> <li>Create monitoring implementation roadmap</li> </ul> <p>Skills: planning, design</p>"},{"location":"day4/sprint1/#strategic-thinking-2-3-points","title":"Strategic Thinking (2-3 points)","text":""},{"location":"day4/sprint1/#pipeline-gap-analysis-2-points","title":"Pipeline Gap Analysis (2 points)","text":"<p>As a Data Engineering Team Lead, I want to understand current monitoring gaps so that I can prioritise observability improvements.</p> <p>Key Tasks:</p> <ul> <li>Document all current failure points in the ETL pipeline</li> <li>Identify which pipeline stages lack visibility</li> <li>Create a monitoring requirements matrix</li> <li>Present findings with recommendations</li> </ul> <p>Skills: analysis, documentation</p>"},{"location":"day4/sprint1/#incident-response-planning-3-points","title":"Incident Response Planning (3 points)","text":"<p>As a Data Engineer, I want a clear incident response process so that I can quickly resolve pipeline failures.</p> <p>Key Tasks:</p> <ul> <li>Map common failure scenarios and their symptoms</li> <li>Create step-by-step troubleshooting procedures</li> <li>Define escalation matrix and communication templates</li> <li>Design incident classification system</li> </ul> <p>Skills: planning, documentation</p>"},{"location":"day4/sprint1/#cloud-modern-tools-2-3-points","title":"Cloud &amp; Modern Tools (2-3 points)","text":""},{"location":"day4/sprint1/#azure-monitor-integration-3-points","title":"Azure Monitor Integration (3 points)","text":"<p>As a Data Engineer, I want to integrate with Azure Monitor so that pipeline metrics appear in our enterprise dashboard.</p> <p>Key Tasks:</p> <ul> <li>Research Azure Monitor capabilities for data pipelines</li> <li>Design integration approach for custom metrics</li> <li>Plan alerting and notification setup</li> <li>Create proof-of-concept integration</li> </ul> <p>Skills: azure, monitoring, integration</p>"},{"location":"day4/sprint1/#fabric-pipeline-observability-3-points","title":"Fabric Pipeline Observability (3 points)","text":"<p>As a Data Engineer, I want to explore Microsoft Fabric monitoring capabilities so that I understand modern data platform observability.</p> <p>Key Tasks:</p> <ul> <li>Explore Fabric monitoring and alerting features</li> <li>Compare with traditional monitoring approaches</li> <li>Design equivalent pipeline monitoring in Fabric</li> <li>Document lessons learned and recommendations</li> </ul> <p>Skills: fabric, monitoring, exploration</p>"},{"location":"day4/sprint1/#technical-deep-dives-3-points","title":"Technical Deep Dives (3 points)","text":""},{"location":"day4/sprint1/#logging-strategy-framework-3-points","title":"Logging Strategy Framework (3 points)","text":"<p>As a Data Operations Manager, I want a comprehensive logging strategy so that our team can effectively troubleshoot pipeline issues.</p> <p>Key Tasks:</p> <ul> <li>Define logging levels and what events to log</li> <li>Specify log message format and structure standards</li> <li>Design log aggregation and search capabilities</li> <li>Create logging policy document for development teams</li> </ul> <p>Skills: planning, design, documentation</p>"},{"location":"day4/sprint1/#performance-metrics-collection-3-points","title":"Performance Metrics Collection (3 points)","text":"<p>As a Data Engineer, I want detailed pipeline performance metrics so that I can optimise processing time and resource usage.</p> <p>Key Tasks:</p> <ul> <li>Implement timing decorators for all major functions</li> <li>Track memory usage during data processing</li> <li>Log API response times and success rates</li> <li>Create performance metrics summary report</li> </ul> <p>Skills: python, performance monitoring</p>"},{"location":"day4/sprint1/#email-alerting-setup-3-points","title":"Email Alerting Setup (3 points)","text":"<p>As a Data Operations Team, I want email notifications for critical pipeline failures so that we can respond quickly to issues.</p> <p>Key Tasks:</p> <ul> <li>Configure Python SMTP settings for email sending</li> <li>Create email template for pipeline failure alerts</li> <li>Test email functionality with sample failure scenarios</li> <li>Document email configuration and troubleshooting</li> </ul> <p>Skills: python, smtp, configuration</p>"},{"location":"day4/sprint1/#tips-for-sprint-1","title":"Tips for Sprint 1","text":"<ul> <li>Start with Quick Wins to build confidence</li> <li>Focus on understanding rather than perfect implementation</li> <li>Document your thinking - that's valuable too!</li> </ul>"},{"location":"day4/sprint1/#if-you-want-to-code","title":"If You Want to Code","text":"<ul> <li>Configuration Externalisation is a great starting point</li> <li>Logging Implementation gives immediate visible results</li> <li>Health Checks are practical and useful</li> </ul>"},{"location":"day4/sprint1/#if-you-like-strategy","title":"If You Like Strategy","text":"<ul> <li>Gap Analysis helps you think like a senior engineer</li> <li>Monitoring Strategy connects technical and business needs</li> <li>Incident Response is valuable for any role</li> </ul>"},{"location":"day4/sprint1/#if-youre-cloud-curious","title":"If You're Cloud-Curious","text":"<ul> <li>Azure Monitor exploration is very relevant</li> <li>Fabric Observability shows the future of data platforms</li> </ul> <p>Remember: This is about learning and exploration, not perfect delivery!</p>"},{"location":"day4/sprint2/","title":"Sprint 2 ~ Make it Resilient (50 minutes)","text":"<p>Choose 1-3 items that interest you</p>"},{"location":"day4/sprint2/#quick-wins-1-2-points","title":"Quick Wins (1-2 points)","text":""},{"location":"day4/sprint2/#error-classification-matrix-2-points","title":"Error Classification Matrix (2 points)","text":"<p>As a Data Engineering Team Lead, I want a clear error classification system so that incidents are escalated appropriately.</p> <p>Key Tasks:</p> <ul> <li>Categorise all possible pipeline errors by severity and type</li> <li>Define escalation paths for different error categories</li> <li>Create communication templates for incident notifications</li> <li>Design on-call rotation and escalation timeline matrix</li> </ul> <p>Skills: planning, incident management</p>"},{"location":"day4/sprint2/#deployment-documentation-2-points","title":"Deployment Documentation (2 points)","text":"<p>As a DevOps Engineer, I want deployment procedures documented so that releases can be executed consistently.</p> <p>Key Tasks:</p> <ul> <li>Document current deployment steps</li> <li>Create deployment checklist and validation procedures</li> <li>Design rollback procedures for failed deployments</li> <li>Include environment-specific configuration notes</li> </ul> <p>Skills: planning, documentation</p>"},{"location":"day4/sprint2/#manual-testing-framework-2-points","title":"Manual Testing Framework (2 points)","text":"<p>As a Quality Engineer, I want a testing checklist so that we can validate pipeline resilience manually.</p> <p>Key Tasks:</p> <ul> <li>Create test scenarios for different failure types</li> <li>Design validation steps for data quality and completeness</li> <li>Build testing checklist for pre-production validation</li> <li>Document expected outcomes and pass/fail criteria</li> </ul> <p>Skills: planning, testing</p>"},{"location":"day4/sprint2/#recovery-procedures-design-2-points","title":"Recovery Procedures Design (2 points)","text":"<p>As a Data Operations Manager, I want documented recovery procedures so that team members can restore service quickly.</p> <p>Key Tasks:</p> <ul> <li>Document step-by-step recovery procedures</li> <li>Create recovery time estimates for different scenarios</li> <li>Design manual override procedures</li> <li>Include contact information and escalation paths</li> </ul> <p>Skills: planning, documentation</p>"},{"location":"day4/sprint2/#hands-on-tasks-2-3-points","title":"Hands-On Tasks (2-3 points)","text":""},{"location":"day4/sprint2/#basic-retry-logic-implementation-3-points","title":"Basic Retry Logic Implementation (3 points)","text":"<p>As a Data Engineer, I want automatic retry functionality for transient failures so that temporary issues don't cause complete pipeline failures.</p> <p>Key Tasks:</p> <ul> <li>Add retry logic to database connection attempts with exponential backoff</li> <li>Implement retry for API calls with configurable retry count</li> <li>Log retry attempts and final success/failure status</li> <li>Test retry behavior with simulated network issues</li> </ul> <p>Skills: python, error handling</p>"},{"location":"day4/sprint2/#configuration-based-error-thresholds-2-points","title":"Configuration-Based Error Thresholds (2 points)","text":"<p>As a Data Engineer, I want configurable error thresholds so that the pipeline can adapt to different tolerance levels.</p> <p>Key Tasks:</p> <ul> <li>Create configuration file for error tolerance settings</li> <li>Implement data quality threshold checking</li> <li>Add API failure rate thresholds before circuit breaking</li> <li>Test threshold behavior with various error scenarios</li> </ul> <p>Skills: python, configuration</p>"},{"location":"day4/sprint2/#database-transaction-management-3-points","title":"Database Transaction Management (3 points)","text":"<p>As a Data Engineer, I want robust transaction handling so that database failures don't leave data in an inconsistent state.</p> <p>Key Tasks:</p> <ul> <li>Implement proper transaction boundaries for batch operations</li> <li>Add automatic rollback on critical errors during data loading</li> <li>Create savepoint management for partial batch recovery</li> <li>Test transaction behavior under various failure scenarios</li> </ul> <p>Skills: sql server, transactions</p>"},{"location":"day4/sprint2/#failure-simulation-recovery-3-points","title":"Failure Simulation &amp; Recovery (3 points)","text":"<p>As a Data Engineer, I want to test failure scenarios so that I can validate our recovery procedures.</p> <p>Key Tasks:</p> <ul> <li>Design chaos engineering test scenarios</li> <li>Simulate network failures, database outages, and API errors</li> <li>Test recovery procedures and measure recovery times</li> <li>Document lessons learned and improve procedures</li> </ul> <p>Skills: planning, testing</p>"},{"location":"day4/sprint2/#strategic-thinking-2-3-points","title":"Strategic Thinking (2-3 points)","text":""},{"location":"day4/sprint2/#disaster-recovery-strategy-3-points","title":"Disaster Recovery Strategy (3 points)","text":"<p>As a Data Operations Manager, I want a comprehensive disaster recovery plan so that we can restore operations quickly after major failures.</p> <p>Key Tasks:</p> <ul> <li>Define Recovery Time Objective (RTO) and Recovery Point Objective (RPO)</li> <li>Design backup and recovery procedures for database and configuration</li> <li>Create failover scenarios and manual override procedures</li> <li>Document disaster recovery testing schedule</li> </ul> <p>Skills: disaster recovery planning, business continuity</p>"},{"location":"day4/sprint2/#business-impact-assessment-2-points","title":"Business Impact Assessment (2 points)","text":"<p>As a Business Continuity Manager, I want to understand the business impact of ETL pipeline failures so that I can prioritise resilience investments.</p> <p>Key Tasks:</p> <ul> <li>Map pipeline failures to business consequences</li> <li>Calculate downtime costs for different failure scenarios</li> <li>Create failure impact matrix (probability vs business impact)</li> <li>Present risk assessment with recommended mitigations</li> </ul> <p>Skills: business analysis, risk assessment</p>"},{"location":"day4/sprint2/#cloud-modern-tools-3-points","title":"Cloud &amp; Modern Tools (3 points)","text":""},{"location":"day4/sprint2/#azure-pipeline-resilience-3-points","title":"Azure Pipeline Resilience (3 points)","text":"<p>As a Data Engineer, I want to explore Azure Data Factory resilience features so that I understand cloud-native reliability patterns.</p> <p>Key Tasks:</p> <ul> <li>Research Azure Data Factory retry and error handling capabilities</li> <li>Explore monitoring and alerting features</li> <li>Design equivalent pipeline with built-in resilience</li> <li>Compare cloud vs on-premises resilience approaches</li> </ul> <p>Skills: azure, data factory, resilience</p>"},{"location":"day4/sprint2/#fabric-error-handling-3-points","title":"Fabric Error Handling (3 points)","text":"<p>As a Data Engineer, I want to implement error handling in Microsoft Fabric so that pipelines gracefully handle failures.</p> <p>Key Tasks:</p> <ul> <li>Explore Fabric pipeline error handling options</li> <li>Design failure scenarios and recovery procedures</li> <li>Implement basic error handling in Fabric environment</li> <li>Document best practices and lessons learned</li> </ul> <p>Skills: fabric, error handling</p>"},{"location":"day4/sprint2/#technical-deep-dives-3-4-points","title":"Technical Deep Dives (3-4 points)","text":""},{"location":"day4/sprint2/#circuit-breaker-implementation-4-points","title":"Circuit Breaker Implementation (4 points)","text":"<p>As a Data Engineer, I want circuit breaker protection for external API calls so that API failures don't overwhelm services.</p> <p>Key Tasks:</p> <ul> <li>Implement circuit breaker pattern for postcode and company APIs</li> <li>Configure failure thresholds and recovery timeouts</li> <li>Provide fallback data sources when APIs fail</li> <li>Monitor circuit breaker state changes and recovery events</li> </ul> <p>Skills: python, circuit breaker pattern</p>"},{"location":"day4/sprint2/#automated-health-checks-3-points","title":"Automated Health Checks (3 points)","text":"<p>As a Data Engineer, I want automated health validation so that system problems are detected before they impact pipelines.</p> <p>Key Tasks:</p> <ul> <li>Create comprehensive health check function</li> <li>Check database connectivity, file system access, and API availability</li> <li>Return structured health status with component-level details</li> <li>Test health checks under various failure conditions</li> </ul> <p>Skills: python, health monitoring</p>"},{"location":"day4/sprint2/#state-management-checkpointing-4-points","title":"State Management &amp; Checkpointing (4 points)","text":"<p>As a Data Engineer, I want pipeline state persistence so that failed pipelines can resume from the last successful checkpoint.</p> <p>Key Tasks:</p> <ul> <li>Design checkpoint system for tracking pipeline progress</li> <li>Implement state persistence using database or file system</li> <li>Add resume capability from last successful checkpoint</li> <li>Test recovery scenarios with various failure points</li> </ul> <p>Skills: python, state management</p>"},{"location":"day4/sprint2/#tips-for-sprint-2","title":"Tips for Sprint 2","text":"<ul> <li>Error Classification helps you think systematically about failures</li> <li>Recovery Procedures are valuable for any data role</li> <li>Testing Framework gives you practical QA skills</li> </ul>"},{"location":"day4/sprint2/#if-you-want-to-code","title":"If You Want to Code","text":"<ul> <li>Retry Logic is immediately useful and satisfying to implement</li> <li>Configuration Thresholds teaches important design patterns</li> <li>Health Checks provide visible system validation</li> </ul>"},{"location":"day4/sprint2/#if-you-like-strategy","title":"If You Like Strategy","text":"<ul> <li>Disaster Recovery thinking is valuable for senior roles</li> <li>Business Impact Assessment connects technical to business</li> <li>Failure Simulation helps you think like a reliability engineer</li> </ul>"},{"location":"day4/sprint2/#if-youre-cloud-curious","title":"If You're Cloud-Curious","text":"<ul> <li>Azure Pipeline Resilience shows enterprise patterns</li> <li>Fabric Error Handling demonstrates modern platform capabilities</li> </ul>"},{"location":"day4/sprint2/#for-the-ambitious","title":"For the Ambitious","text":"<ul> <li>Circuit Breaker is a classic reliability pattern worth learning</li> <li>State Management is advanced but very powerful</li> <li>Automated Health Checks combine multiple concepts</li> </ul> <p>Remember: Resilience is about graceful degradation, not perfection!</p>"},{"location":"day4/sprint3/","title":"Sprint 3 ~ Make it Governed (50 minutes)","text":"<p>Choose 1-3 items that interest you</p>"},{"location":"day4/sprint3/#quick-wins-1-2-points","title":"Quick Wins (1-2 points)","text":""},{"location":"day4/sprint3/#data-classification-framework-2-points","title":"Data Classification Framework (2 points)","text":"<p>As a Data Steward, I want a data classification scheme so that sensitive data is identified and handled appropriately.</p> <p>Key Tasks:</p> <ul> <li>Document data classification scheme (PII, sensitive, public, confidential)</li> <li>Map customer data fields to classification levels</li> <li>Define handling requirements for each classification</li> <li>Create data tagging and labeling standards</li> </ul> <p>Skills: data governance, classification</p>"},{"location":"day4/sprint3/#compliance-checklist-2-points","title":"Compliance Checklist (2 points)","text":"<p>As a Compliance Officer, I want a GDPR compliance checklist so that data processing meets regulatory requirements.</p> <p>Key Tasks:</p> <ul> <li>Create GDPR compliance checklist for data processing</li> <li>Map pipeline activities to legal basis for processing</li> <li>Document data subject rights and response procedures</li> <li>Include breach notification requirements</li> </ul> <p>Skills: compliance, GDPR</p>"},{"location":"day4/sprint3/#quality-metrics-definition-2-points","title":"Quality Metrics Definition (2 points)","text":"<p>As a Data Quality Manager, I want standardised quality measures so that we can consistently evaluate data across pipeline stages.</p> <p>Key Tasks:</p> <ul> <li>Define data quality dimensions (accuracy, completeness, consistency)</li> <li>Create quality scoring methodology and thresholds</li> <li>Design quality metrics dashboard requirements</li> <li>Document quality improvement processes</li> </ul> <p>Skills: data quality, metrics</p>"},{"location":"day4/sprint3/#security-assessment-2-points","title":"Security Assessment (2 points)","text":"<p>As an Information Security Manager, I want a security assessment of our pipeline so that vulnerabilities are identified.</p> <p>Key Tasks:</p> <ul> <li>Review pipeline for security vulnerabilities</li> <li>Assess data encryption and access controls</li> <li>Identify security gaps and prioritise improvements</li> <li>Create security improvement roadmap</li> </ul> <p>Skills: security analysis, assessment</p>"},{"location":"day4/sprint3/#hands-on-tasks-2-3-points","title":"Hands-On Tasks (2-3 points)","text":""},{"location":"day4/sprint3/#pii-data-masking-3-points","title":"PII Data Masking (3 points)","text":"<p>As a Data Protection Engineer, I want PII data masking capabilities so that non-production environments don't expose sensitive information.</p> <p>Key Tasks:</p> <ul> <li>Identify PII fields in customer data (email, phone, postcode)</li> <li>Implement masking functions for different data types</li> <li>Add configuration for production vs non-production handling</li> <li>Test masked data still supports business logic validation</li> </ul> <p>Skills: python, data masking</p>"},{"location":"day4/sprint3/#data-quality-validation-rules-2-points","title":"Data Quality Validation Rules (2 points)","text":"<p>As a Data Engineer, I want configurable quality validation rules so that data quality standards are automatically enforced.</p> <p>Key Tasks:</p> <ul> <li>Create validation rules for customer data (email format, postcode validity)</li> <li>Implement quality scoring based on rule violations</li> <li>Log quality metrics and failed validation details</li> <li>Test quality rules with various data scenarios</li> </ul> <p>Skills: python, data validation</p>"},{"location":"day4/sprint3/#audit-trail-enhancement-3-points","title":"Audit Trail Enhancement (3 points)","text":"<p>As a Compliance Officer, I want comprehensive audit trails so that we can demonstrate data processing compliance.</p> <p>Key Tasks:</p> <ul> <li>Enhance existing audit table with detailed change tracking</li> <li>Log all data transformations and business rule applications</li> <li>Track user actions and system changes with timestamps</li> <li>Create audit report generation functionality</li> </ul> <p>Skills: database design, audit logging</p>"},{"location":"day4/sprint3/#gdpr-compliance-assessment-3-points","title":"GDPR Compliance Assessment (3 points)","text":"<p>As a Data Protection Officer, I want to assess GDPR compliance so that our pipeline meets data protection requirements.</p> <p>Key Tasks:</p> <ul> <li>Review pipeline against GDPR requirements</li> <li>Document lawful basis for each type of processing</li> <li>Assess data retention and deletion procedures</li> <li>Create compliance gap analysis and improvement plan</li> </ul> <p>Skills: planning, compliance</p>"},{"location":"day4/sprint3/#strategic-thinking-2-3-points","title":"Strategic Thinking (2-3 points)","text":""},{"location":"day4/sprint3/#data-governance-policy-framework-3-points","title":"Data Governance Policy Framework (3 points)","text":"<p>As a Data Protection Officer, I want comprehensive data governance policies so that our ETL pipeline complies with regulations.</p> <p>Key Tasks:</p> <ul> <li>Document data governance policies for processing activities</li> <li>Define data retention and deletion policies</li> <li>Create data lineage requirements and documentation standards</li> <li>Map regulatory compliance requirements</li> </ul> <p>Skills: governance, policy design</p>"},{"location":"day4/sprint3/#environmental-impact-assessment-2-points","title":"Environmental Impact Assessment (2 points)","text":"<p>As a Sustainability Officer, I want to understand the environmental impact of our data processing so that we contribute to net-zero targets.</p> <p>Key Tasks:</p> <ul> <li>Calculate current carbon footprint of ETL operations</li> <li>Identify energy-efficient processing opportunities</li> <li>Design green data processing policies and metrics</li> <li>Create sustainability reporting and improvement targets</li> </ul> <p>Skills: sustainability, environmental assessment</p>"},{"location":"day4/sprint3/#cloud-modern-tools-3-points","title":"Cloud &amp; Modern Tools (3 points)","text":""},{"location":"day4/sprint3/#microsoft-purview-integration-3-points","title":"Microsoft Purview Integration (3 points)","text":"<p>As a Data Governance Engineer, I want to integrate with Microsoft Purview so that data lineage and governance are automated.</p> <p>Key Tasks:</p> <ul> <li>Explore Microsoft Purview data governance capabilities</li> <li>Design integration approach for automated lineage tracking</li> <li>Plan data classification and policy enforcement</li> <li>Create proof-of-concept governance integration</li> </ul> <p>Skills: purview, data governance</p>"},{"location":"day4/sprint3/#fabric-data-governance-3-points","title":"Fabric Data Governance (3 points)","text":"<p>As a Data Steward, I want to explore Microsoft Fabric governance capabilities so that I understand modern data governance approaches.</p> <p>Key Tasks:</p> <ul> <li>Explore Fabric data governance and lineage features</li> <li>Compare with traditional governance approaches</li> <li>Design governance strategy using Fabric capabilities</li> <li>Document best practices and recommendations</li> </ul> <p>Skills: fabric, governance</p>"},{"location":"day4/sprint3/#technical-deep-dives-3-4-points","title":"Technical Deep Dives (3-4 points)","text":""},{"location":"day4/sprint3/#automated-data-classification-4-points","title":"Automated Data Classification (4 points)","text":"<p>As a Data Governance Engineer, I want automatic data classification so that sensitive data is identified and handled according to policies.</p> <p>Key Tasks:</p> <ul> <li>Implement pattern recognition for PII and sensitive data</li> <li>Add automatic data tagging based on content analysis</li> <li>Create classification confidence scoring</li> <li>Integrate classification results with security policies</li> </ul> <p>Skills: python, data classification</p>"},{"location":"day4/sprint3/#encryption-implementation-4-points","title":"Encryption Implementation (4 points)","text":"<p>As a Security Engineer, I want end-to-end encryption for sensitive data so that customer information is protected throughout the pipeline.</p> <p>Key Tasks:</p> <ul> <li>Implement field-level encryption for PII data in database</li> <li>Add secure key management and rotation procedures</li> <li>Encrypt sensitive data in transit between components</li> <li>Test encryption performance impact and recovery</li> </ul> <p>Skills: encryption, security</p>"},{"location":"day4/sprint3/#data-lineage-tracking-3-points","title":"Data Lineage Tracking (3 points)","text":"<p>As a Data Steward, I want automated data lineage tracking so that I can understand data flow and transformations for audit purposes.</p> <p>Key Tasks:</p> <ul> <li>Add lineage metadata to each transformation step</li> <li>Log source-to-target mappings for all data movements</li> <li>Create simple lineage visualisation or report</li> <li>Test lineage tracking through complete ETL pipeline</li> </ul> <p>Skills: python, metadata management</p>"},{"location":"day4/sprint3/#tips-for-sprint-3","title":"Tips for Sprint 3","text":"<ul> <li>Data Classification helps you think about data systematically</li> <li>Compliance Checklist is practical for any data role</li> <li>Quality Metrics connect to business value</li> <li>Security Assessment builds security awareness</li> </ul>"},{"location":"day4/sprint3/#if-you-want-to-code","title":"If You Want to Code","text":"<ul> <li>PII Masking is immediately useful and visible</li> <li>Quality Validation teaches important data patterns</li> <li>Audit Trail demonstrates compliance thinking</li> <li>Data Lineage is advanced but very valuable</li> </ul>"},{"location":"day4/sprint3/#if-you-like-strategy","title":"If You Like Strategy","text":"<ul> <li>Governance Policy thinking is valuable for senior roles</li> <li>Environmental Impact connects to sustainability goals</li> <li>GDPR Assessment demonstrates regulatory awareness</li> </ul>"},{"location":"day4/sprint3/#if-youre-cloud-curious","title":"If You're Cloud-Curious","text":"<ul> <li>Microsoft Purview shows enterprise governance platforms</li> <li>Fabric Governance demonstrates modern approaches</li> </ul>"},{"location":"day4/sprint3/#for-the-ambitious","title":"For the Ambitious","text":"<ul> <li>Automated Classification uses advanced techniques</li> <li>Encryption is essential security knowledge</li> <li>Data Lineage is a complex but important capability</li> </ul>"},{"location":"day4/sprint3/#sustainability-focus","title":"Sustainability Focus","text":"<p>Consider how your chosen items can contribute to:</p> <ul> <li>Reduced energy consumption through efficient processing</li> <li>Minimised data movement to reduce carbon footprint</li> <li>Optimised resource usage for environmental responsibility</li> </ul> <p>Remember: Governance is about enabling good practices, not blocking progress!</p>"},{"location":"day4/welcome/","title":"Day 4: Welcome &amp; Improvement Mindset","text":"<p>Time: 9:30-9:50 (20 minutes) Learning Types: Discussion \u2192 Acquisition</p>"},{"location":"day4/welcome/#overview","title":"Overview","text":"<p>This final day focuses on operational evolution - moving from reactive problem-solving to proactive improvement and optimization. The welcome session establishes the mindset shift from \"fixing what's broken\" to \"making good systems even better\" while connecting to the systematic thinking developed over the previous three days.</p>"},{"location":"day4/welcome/#session-structure","title":"Session Structure","text":""},{"location":"day4/welcome/#discussion-from-reactive-to-proactive-operations-12-minutes","title":"Discussion: From Reactive to Proactive Operations (12 minutes)","text":""},{"location":"day4/welcome/#opening-frame-2-minutes","title":"Opening Frame (2 minutes)","text":"<p>Facilitator Introduction:</p> <ul> <li>\"Welcome to Day 4 - our final day together on operational data engineering\"</li> <li>\"Days 1-3 focused on building, monitoring, breaking, and governing data operations\"</li> <li>\"Today we shift from reactive to proactive - how do you continuously improve and evolve data systems?\"</li> </ul>"},{"location":"day4/welcome/#learning-journey-reflection-3-minutes","title":"Learning Journey Reflection (3 minutes)","text":"<p>Quick recap with learners: - Day 1: \"What was the key insight about monitoring?\" - Day 2: \"What surprised you most about incident response?\" - Day 3: \"What's the biggest quality/governance challenge you identified?\"</p> <p>Connect the progression: - \"You've built operational skills for when things work, when they break, and when they need governing\" - \"Today: How do you make them work even better?\"</p>"},{"location":"day4/welcome/#individual-reflection-4-minutes","title":"Individual Reflection (4 minutes)","text":"<p>Question for learners: \"Think about a system or process in your workplace that works 'well enough' but could be better. What would 'better' look like? What stops you from improving it?\"</p> <p>Reflection prompts:</p> <ul> <li>Data pipeline that works but is slow/expensive</li> <li>Process that functions but requires manual intervention</li> <li>System that delivers results but is hard to maintain</li> <li>Tool that does the job but frustrates users</li> </ul>"},{"location":"day4/welcome/#group-sharing-3-minutes","title":"Group Sharing (3 minutes)","text":"<p>Structured sharing:</p> <p>Each learner shares one example (20-30 seconds): - \"Something that works but could be better is...\" - \"Better would mean...\" - \"The main obstacle to improvement is...\"</p> <p>Facilitator role:</p> <ul> <li>Listen for improvement themes: performance, cost, maintainability, user experience</li> <li>Note barriers: time, resources, prioritization, technical debt</li> <li>Capture on screen: Common improvement opportunities and obstacles</li> </ul>"},{"location":"day4/welcome/#acquisition-continuous-improvement-framework-8-minutes","title":"Acquisition: Continuous Improvement Framework (8 minutes)","text":""},{"location":"day4/welcome/#the-improvement-mindset-3-minutes","title":"The Improvement Mindset (3 minutes):","text":"<p>Operational Maturity Levels:</p> <pre><code>REACTIVE OPERATIONS          PROACTIVE OPERATIONS\n\u2193                           \u2193\nFix when broken      \u2192      Prevent problems\nRespond to requests  \u2192      Anticipate needs  \nManual processes     \u2192      Automated workflows\nIndividual heroics   \u2192      Systematic improvement\n\"If it works, don't touch\"  \"If it works, make it work better\"\n</code></pre> <p>Why Continuous Improvement Matters:</p> <ul> <li>Technical debt accumulates - small problems become big problems</li> <li>Business needs evolve - today's solution may not fit tomorrow's requirements</li> <li>Technology advances - better tools and approaches become available</li> <li>Cost optimization - efficient systems save money and resources</li> </ul>"},{"location":"day4/welcome/#improvement-opportunity-categories-3-minutes","title":"Improvement Opportunity Categories (3 minutes):","text":"<p>Performance &amp; Efficiency:</p> <ul> <li>Faster processing, lower latency, reduced resource usage</li> <li>Automation of manual tasks, elimination of bottlenecks</li> </ul> <p>Reliability &amp; Maintainability:</p> <ul> <li>Easier troubleshooting, better error handling, simplified architecture</li> <li>Reduced dependency on specific people or knowledge</li> </ul> <p>User Experience &amp; Business Value:</p> <ul> <li>More intuitive interfaces, self-service capabilities, better data accessibility</li> <li>Enhanced analytics capabilities, real-time insights</li> </ul> <p>Cost &amp; Resource Optimization:</p> <ul> <li>Lower infrastructure costs, improved resource utilization</li> <li>Reduced operational overhead, simplified maintenance</li> </ul>"},{"location":"day4/welcome/#todays-improvement-framework-2-minutes","title":"Today's Improvement Framework (2 minutes):","text":"<p>The Day 4 Approach:</p> <ol> <li>Assess current state - where are we now? (gap analysis)</li> <li>Explore possibilities - what's available? (technology assessment)</li> <li>Plan systematically - how do we get there? (deployment and change management)</li> <li>Measure and iterate - how do we know it's better? (continuous improvement)</li> </ol> <p>Key Questions We'll Answer:</p> <ul> <li>How do you identify improvement opportunities systematically?</li> <li>What new technologies could enhance your data operations?</li> <li>How do you plan and execute operational improvements safely?</li> <li>How do you build a culture of continuous improvement?</li> </ul>"},{"location":"day4/welcome/#transition-to-session-1","title":"Transition to Session 1","text":""},{"location":"day4/welcome/#bridge-to-technology-assessment-1-minute","title":"Bridge to Technology Assessment (1 minute)","text":"<p>Facilitator:</p> <ul> <li>\"You've identified improvement opportunities from your experience\"</li> <li>\"Now we'll take a systematic approach to assessing what's possible\"</li> <li>\"We'll start with gap analysis - understanding the difference between current state and desired state\"</li> <li>\"Then explore emerging technologies that could bridge those gaps\"</li> </ul>"},{"location":"day4/welcome/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day4/welcome/#managing-the-discussion","title":"Managing the Discussion:","text":"<p>If learners focus only on technical improvements:</p> <ul> <li>\"What about user experience or business process improvements?\"</li> <li>\"How would non-technical stakeholders benefit from these changes?\"</li> </ul> <p>If examples are too specific/narrow:</p> <ul> <li>\"Think broader - process improvements, tool upgrades, automation opportunities\"</li> <li>\"Consider improvements that affect multiple people or systems\"</li> </ul> <p>If discussion gets pessimistic about obstacles:</p> <ul> <li>\"Those are real constraints - how do you work within them to still make progress?\"</li> <li>\"What small improvements could you make without requiring major resources?\"</li> </ul>"},{"location":"day4/welcome/#common-improvement-themes-to-expect","title":"Common Improvement Themes to Expect:","text":"<p>Technical Improvements:</p> <ul> <li>Performance optimization, automation opportunities</li> <li>Tool upgrades, architecture simplification</li> <li>Monitoring and alerting enhancements</li> </ul> <p>Process Improvements:</p> <ul> <li>Workflow streamlining, self-service capabilities</li> <li>Documentation and knowledge sharing</li> <li>Collaboration and communication enhancements</li> </ul> <p>Business Value Improvements:</p> <ul> <li>Faster decision-making, better data accessibility</li> <li>New analytical capabilities, real-time insights</li> <li>Cost reduction, resource optimization</li> </ul>"},{"location":"day4/welcome/#expected-outcomes","title":"Expected Outcomes","text":"<p>By the end of this session, learners should:</p> <ol> <li>Shift mindset from reactive to proactive operational thinking</li> <li>Identify improvement opportunities systematically rather than reactively</li> <li>Understand improvement categories and how they create business value</li> <li>Be prepared for structured assessment of current state vs. desired state</li> </ol>"},{"location":"day4/welcome/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K28: Continuous improvement including how to capture good practice and lessons learned</li> <li>S21: Identify and remediate technical debt, assess for updates and obsolescence</li> <li>S25: Assess and identify gaps in existing tools and technologies</li> <li>B3: Quality focus that promotes continuous improvement</li> <li>B6: Keeps abreast of developments in emerging technologies</li> </ul>"},{"location":"labs/01-lakehouse/","title":"Lab ~ Create a Microsoft Fabric Lakehouse","text":"<p>Large-scale data analytics solutions have traditionally been built around a data warehouse, in which data is stored in relational tables and queried using SQL. The growth in \u201cbig data\u201d (characterized by high volumes, variety, and velocity of new data assets) together with the availability of low-cost storage and cloud-scale distributed compute technologies has led to an alternative approach to analytical data storage; the data lake. In a data lake, data is stored as files without imposing a fixed schema for storage. Increasingly, data engineers and analysts seek to benefit from the best features of both of these approaches by combining them in a data lakehouse; in which data is stored in files in a data lake and a relational schema is applied to them as a metadata layer so that they can be queried using traditional SQL semantics.</p> <p>In Microsoft Fabric, a lakehouse provides highly scalable file storage in a OneLake store (built on Azure Data Lake Store Gen2) with a metastore for relational objects such as tables and views based on the open source Delta Lake table format. Delta Lake enables you to define a schema of tables in your lakehouse that you can query using SQL.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"labs/01-lakehouse/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> <li> <p>View the new lakehouse, and note that the Lakehouse explorer pane on the left enables you to browse tables and files in the lakehouse:</p> <ul> <li> <p>The Tables folder contains tables that you can query using SQL semantics. Tables in a Microsoft Fabric lakehouse are based on the open source Delta Lake file format, commonly used in Apache Spark.</p> </li> <li> <p>The Files folder contains data files in the OneLake storage for the lakehouse that aren't associated with managed delta tables. You can also create shortcuts in this folder to reference data that is stored externally.</p> </li> </ul> </li> </ol> <p>Currently, there are no tables or files in this lakehouse.</p>"},{"location":"labs/01-lakehouse/#step-4-upload-a-file","title":"Step 4: Upload a file","text":"<p>Fabric provides multiple ways to load data into the lakehouse, including built-in support for pipelines that copy data from external sources and data flows (Gen 2) that you can define using visual tools based on Power Query. However one of the simplest ways to ingest small amounts of data is to upload files or folders from your local computer (or lab VM if applicable).</p> <ol> <li> <p>Download the sales.csv file from https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv, </p> <ul> <li>Save it as <code>sales.csv</code> on your local computer (or lab VM if applicable).</li> </ul> <p>Note</p> <ul> <li>To download the file, open a new tab in the browser and paste in the URL.</li> <li>Right click anywhere on the page containing the data and select \"Save as\" to save the data as a CSV file.</li> </ul> </li> <li> <p>Return to the web browser tab containing your lakehouse</p> <ul> <li>Click the ... menu for the Files folder in the Explorer pane select New subfolder</li> <li>Name the new subfolder: <code>data</code></li> <li>Click Create</li> </ul> </li> <li> <p>In the ... menu for the new data folder, select Upload and Upload files.</p> <ul> <li>Then upload the sales.csv file from your local computer (or lab VM if applicable).</li> </ul> </li> <li> <p>After the file has been uploaded, select the Files/data folder and verify that the sales.csv file has been uploaded, as shown here:</p> <p></p> </li> <li> <p>Select the sales.csv file to see a preview of its contents.</p> <p>If the sales.csv file does not automatically appear, in the ... menu for the data folder, select Refresh.</p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-5-explore-shortcuts","title":"Step 5: Explore shortcuts","text":"<p>In many scenarios, the data you need to work with in your lakehouse may be stored in some other location. While there are many ways to ingest data into the OneLake storage for your lakehouse, another option is to instead create a shortcut. Shortcuts enable you to include externally sourced data in your analytics solution without the overhead and risk of data inconsistency associated with copying it.</p> <ol> <li> <p>In the ... menu for the Files folder, select New shortcut.</p> </li> <li> <p>View the available data source types for shortcuts.</p> <ul> <li>Then close the New shortcut dialog box without creating a shortcut.</li> </ul> </li> </ol>"},{"location":"labs/01-lakehouse/#step-6-load-file-data-into-a-table","title":"Step 6: Load file data into a table","text":"<p>The sales data you uploaded is in a file, which you can work with directly by using Apache Spark code. However, in many scenarios you may want to load the data from the file into a table so that you can query it using SQL.</p> <ol> <li> <p>In the Explorer pane, select the Files/data folder so you can see the sales.csv file it contains.</p> </li> <li> <p>In the ... menu for the sales.csv file, select Load to Tables &gt; New table.</p> <p></p> </li> <li> <p>In Load to table dialog box, set the table name to sales and confirm the load operation.</p> <ul> <li>Then wait for the table to be created and loaded.</li> </ul> <p>If the sales table does not automatically appear, in the ... menu for the Tables folder, select Refresh.</p> </li> <li> <p>In the Explorer pane, select the sales table that has been created to view the data:</p> <p></p> </li> <li> <p>In the ... menu for the sales table, select View files to see the underlying files for this table:</p> <p></p> <p>Files for a delta table are stored in Parquet format, and include a subfolder named <code>_delta_log</code> in which details of transactions applied to the table are logged.</p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-7-use-sql-to-query-tables","title":"Step 7: Use SQL to query tables","text":"<p>When you create a lakehouse and define tables in it, a SQL endpoint is automatically created through which the tables can be queried using SQL <code>SELECT</code> statements.</p> <ol> <li> <p>At the top-right of the Lakehouse page, switch from Lakehouse to SQL analytics endpoint.</p> <ul> <li>Then wait a short time until the SQL analytics endpoint for your lakehouse opens in a visual interface from which you can query its tables.</li> </ul> </li> <li> <p>Use the New SQL query button to open a new query editor, and enter the following SQL query:</p> <pre><code>SELECT Item, SUM(Quantity * UnitPrice) AS Revenue\nFROM sales\nGROUP BY Item\nORDER BY Revenue DESC;\n</code></pre> </li> <li> <p>Use the  Run button to run the query and view the results, which should show the total revenue for each product.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-8-create-a-visual-query","title":"Step 8: Create a visual query","text":"<p>While many data professionals are familiar with SQL, those with Power BI experience can apply their Power Query skills to create visual queries.</p> <ol> <li> <p>On the toolbar, expand the New SQL query option and select New visual query.</p> </li> <li> <p>Drag the sales table (under dbo &gt; Tables) to the new visual query editor pane that opens to create a Power Query as shown here:</p> <p></p> </li> <li> <p>In the Manage columns menu, select Choose columns.</p> <ul> <li>Then select only the SalesOrderNumber and SalesOrderLineNumber columns. Click OK</li> </ul> <p></p> </li> <li> <p>in the Transform menu, select Group by. Then group the data by using the following Basic settings:</p> <ul> <li>Group by: SalesOrderNumber</li> <li>New column name: <code>LineItems</code></li> <li>Operation: Count distinct values</li> <li>Column: SalesOrderLineNumber (if not greyed out)</li> </ul> <p>When you're done, the results pane under the visual query shows the number of line items for each sales order.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse and imported data into it. You\u2019ve seen how a lakehouse consists of files and tables stored in a OneLake data store. The managed tables can be queried using SQL, and are included in a default semantic model to support data visualizations.</p> <p>If you've finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/01-lakehouse.html </p>"},{"location":"labs/03b-medallion-lakehouse/","title":"Lab: Create Medallion Architecture in a Fabric Lakehouse","text":"<p>In this exercise you will build out a medallion architecture in a Fabric lakehouse using notebooks. You will create a workspace, create a lakehouse, upload data to the bronze layer, transform the data and load it to the silver Delta table, transform the data further and load it to the gold Delta tables, and then explore the semantic model and create relationships.</p> <p>This exercise should take approximately 45 minutes to complete</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/03b-medallion-lakehouse/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> <li> <p>Navigate to the workspace settings and verify that the Data model settings preview feature is enabled. This will enable you to create relationships between tables in your lakehouse using a Power BI semantic model.</p> <p></p> <p>You may need to refresh the browser tab after enabling the preview feature.</p> </li> </ol>"},{"location":"labs/03b-medallion-lakehouse/#create-a-lakehouse-and-upload-data-to-bronze-layer","title":"Create a lakehouse and upload data to bronze layer","text":"<p>Now that you have a workspace, it's time to create a data lakehouse for the data you're going to analyze.</p> <ol> <li> <p>In the workspace you just created, create a new Lakehouse named Sales by selecting the + New item button.</p> <p>After a minute or so, a new empty lakehouse will be created. Next, you'll ingest some data into the data lakehouse for analysis. There are multiple ways to do this, but in this exercise you'll simply download a text file to your local computer (or lab VM if applicable) and then upload it to your lakehouse.</p> </li> <li> <p>Download the data file for this exercise from <code>https://github.com/MicrosoftLearning/dp-data/blob/main/orders.zip</code> Extract the files and save them with their original names on your local computer (or lab VM if applicable).</p> <p>There should be 3 files containing sales data for 3 years: 2019.csv, 2020.csv, and 2021.csv</p> </li> <li> <p>Return to the web browser tab containing your lakehouse, and in the ... menu for the Files folder in the Explorer pane, select New subfolder and create a folder named bronze.</p> </li> <li> <p>In the ... menu for the bronze folder, select Upload and Upload files, and then upload the 3 files (2019.csv, 2020.csv, and 2021.csv) from your local computer (or lab VM if applicable) to the lakehouse. Use the shift key to upload all 3 files at once.</p> </li> <li> <p>After the files have been uploaded, select the bronze folder; and verify that the files have been uploaded, as shown here:</p> <p></p> </li> </ol>"},{"location":"labs/03b-medallion-lakehouse/#transform-data-and-load-to-silver-delta-table","title":"Transform data and load to silver Delta table","text":"<p>Now that you have some data in the bronze layer of your lakehouse, you can use a notebook to transform the data and load it to a delta table in the silver layer.</p> <ol> <li> <p>On the Home page while viewing the contents of the bronze folder in your data lake, in the Open notebook menu, select New notebook.</p> <p>After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).</p> </li> <li> <p>When the notebook opens, rename it to <code>Transform data for Silver</code> by selecting the <code>Notebook xxxx</code> text at the top left of the notebook and entering the new name.</p> <p></p> </li> <li> <p>Select the existing cell in the notebook, which contains some simple commented-out code.</p> <p>Highlight and delete the two lines - you will not need this code.</p> <p>Note: In this exercise, you'll use PySpark and SQL.</p> </li> <li> <p>Paste the following code into the cell:</p> <pre><code>from pyspark.sql.types import *\n\n# Create the schema for the table\norderSchema = StructType([\n    StructField(\"SalesOrderNumber\", StringType()),\n    StructField(\"SalesOrderLineNumber\", IntegerType()),\n    StructField(\"OrderDate\", DateType()),\n    StructField(\"CustomerName\", StringType()),\n    StructField(\"Email\", StringType()),\n    StructField(\"Item\", StringType()),\n    StructField(\"Quantity\", IntegerType()),\n    StructField(\"UnitPrice\", FloatType()),\n    StructField(\"Tax\", FloatType())\n    ])\n\n# Import all files from bronze folder of lakehouse\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").schema(orderSchema).load(\"Files/bronze/*.csv\")\n\n# Display the first 10 rows of the dataframe to preview your data\ndisplay(df.head(10))\n</code></pre> </li> <li> <p>Use the  (Run cell) button on the left of the cell to run the code.</p> <p>Note</p> <ul> <li>Since this is the first time you've run any Spark code in this notebook, a Spark session must be started.</li> <li>This means that the first run can take a minute or so to complete. </li> <li>Subsequent runs will be quicker.</li> </ul> </li> <li> <p>When the cell command has completed, review the output below the cell, which should look similar to this:</p> <pre><code>Index SalesOrderNumber SalesOrderLineNumber OrderDate  CustomerName Email        Item                Quantity UnitPrice Tax\n1     SO49172          1                    2021-01-01 Brian Howard brian@aw.com Road-250 Red        1        2443.35   195.468\n2     SO49173          1                    2021-01-01 Linda Alvare linda@aw.com Mountain-200 Silver 1        2071.4197 165.7136\n</code></pre> <p>The code you ran loaded the data from the CSV files in the bronze folder into a Spark dataframe, and then displayed the first few rows of the dataframe.</p> <p>Note: You can clear, hide, and auto-resize the contents of the cell output by selecting the ... menu at the top left of the output pane.</p> </li> <li> <p>Now you'll add columns for data validation and cleanup, using a PySpark dataframe to add columns and update the values of some of the existing columns. Use the + Code button to add a new code block and add the following code to the cell:</p> <pre><code>from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n\n# Add columns IsFlagged, CreatedTS and ModifiedTS\ndf = df.withColumn(\"FileName\", input_file_name()) \\\n    .withColumn(\"IsFlagged\", when(col(\"OrderDate\") &lt; '2019-08-01',True).otherwise(False)) \\\n    .withColumn(\"CreatedTS\", current_timestamp()).withColumn(\"ModifiedTS\", current_timestamp())\n\n# Update CustomerName to \"Unknown\" if CustomerName null or empty\ndf = df.withColumn(\"CustomerName\", when((col(\"CustomerName\").isNull() | (col(\"CustomerName\")==\"\")),lit(\"Unknown\")).otherwise(col(\"CustomerName\")))\n</code></pre> <ul> <li>The first line of the code imports the necessary functions from PySpark. </li> <li>You're then adding new columns to the dataframe so you can track the source file name, whether the order was flagged as being a before the fiscal year of interest, and when the row was created and modified.</li> <li>Finally, you're updating the CustomerName column to \"Unknown\" if it's null or empty.</li> </ul> </li> <li> <p>Run the cell to execute the code using the  (Run cell) button.</p> </li> <li> <p>Next, you'll define the schema for the sales_silver table in the sales database using Delta Lake format. Create a new code block and add the following code to the cell:</p> <pre><code># Define the schema for the sales_silver table\n\nfrom pyspark.sql.types import *\nfrom delta.tables import *\n\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.sales_silver\") \\\n    .addColumn(\"SalesOrderNumber\", StringType()) \\\n    .addColumn(\"SalesOrderLineNumber\", IntegerType()) \\\n    .addColumn(\"OrderDate\", DateType()) \\\n    .addColumn(\"CustomerName\", StringType()) \\\n    .addColumn(\"Email\", StringType()) \\\n    .addColumn(\"Item\", StringType()) \\\n    .addColumn(\"Quantity\", IntegerType()) \\\n    .addColumn(\"UnitPrice\", FloatType()) \\\n    .addColumn(\"Tax\", FloatType()) \\\n    .addColumn(\"FileName\", StringType()) \\\n    .addColumn(\"IsFlagged\", BooleanType()) \\\n    .addColumn(\"CreatedTS\", DateType()) \\\n    .addColumn(\"ModifiedTS\", DateType()) \\\n    .execute()\n</code></pre> </li> <li> <p>Run the cell to execute the code using the  (Run cell) button.</p> </li> <li> <p>Select the ... in the Tables section of the Explorer pane and select Refresh. You should now see the new sales_silver table listed. The  (triangle icon) indicates that it's a Delta table.</p> <p>Note: If you don't see the new table, wait a few seconds and then select Refresh again, or refresh the entire browser tab.</p> </li> <li> <p>Now you're going to perform an upsert operation on a Delta table, updating existing records based on specific conditions and inserting new records when no match is found. Add a new code block and paste the following code:</p> <pre><code># Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.\n\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/sales_silver')\n\ndfUpdates = df\n\ndeltaTable.alias('silver') \\\n  .merge(\n    dfUpdates.alias('updates'),\n    'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'\n  ) \\\n  .whenMatchedUpdate(set =\n    {\n\n    }\n  ) \\\n.whenNotMatchedInsert(values =\n    {\n      \"SalesOrderNumber\": \"updates.SalesOrderNumber\",\n      \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",\n      \"OrderDate\": \"updates.OrderDate\",\n      \"CustomerName\": \"updates.CustomerName\",\n      \"Email\": \"updates.Email\",\n      \"Item\": \"updates.Item\",\n      \"Quantity\": \"updates.Quantity\",\n      \"UnitPrice\": \"updates.UnitPrice\",\n      \"Tax\": \"updates.Tax\",\n      \"FileName\": \"updates.FileName\",\n      \"IsFlagged\": \"updates.IsFlagged\",\n      \"CreatedTS\": \"updates.CreatedTS\",\n      \"ModifiedTS\": \"updates.ModifiedTS\"\n    }\n  ) \\\n  .execute()\n</code></pre> </li> <li> <p>Run the cell to execute the code using the  (Run cell) button.</p> <p>This operation is important because it enables you to update existing records in the table based on the values of specific columns, and insert new records when no match is found. This is a common requirement when you're loading data from a source system that may contain updates to existing and new records.</p> </li> </ol> <p>You now have data in your silver delta table that is ready for further transformation and modeling.</p>"},{"location":"labs/03b-medallion-lakehouse/#explore-data-in-the-silver-layer-using-the-sql-endpoint","title":"Explore data in the silver layer using the SQL endpoint","text":"<p>Now that you have data in your silver layer, you can use the SQL analytics endpoint to explore the data and perform some basic analysis. This is useful if you're familiar with SQL and want to do some basic exploration of your data. In this exercise we're using the SQL endpoint view in Fabric, but you can use other tools like SQL Server Management Studio (SSMS) and Azure Data Explorer.</p> <ol> <li> <p>Navigate back to your workspace and notice that you now have several items listed. Select the Sales SQL analytics endpoint to open your lakehouse in the SQL analytics endpoint view.</p> <p></p> </li> <li> <p>Select New SQL query from the ribbon, which will open a SQL query editor. Note that you can rename your query using the ... menu item next to the existing query name in the Explorer pane.</p> <p>Next, you'll run two sql queries to explore the data.</p> </li> <li> <p>Paste the following query into the query editor and select Run:</p> <pre><code>SELECT YEAR(OrderDate) AS Year\n    , CAST (SUM(Quantity * (UnitPrice + Tax)) AS DECIMAL(12, 2)) AS TotalSales\nFROM sales_silver\nGROUP BY YEAR(OrderDate) \nORDER BY YEAR(OrderDate)\n</code></pre> <p>This query calculates the total sales for each year in the sales_silver table. Your results should look like this:</p> <p></p> </li> <li> <p>Next you'll review which customers are purchasing the most (in terms of quantity). Paste the following query into the query editor and select Run:</p> <pre><code>SELECT TOP 10 CustomerName, SUM(Quantity) AS TotalQuantity\nFROM sales_silver\nGROUP BY CustomerName\nORDER BY TotalQuantity DESC\n</code></pre> <p>This query calculates the total quantity of items purchased by each customer in the sales_silver table, and then returns the top 10 customers in terms of quantity.</p> <p>Note: Data exploration at the Silver layer is useful for basic analysis ...</p> <ul> <li>But you need to transform the data further and model it into a star schema.</li> <li>This will enable you to do more advanced analysis and reporting.</li> <li>You'll do that in the next section.</li> </ul> </li> </ol>"},{"location":"labs/03b-medallion-lakehouse/#transform-data-for-gold-layer","title":"Transform data for gold layer","text":"<p>You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you'll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.</p> <p>Note: You could have done all of this in a single notebook ...</p> <ul> <li>But for this exercise you're using separate notebooks</li> <li>This will better demonstrate the process of transforming data from bronze to silver and then from silver to gold.</li> <li>And it makes it easier to do debugging, troubleshooting, and for reuse.</li> </ul> <ol> <li> <p>Return to the workspace home page and create a new notebook called <code>Transform data for Gold</code></p> </li> <li> <p>In the Explorer pane, add your Sales lakehouse by selecting Add data items and then selecting the Sales lakehouse you created earlier. You should see the sales_silver table listed in the Tables section of the explorer pane.</p> </li> <li> <p>In the existing code block, remove the commented text and add the following code to load data to your dataframe and start building your star schema, then run it:</p> <pre><code># Load data to the dataframe as a starting point to create the gold layer\ndf = spark.read.table(\"Sales.sales_silver\")\n</code></pre> <p>If you receive a <code>TooManyRequestsForCapacity</code> error when running the first cell:</p> <ul> <li>Make sure you stopped the session previously running in the first notebook.</li> </ul> </li> <li> <p>Add a new code block and paste the following code to create your date dimension table and run it:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import*\n\n# Define the schema for the dimdate_gold table\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.dimdate_gold\") \\\n    .addColumn(\"OrderDate\", DateType()) \\\n    .addColumn(\"Day\", IntegerType()) \\\n    .addColumn(\"Month\", IntegerType()) \\\n    .addColumn(\"Year\", IntegerType()) \\\n    .addColumn(\"mmmyyyy\", StringType()) \\\n    .addColumn(\"yyyymm\", StringType()) \\\n    .execute()\n</code></pre> <p>Note</p> <ul> <li>You can run the <code>display(df)</code> command at any time to check the progress of your work.</li> <li>In this case, you'd run <code>display(dfdimDate_gold)</code> to see the contents of the dimDate_gold dataframe.</li> </ul> </li> <li> <p>In a new code block, add and run the following code to create a dataframe for your date dimension, dimdate_gold:</p> <pre><code>from pyspark.sql.functions import col, dayofmonth, month, year, date_format\n\n# Create dataframe for dimDate_gold\n\ndfdimDate_gold = df.dropDuplicates([\"OrderDate\"]).select(col(\"OrderDate\"), \\\n        dayofmonth(\"OrderDate\").alias(\"Day\"), \\\n        month(\"OrderDate\").alias(\"Month\"), \\\n        year(\"OrderDate\").alias(\"Year\"), \\\n        date_format(col(\"OrderDate\"), \"MMM-yyyy\").alias(\"mmmyyyy\"), \\\n        date_format(col(\"OrderDate\"), \"yyyyMM\").alias(\"yyyymm\"), \\\n    ).orderBy(\"OrderDate\")\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimDate_gold.head(10))\n</code></pre> </li> <li> <p>You're separating the code out into new code blocks so that you can understand and watch what's happening in the notebook as you transform the data. In another new code block, add and run the following code to update the date dimension as new data comes in:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')\n\ndfUpdates = dfdimDate_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n    dfUpdates.alias('updates'),\n    'gold.OrderDate = updates.OrderDate'\n) \\\n.whenMatchedUpdate(set =\n    {\n\n    }\n) \\\n.whenNotMatchedInsert(values =\n    {\n    \"OrderDate\": \"updates.OrderDate\",\n    \"Day\": \"updates.Day\",\n    \"Month\": \"updates.Month\",\n    \"Year\": \"updates.Year\",\n    \"mmmyyyy\": \"updates.mmmyyyy\",\n    \"yyyymm\": \"updates.yyyymm\"\n    }\n) \\\n.execute()\n</code></pre> <p>The date dimension is now set up. Now you'll create your customer dimension.</p> </li> <li> <p>To build out the customer dimension table, add a new code block, paste and run the following code:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import *\n\n# Create customer_gold dimension delta table\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.dimcustomer_gold\") \\\n    .addColumn(\"CustomerName\", StringType()) \\\n    .addColumn(\"Email\",  StringType()) \\\n    .addColumn(\"First\", StringType()) \\\n    .addColumn(\"Last\", StringType()) \\\n    .addColumn(\"CustomerID\", LongType()) \\\n    .execute()\n</code></pre> </li> <li> <p>In a new code block, add and run the following code to drop duplicate customers, select specific columns, and split the \"CustomerName\" column to create \"First\" and \"Last\" name columns:</p> <pre><code>from pyspark.sql.functions import col, split\n\n# Create customer_silver dataframe\n\ndfdimCustomer_silver = df.dropDuplicates([\"CustomerName\",\"Email\"]).select(col(\"CustomerName\"),col(\"Email\")) \\\n    .withColumn(\"First\",split(col(\"CustomerName\"), \" \").getItem(0)) \\\n    .withColumn(\"Last\",split(col(\"CustomerName\"), \" \").getItem(1)) \n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimCustomer_silver.head(10))\n</code></pre> <p>Here you have created a new DataFrame <code>dfdimCustomer_silver</code> by performing various transformations such as dropping duplicates, selecting specific columns, and splitting the \"CustomerName\" column to create \"First\" and \"Last\" name columns.</p> <p>The result is a DataFrame with cleaned and structured customer data, including separate \"First\" and \"Last\" name columns extracted from the \"CustomerName\"\" column.</p> </li> <li> <p>Next we'll create the ID column for our customers. In a new code block, paste and run the following:</p> <pre><code>from pyspark.sql.functions import monotonically_increasing_id, col, when, coalesce, max, lit\n\ndfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")\n\nMAXCustomerID = dfdimCustomer_temp.select(coalesce(max(col(\"CustomerID\")),lit(0)).alias(\"MAXCustomerID\")).first()[0]\n\ndfdimCustomer_gold = dfdimCustomer_silver.join(dfdimCustomer_temp,(dfdimCustomer_silver.CustomerName == dfdimCustomer_temp.CustomerName) &amp; (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email), \"left_anti\")\n\ndfdimCustomer_gold = dfdimCustomer_gold.withColumn(\"CustomerID\",monotonically_increasing_id() + MAXCustomerID + 1)\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimCustomer_gold.head(10))\n</code></pre> <p>Here you're cleaning and transforming customer data (<code>dfdimCustomer_silver</code>) by performing a left anti join to exclude duplicates that already exist in the <code>dimCustomer_gold</code> table, and then generating unique CustomerID values using the <code>monotonically_increasing_id()</code> function.</p> </li> <li> <p>Now you'll ensure that your customer table remains up-to-date as new data comes in.</p> <p>In a new code block, paste and run the following:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')\n\ndfUpdates = dfdimCustomer_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n    dfUpdates.alias('updates'),\n    'gold.CustomerName = updates.CustomerName AND gold.Email = updates.Email'\n) \\\n.whenMatchedUpdate(set =\n    {\n\n    }\n) \\\n.whenNotMatchedInsert(values =\n    {\n    \"CustomerName\": \"updates.CustomerName\",\n    \"Email\": \"updates.Email\",\n    \"First\": \"updates.First\",\n    \"Last\": \"updates.Last\",\n    \"CustomerID\": \"updates.CustomerID\"\n    }\n) \\\n.execute()\n</code></pre> </li> <li> <p>Now you'll repeat those steps to create your product dimension. </p> <p>In a new code block, paste and run the following:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import *\n\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.dimproduct_gold\") \\\n    .addColumn(\"ItemName\", StringType()) \\\n    .addColumn(\"ItemID\", LongType()) \\\n    .addColumn(\"ItemInfo\", StringType()) \\\n    .execute()\n</code></pre> </li> <li> <p>Add another code block to create the product_silver dataframe.</p> <pre><code>from pyspark.sql.functions import col, split, lit, when\n\n# Create product_silver dataframe\n\ndfdimProduct_silver = df.dropDuplicates([\"Item\"]).select(col(\"Item\")) \\\n    .withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\\n    .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimProduct_silver.head(10))\n</code></pre> </li> <li> <p>Now you'll create IDs for your dimProduct_gold table. </p> <p>Add the following syntax to a new code block and run it:</p> <pre><code>from pyspark.sql.functions import monotonically_increasing_id, col, lit, max, coalesce\n\n#dfdimProduct_temp = dfdimProduct_silver\ndfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")\n\nMAXProductID = dfdimProduct_temp.select(coalesce(max(col(\"ItemID\")),lit(0)).alias(\"MAXItemID\")).first()[0]\n\ndfdimProduct_gold = dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName == dfdimProduct_temp.ItemName) &amp; (dfdimProduct_silver.ItemInfo == dfdimProduct_temp.ItemInfo), \"left_anti\")\n\ndfdimProduct_gold = dfdimProduct_gold.withColumn(\"ItemID\",monotonically_increasing_id() + MAXProductID + 1)\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimProduct_gold.head(10))\n</code></pre> <p>This calculates the next available product ID based on the current data in the table, assigns these new IDs to the products, and then displays the updated product information.</p> </li> <li> <p>Similar to what you've done with your other dimensions, you need to ensure that your product table remains up-to-date as new data comes in.</p> <p>In a new code block, paste and run the following:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')\n\ndfUpdates = dfdimProduct_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n        dfUpdates.alias('updates'),\n        'gold.ItemName = updates.ItemName AND gold.ItemInfo = updates.ItemInfo'\n        ) \\\n        .whenMatchedUpdate(set =\n        {\n\n        }\n        ) \\\n        .whenNotMatchedInsert(values =\n        {\n        \"ItemName\": \"updates.ItemName\",\n        \"ItemInfo\": \"updates.ItemInfo\",\n        \"ItemID\": \"updates.ItemID\"\n        }\n        ) \\\n        .execute()\n</code></pre> <p>Now that you have your dimensions built out, the final step is to create the fact table.</p> </li> <li> <p>In a new code block, paste and run the following code to create the fact table:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import *\n\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.factsales_gold\") \\\n    .addColumn(\"CustomerID\", LongType()) \\\n    .addColumn(\"ItemID\", LongType()) \\\n    .addColumn(\"OrderDate\", DateType()) \\\n    .addColumn(\"Quantity\", IntegerType()) \\\n    .addColumn(\"UnitPrice\", FloatType()) \\\n    .addColumn(\"Tax\", FloatType()) \\\n    .execute()\n</code></pre> </li> <li> <p>In a new code block, paste and run the following code to create a new dataframe to combine sales data with customer and product information include customer ID, item ID, order date, quantity, unit price, and tax:</p> <pre><code>from pyspark.sql.functions import col\n\ndfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")\ndfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")\n\ndf = df.withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\\n    .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \\\n\n# Create Sales_gold dataframe\n\ndffactSales_gold = df.alias(\"df1\").join(dfdimCustomer_temp.alias(\"df2\"),(df.CustomerName == dfdimCustomer_temp.CustomerName) &amp; (df.Email == dfdimCustomer_temp.Email), \"left\") \\\n        .join(dfdimProduct_temp.alias(\"df3\"),(df.ItemName == dfdimProduct_temp.ItemName) &amp; (df.ItemInfo == dfdimProduct_temp.ItemInfo), \"left\") \\\n    .select(col(\"df2.CustomerID\") \\\n        , col(\"df3.ItemID\") \\\n        , col(\"df1.OrderDate\") \\\n        , col(\"df1.Quantity\") \\\n        , col(\"df1.UnitPrice\") \\\n        , col(\"df1.Tax\") \\\n    ).orderBy(col(\"df1.OrderDate\"), col(\"df2.CustomerID\"), col(\"df3.ItemID\"))\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dffactSales_gold.head(10))\n</code></pre> </li> <li> <p>Now you'll ensure that sales data remains up-to-date by running the following code in a new code block:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')\n\ndfUpdates = dffactSales_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n    dfUpdates.alias('updates'),\n    'gold.OrderDate = updates.OrderDate AND gold.CustomerID = updates.CustomerID AND gold.ItemID = updates.ItemID'\n) \\\n.whenMatchedUpdate(set =\n    {\n\n    }\n) \\\n.whenNotMatchedInsert(values =\n    {\n    \"CustomerID\": \"updates.CustomerID\",\n    \"ItemID\": \"updates.ItemID\",\n    \"OrderDate\": \"updates.OrderDate\",\n    \"Quantity\": \"updates.Quantity\",\n    \"UnitPrice\": \"updates.UnitPrice\",\n    \"Tax\": \"updates.Tax\"\n    }\n) \\\n.execute()\n</code></pre> <p>Here you're using Delta Lake's merge operation to synchronize and update the factsales_gold table with new sales data (<code>dffactSales_gold</code>). The operation compares the order date, customer ID, and item ID between the existing data (silver table) and the new data (updates DataFrame), updating matching records and inserting new records as needed.</p> </li> </ol> <p>You now have a curated, modeled gold layer that can be used for reporting and analysis.</p>"},{"location":"labs/03b-medallion-lakehouse/#create-a-semantic-model","title":"Create a semantic model","text":"<p>In your workspace, you can now use the gold layer to create a report and analyze the data. You can access the semantic model directly in your workspace to create relationships and measures for reporting.</p> <p>Note that you can't use the default semantic model that is automatically created when you create a lakehouse. You must create a new semantic model that includes the gold tables you created in this exercise, from the Explorer.</p> <ol> <li> <p>In your workspace, navigate to your Sales lakehouse.</p> </li> <li> <p>Select New semantic model from the ribbon of the Explorer view.</p> </li> <li> <p>Assign the name Sales_Gold to your new semantic model.</p> </li> <li> <p>Select your transformed gold tables to include in your semantic model and select Confirm.</p> <ul> <li>dimdate_gold</li> <li>dimcustomer_gold</li> <li>dimproduct_gold</li> <li>factsales_gold</li> </ul> <p>This will open the semantic model in Fabric where you can create relationships and measures, as shown here:</p> <p></p> </li> </ol> <p>Before you create relationships, you need to have a PowerBI Pro licence</p>"},{"location":"labs/03b-medallion-lakehouse/#get-a-powerbi-licence","title":"Get a PowerBi licence","text":"<ul> <li>Open your semantic model</li> <li>Click the 3 dots ...</li> <li>Click Show tables</li> </ul> <p>You should now be able to create the semantic model as depicted below.</p>"},{"location":"labs/03b-medallion-lakehouse/#create-a-powerbi-report","title":"Create a PowerBi report","text":"<ul> <li>You can now create new report</li> <li>Or just auto create a report: Explore &gt; Auto create report</li> </ul> <p>From here, you or other members of your data team can create reports and dashboards based on the data in your lakehouse. These reports will be connected directly to the gold layer of your lakehouse, so they'll always reflect the latest data.</p>"},{"location":"labs/03b-medallion-lakehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you've learned how to create a medallion architecture in a Microsoft Fabric lakehouse.</p> <p>If you've finished exploring the medallion architecture, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03b-medallion-lakehouse.html </p>"},{"location":"labs/04-ingest-pipeline/","title":"Lab: Ingest Data with a Pipeline in Microsoft Fabric","text":"<p>A data lakehouse is a common analytical data store for cloud-scale analytics solutions. One of the core tasks of a data engineer is to implement and manage the ingestion of data from multiple operational data sources into the lakehouse. In Microsoft Fabric, you can implement extract, transform, and load (ETL) or extract, load, and transform (ELT) solutions for data ingestion through the creation of pipelines.</p> <p>Fabric also supports Apache Spark, enabling you to write and run code to process data at scale. By combining the pipeline and Spark capabilities in Fabric, you can implement complex data ingestion logic that copies data from external sources into the OneLake storage on which the lakehouse is based, and then uses Spark code to perform custom data transformations before loading it into tables for analysis.</p> <p>This lab will take approximately 45 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> </ol> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p>"},{"location":"labs/04-ingest-pipeline/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a uniquename of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new lakehouse with no Tables or Files will be created.</p> <p></p> </li> <li> <p>On the Explorer pane on the left, in the ... menu for the Files node, select New subfolder and create a subfolder named new_data</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-pipeline","title":"Create a pipeline","text":"<p>A simple way to ingest data is to use a Copy Data activity in a pipeline to extract the data from a source and copy it to a file in the lakehouse.</p> <ol> <li> <p>On the Home page for your lakehouse, select Get data and then select New data pipeline, and create a new data pipeline named <code>Ingest Sales Data</code></p> </li> <li> <p>If the Copy Data wizard doesn't open automatically, select Copy Data &gt; Use copy assistant in the pipeline editor page.</p> </li> <li> <p>In the Copy Data wizard, on the Choose data source page, type HTTP in the search bar and then select HTTP in the New sources section.</p> <p></p> </li> <li> <p>In the Connect to data source pane, enter the following settings for the connection to your data source:</p> <ul> <li>URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv</li> <li>Connection: Create new connection</li> <li>Connection name: Specify a unique name</li> <li>Data gateway: (none)</li> <li>Authentication kind: Anonymous</li> </ul> </li> <li> <p>Select Next. Then ensure the following settings are selected:</p> <ul> <li>Relative URL: Leave blank</li> <li>Request method: GET</li> <li>Additional headers: Leave blank</li> <li>Binary copy: Unselected</li> <li>Request timeout: Leave blank</li> <li>Max concurrent connections: Leave blank</li> </ul> </li> <li> <p>Select Next, and wait for the data to be sampled and then ensure that the following settings are selected:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>First row as header: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>Select Preview data to see a sample of the data that will be ingested. Then close the data preview and select Next.</p> </li> <li> <p>On the Connect to data destination page, set the following data destination options, and then select Next:</p> <ul> <li>Root folder: Files</li> <li>Folder path name: new_data</li> <li>File name: sales.csv</li> <li>Copy behavior: None</li> </ul> </li> <li> <p>Set the following file format options and then select Next:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>Add header to file: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>On the Copy summary page, review the details of your copy operation and then select Save + Run.</p> <p>A new pipeline containing a Copy Data activity is created, as shown here:</p> <p></p> </li> <li> <p>When the pipeline starts to run, you can monitor its status in the Output pane under the pipeline designer. Use the  (Refresh) icon to refresh the status, and wait until it has succeeeded.</p> </li> <li> <p>In the menu bar on the left, select your lakehouse.</p> </li> <li> <p>On the Home page, in the Explorer pane, expand Files and select the new_data folder to verify that the sales.csv file has been copied.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-notebook","title":"Create a notebook","text":"<ol> <li> <p>On the Home page for your lakehouse, in the Open notebook menu, select New notebook.</p> <p>After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).</p> </li> <li> <p>Select the existing cell in the notebook, which contains some simple code, and then replace the default code with the following variable declaration.</p> <pre><code>table_name = \"sales\"\n</code></pre> </li> <li> <p>In the ... menu for the cell (at its top-right) select Toggle parameter cell. This configures the cell so that the variables declared in it are treated as parameters when running the notebook from a pipeline.</p> </li> <li> <p>Under the parameters cell, use the + Code button to add a new code cell. Then add the following code to it:</p> <pre><code>from pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n</code></pre> <p>This code loads the data from the sales.csv file that was ingested by the Copy Data activity, applies some transformation logic, and saves the transformed data as a table - appending the data if the table already exists.</p> </li> <li> <p>Verify that your notebooks looks similar to this, and then use the  Run all button on the toolbar to run all of the cells it contains.</p> <p></p> <p>Note</p> <ul> <li>Since this is the first time you've run any Spark code in this session, the Spark pool must be started.</li> <li>This means that the first cell can take a minute or so to complete.</li> </ul> </li> <li> <p>When the notebook run has completed, in the Explorer pane on the left, in the ... menu for Tables select Refresh and verify that a sales table has been created.</p> </li> <li> <p>In the notebook menu bar, use the \u2699\ufe0f Settings icon to view the notebook settings. Then set the Name of the notebook to <code>Load Sales</code> and close the settings pane.</p> </li> <li> <p>In the hub menu bar on the left, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, refresh the view. Then expand Tables, and select the sales table to see a preview of the data it contains.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#modify-the-pipeline","title":"Modify the pipeline","text":"<p>Now that you've implemented a notebook to transform data and load it into a table, you can incorporate the notebook into a pipeline to create a reusable ETL process.</p> <ol> <li> <p>In the hub menu bar on the left select the Ingest Sales Data pipeline you created previously.</p> </li> <li> <p>On the Activities tab, in the All activities list, select Delete data. Then position the new Delete data activity to the left of the Copy data activity and connect its On completion output to the Copy data activity, as shown here:</p> <p></p> </li> <li> <p>Select the Delete data activity, and in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Delete old files</code></li> </ul> </li> <li> <p>Source:</p> <ul> <li>Connection: Your lakehouse</li> <li>File path type: Wildcard file path</li> <li>Folder path: Files / new_data</li> <li>Wildcard file name: <code>*.csv</code></li> <li>Recursively: Selected</li> </ul> </li> <li> <p>Logging settings:</p> <ul> <li>Enable logging: Unselected</li> </ul> </li> </ul> <p>These settings will ensure that any existing .csv files are deleted before copying the sales.csv file.</p> </li> <li> <p>In the pipeline designer, on the Activities tab, select Notebook to add a Notebook activity to the pipeline.</p> </li> <li> <p>Select the Copy data activity and then connect its On Completion output to the Notebook activity as shown here:</p> <p></p> </li> <li> <p>Select the Notebook activity, and then in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Load Sales notebook</code></li> </ul> </li> <li> <p>Settings:</p> <ul> <li>Notebook: Load Sales</li> <li>Base parameters: Add a new parameter with the following properties:</li> </ul> Name Type Value table_name String new_sales </li> </ul> <p>The table_name parameter will be passed to the notebook and override the default value assigned to the table_name variable in the parameters cell.</p> </li> <li> <p>On the Home tab, use the  (Save) icon to save the pipeline. Then use the  Run button to run the pipeline, and wait for all of the activities to complete.</p> <p></p> <p>If you see an error message</p> <ul> <li>In case you receive the error message:<ul> <li>Spark SQL queries are only possible in the context of a lakehouse. Please attach a lakehouse to proceed:</li> </ul> </li> <li>Open your notebook, select the lakehouse you created on the left pane,</li> <li>select Remove all Lakehouses and then add it again.</li> <li>Go back to the pipeline designer and select  Run.</li> </ul> </li> <li> <p>In the hub menu bar on the left edge of the portal, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, expand Tables and select the new_sales table to see a preview of the data it contains. This table was created by the notebook when it was run by the pipeline.</p> </li> </ol> <p>In this exercise, you implemented a data ingestion solution that uses a pipeline to copy data to your lakehouse from an external source, and then uses a Spark notebook to transform the data and load it into a table.</p>"},{"location":"labs/04-ingest-pipeline/#dont-delete-the-workspace","title":"Don't delete the workspace","text":"<p>Do not delete the workspace as you will need it in the next acticity.</p> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/04-ingest-pipeline.html </p>"},{"location":"labs/05-dataflows-gen2/","title":"Lab: Create and use Dataflows (Gen2) in Microsoft Fabric","text":"<p>In Microsoft Fabric, Dataflows (Gen2) connect to various data sources and perform transformations in Power Query Online. They can then be used in Data Pipelines to ingest data into a lakehouse or other analytical store, or to define a dataset for a Power BI report.</p> <p>This lab is designed to introduce the different elements of Dataflows (Gen2), and not create a complex solution that may exist in an enterprise. This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-dataflow-gen2-to-ingest-data","title":"Create a Dataflow (Gen2) to ingest data","text":"<p>Now that you have a lakehouse, you need to ingest some data into it. One way to do this is to define a dataflow that encapsulates an extract, transform, and load (ETL) process.</p> <ol> <li> <p>In the home page for your lakehouse, select Get data &gt; New Dataflow Gen2</p> <p></p> <p>Click Create, and after a few seconds, the Power Query editor for your new dataflow opens as shown here:</p> <p></p> </li> <li> <p>Select Import from a Text/CSV file, and create a new data source with the following settings:</p> <ul> <li>Link to file: Selected</li> <li>File path or URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv</li> <li>Connection: Create new connection</li> <li>Connection Name: default value ~ or orders.csv if name already exists</li> <li>data gateway: (none)</li> <li>Authentication kind: Anonymous</li> <li>Privacy Level: None</li> </ul> </li> <li> <p>Select Next to preview the file data, and then Create the data source.</p> <p>The Power Query editor shows the data source and an initial set of query steps to format the data, as shown here:</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Add column tab. Then select Custom column and create a new column.</p> <p></p> </li> <li> <p>Do the following:</p> <ul> <li>Set the New column name to: MonthNo</li> <li>Set the Data type to: Whole number</li> <li>Add this Custom column formula: <code>Date.Month([OrderDate])</code></li> </ul> <p></p> </li> <li> <p>Click OK to create the column. Notice how the step to add the custom column is added to the query.</p> <p>The resulting column is displayed in the data pane:</p> <p></p> <p>Info</p> <ul> <li>In the Query Settings pane on the right side, notice the Applied Steps include each transformation step.</li> <li>At the bottom, you can also toggle the Diagram view button to turn on the Visual Diagram of the steps.</li> </ul> <p>Info</p> <p>Steps can be moved up or down, edited by selecting the gear icon, and you can select each step to see the transformations apply in the preview pane.</p> </li> <li> <p>Check and confirm that the data type for the OrderDate column is set to Date and the data type for the newly created column MonthNo is set to Whole Number.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#add-data-destination-for-dataflow","title":"Add data destination for Dataflow","text":"<ol> <li> <p>On the toolbar ribbon, select the Home tab. Then in the Add data destination drop-down menu, select Lakehouse.</p> <p>Note</p> <ul> <li>If this option is grayed out, you may already have a data destination set.</li> <li>Check the data destination at the bottom of the Query settings pane on the right side of the Power Query editor.</li> <li>If a default destination is already set, you can remove it and add a new one.</li> </ul> </li> <li> <p>In the Connect to data destination dialog box, use the existing connection credentials:</p> <p></p> </li> <li> <p>Select Next and in the list of available workspaces, find your workspace and select the lakehouse you created in it at the start of this exercise. Then specify a new table named orders:</p> <p></p> </li> <li> <p>Select Next and on the Choose destination settings page:</p> <ul> <li>Disable the Use automatic settings option, select Append, and then Save settings.</li> </ul> <p></p> </li> <li> <p>On the Menu bar, open View and select Diagram view. Notice the Lakehouse destination is indicated as an icon in the query in the Power Query editor.</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Home tab. Then select Save &amp; run and wait for the Dataflow 1 dataflow to be created in your workspace.</p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#add-a-dataflow-to-a-pipeline","title":"Add a dataflow to a pipeline","text":"<p>You can include a dataflow as an activity in a pipeline. Pipelines are used to orchestrate data ingestion and processing activities, enabling you to combine dataflows with other kinds of operation in a single, scheduled process. Pipelines can be created in a few different experiences, including Data Factory experience.</p> <ol> <li> <p>From your Fabric-enabled workspace, select + New item &gt; Data pipeline</p> <ul> <li>When prompted, create a new pipeline named: Load data</li> </ul> <p>Click Create, and the pipeline editor will open:</p> <p></p> <p>If the Copy Data wizard opens automatically, you can just close it.</p> </li> <li> <p>Select Pipeline activity, and add a Dataflow activity to the pipeline.</p> </li> <li> <p>With the new Dataflow1 activity selected, on the Settings tab, in the Dataflow drop-down list, select Dataflow 1 (the data flow you created previously)</p> <p></p> </li> <li> <p>On the Home tab, save the pipeline using the  (Save) icon.</p> </li> <li> <p>Use the  Run button to run the pipeline, and wait for it to complete. It may take a few minutes.</p> <p></p> </li> <li> <p>In the menu bar on the left edge, select your lakehouse.</p> </li> <li> <p>In the ... menu for Tables, select refresh. </p> <p>Then expand Tables and select the orders table, which has been created by your dataflow.</p> <p></p> </li> </ol> Tip for Power Bi Desktop users: <ul> <li>In Power BI Desktop, you can connect directly to the data transformations done with your dataflow by using the Power BI dataflows (Legacy) connector.</li> <li>You can also make additional transformations, publish as a new dataset, and distribute with intended audience for specialized datasets.</li> </ul> <p></p>"},{"location":"labs/05-dataflows-gen2/#clean-up-resources","title":"Clean up resources","text":"<p>If you've finished exploring dataflows in Microsoft Fabric, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/05-dataflows-gen2.html </p>"},{"location":"labs/06c-monitor-data-warehouse/","title":"Lab: Monitor a data warehouse in Microsoft Fabric","text":"<p>In Microsoft Fabric, a data warehouse provides a relational database for large-scale analytics. Data warehouses in Microsoft Fabric include dynamic management views that you can use to monitor activity and queries.</p> <p>This lab will take approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"labs/06c-monitor-data-warehouse/#signing-in-to-microsoft-fabric","title":"Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/06c-monitor-data-warehouse/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/06c-monitor-data-warehouse/#create-a-sample-data-warehouse","title":"Create a sample data warehouse","text":"<p>Now that you have a workspace, it\u2019s time to create a data warehouse.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Warehouse section, select Sample warehouse and create a new data warehouse named <code>sample-dw</code></p> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new warehouse will be created and populated with sample data for a taxi ride analysis scenario.</p> <p></p> </li> </ol>"},{"location":"labs/06c-monitor-data-warehouse/#explore-dynamic-management-views","title":"Explore dynamic management views","text":"<p>Microsoft Fabric data warehouses include dynamic management views (DMVs), which you can use to identify current activity in the data warehouse instance.</p> <ol> <li> <p>In the sample-dw data warehouse page, in the New SQL query drop-down list, select New SQL query.</p> </li> <li> <p>In the new blank query pane, enter the following Transact-SQL code to query the <code>sys.dm_exec_connections</code> DMV:</p> <pre><code>SELECT * FROM sys.dm_exec_connections;\n</code></pre> </li> <li> <p>Use the  Run button to run the SQL script and view the results, which include details of all connections to the data warehouse.</p> </li> <li> <p>Modify the SQL code to query the <code>sys.dm_exec_sessions</code> DMV, like this:</p> <pre><code>SELECT * FROM sys.dm_exec_sessions;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all authenticated sessions.</p> </li> <li> <p>Modify the SQL code to query the sys.dm_exec_requests DMV, like this:</p> <pre><code>SELECT * FROM sys.dm_exec_requests;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all requests being executed in the data warehouse.</p> </li> <li> <p>Modify the SQL code to join the DMVs and return information about currently running requests in the same database, like this:</p> <pre><code>SELECT connections.connection_id,\n       sessions.session_id, sessions.login_name, sessions.login_time,\n       requests.command, requests.start_time, requests.total_elapsed_time\nFROM sys.dm_exec_connections AS connections\nINNER JOIN sys.dm_exec_sessions AS sessions\nON connections.session_id=sessions.session_id\nINNER JOIN sys.dm_exec_requests AS requests\nON requests.session_id = sessions.session_id\nWHERE requests.status = 'running'\nAND requests.database_id = DB_ID()\nORDER BY requests.total_elapsed_time DESC;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all running queries in the database (including this one).</p> </li> <li> <p>In the New SQL query drop-down list, select New SQL query to add a second query tab. Then in the new empty query tab, run the following code:</p> <pre><code>WHILE 1 = 1\n    SELECT * FROM Trip;\n</code></pre> <p>What does this code do: <code>WHILE 1 = 1</code>?</p> </li> <li> <p>Leave the query running, and return to the tab containing the code to query the DMVs and re-run it. This time, the results should include the second query that is running in the other tab. Note the elapsed time for that query.</p> </li> <li> <p>Wait a few seconds and re-run the code to query the DMVs again. The elapsed time for the query in the other tab should have increased.</p> </li> <li> <p>Return to the second query tab where the query is still running and select Cancel to cancel it.</p> </li> <li> <p>Back on the tab with the code to query the DMVs, re-run the query to confirm that the second query is no longer running.</p> </li> <li> <p>Close all query tabs.</p> </li> </ol> <p>Further Information</p> <p>See Monitor connections, sessions, and requests using DMVs in the Microsoft Fabric documentation for more information about using DMVs.</p>"},{"location":"labs/06c-monitor-data-warehouse/#explore-query-insights","title":"Explore query insights","text":"<p>Microsoft Fabric data warehouses provide query insights - a special set of views that provide details about the queries being run in your data warehouse.</p> <ol> <li> <p>In the sample-dw data warehouse page, in the New SQL query drop-down list, select New SQL query.</p> </li> <li> <p>In the new blank query pane, enter the following Transact-SQL code to query the exec_requests_history view:</p> <pre><code>SELECT * FROM queryinsights.exec_requests_history;\n</code></pre> </li> <li> <p>Use the  Run button to run the SQL script and view the results, which include details of previously executed queries.</p> </li> <li> <p>Modify the SQL code to query the frequently_run_queries view, like this:</p> <pre><code>SELECT * FROM queryinsights.frequently_run_queries;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of frequently run queries.</p> </li> <li> <p>Modify the SQL code to query the long_running_queries view, like this:</p> <pre><code>SELECT * FROM queryinsights.long_running_queries;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all queries and their durations.</p> </li> </ol> <p>Further Information</p> <p>See Query insights in Fabric data warehousing in the Microsoft Fabric documentation for more information about using query insights.</p>"},{"location":"labs/06c-monitor-data-warehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have used dynamic management views and query insights to monitor activity in a Microsoft Fabric data warehouse.</p> <p>If you've finished exploring your data warehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/06c-monitor-data-warehouse.html </p>"},{"location":"labs/11-data-activator/","title":"Lab: Use Data Activator in Fabric","text":"<p>Data Activator in Microsoft Fabric takes action based on what\u2019s happening in your data. Data Activator lets you monitor your data and create triggers to react to your data changes.</p> <p>https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-tutorial</p> <p>IMPORTANT!</p> <p>This exercise is deprecated, and will be removed or updated soon. The instructions are no longer accurate, and the exercise is unsupported.</p> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/11-data-activator.html </p>"},{"location":"labs/18-monitor-hub/","title":"Lab: Monitor Fabric Activity in the Monitoring Hub","text":"<p>The monitoring hub in Microsoft Fabric provides a central place where you can monitor activity. You can use the monitoring hub to review events related to items you have permission to view.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/18-monitor-hub/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/18-monitor-hub/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> </ol>"},{"location":"labs/18-monitor-hub/#create-and-monitor-a-dataflow","title":"Create and monitor a Dataflow","text":"<p>In Microsoft Fabric, you can use a Dataflow (Gen2) to ingest data from a wide range of sources. In this exercise, you\u2019ll use a dataflow to get data from a CSV file and load it into a table in your lakehouse.</p> <ol> <li> <p>On the Home page for your lakehouse, in the Get data menu, select New Dataflow Gen2.</p> </li> <li> <p>Name the new dataflow <code>Get Product Data</code> and select Create.</p> <p></p> </li> <li> <p>In the dataflow designer, select Import from a Text/CSV file. Then complete the Get Data wizard to create a data connection by linking to <code>https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/products.csv</code> using anonymous authentication.</p> <p>When you have completed the wizard, a preview of the data will be shown in the dataflow designer like this:</p> <p></p> </li> <li> <p>Publish the dataflow.</p> </li> <li> <p>In the navigation bar on the left, select Monitor to view the monitoring hub and observe that your dataflow is in-progress (if not, refresh the view until you see it).</p> <p></p> </li> <li> <p>Wait for a few seconds, and then refresh the page until the status of the dataflow is Succeeded.</p> </li> <li> <p>In the navigation pane, select your lakehouse. Then expand the Tables folder to verify that a table named products has been created and loaded by the dataflow (you may need to refresh the Tables folder).</p> <p></p> </li> </ol>"},{"location":"labs/18-monitor-hub/#create-and-monitor-a-spark-notebook","title":"Create and monitor a Spark notebook","text":"<p>In Microsoft Fabric, you can use notebooks to run Spark code.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Notebook.</p> <p>A new notebook named Notebook 1 is created and opened.</p> <p></p> </li> <li> <p>At the top left of the notebook, select Notebook 1 to view its details, and change its name to <code>Query Products</code></p> </li> <li> <p>In the notebook editor, in the Explorer pane, select Add data items and then select Existing data sources.</p> </li> <li> <p>Add the lakehouse you created previously.</p> </li> <li> <p>Expand the lakehouse item until you reach the products table.</p> </li> <li> <p>In the ... menu for the products table, select Load data &gt; Spark. This adds a new code cell to the notebook as shown here:</p> <p></p> </li> <li> <p>Use the  Run all button to run all cells in the notebook. It will take a moment or so to start the Spark session, and then the results of the query will be shown under the code cell.</p> <p></p> </li> <li> <p>On the toolbar, use the   (Stop session) button to stop the Spark session.</p> </li> <li> <p>In the navigation bar, select Monitor to view the monitoring hub, and note that the notebook activity is listed.</p> <p></p> </li> </ol>"},{"location":"labs/18-monitor-hub/#monitor-history-for-an-item","title":"Monitor history for an item","text":"<p>Some items in a workspace might be run multiple times. You can use the monitoring hub to view their run history.</p> <ol> <li> <p>In the navigation bar, return to the page for your workspace. Then use the  (Refresh now) button for your Get Product Data dataflow to re-run it.</p> </li> <li> <p>In the navigation pane, select the Monitor page to view the monitoring hub and verify that the dataflow is in-progress.</p> </li> <li> <p>In the ... menu for the Get Product Data dataflow, select Historical runs to view the run history for the dataflow:</p> <p></p> </li> <li> <p>In the ... menu for any of the historical runs select View detail to see details of the run.</p> </li> <li> <p>Close the Details pane and use the Back to main view button to return to the main monitoring hub page.</p> </li> </ol>"},{"location":"labs/18-monitor-hub/#customize-monitoring-hub-views","title":"Customize monitoring hub views","text":"<p>In this exercise you\u2019ve only run a few activities, so it should be fairly easy to find events in the monitoring hub. However, in a real environment you may need to search through a large number of events. Using filters and other view customizations can make this easier.</p> <ol> <li> <p>In the monitoring hub, use the Filter button to apply the following filter:</p> <ul> <li>Status: Succeeeded</li> <li>Item type: Dataflow Gen2</li> </ul> <p>With the filter applied, only successful runs of dataflows are listed.</p> <p></p> </li> <li> <p>Use the Column Options button to include the following columns in the view (use the Apply button to apply the changes):</p> <ul> <li>Activity name</li> <li>Status</li> <li>Item type</li> <li>Start time</li> <li>Submitted by</li> <li>Location</li> <li>End time</li> <li>Duration</li> <li>Refresh type</li> </ul> <p>You may need to scroll horizontally to see all of the columns:</p> <p></p> </li> </ol>"},{"location":"labs/18-monitor-hub/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse, a dataflow, and a Spark notebook; and you\u2019ve used the monitoring hub to view item activity.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/18-monitor-hub.html </p>"},{"location":"labs/19-secure-data-access%20copy/","title":"Lab: Secure Data Access in Microsoft Fabric","text":"<p>Microsoft Fabric has a multi-layer security model for managing data access. Security can be set for an entire workspace, for individual items, or through granular permissions in each Fabric engine. In this exercise, you secure data using workspace, and item access controls and OneLake data access roles.</p> <p>This lab takes approximately 45 minutes to complete.</p>"},{"location":"labs/19-secure-data-access%20copy/#create-a-workspace","title":"Create a workspace","text":"<ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol> <p>When you create a workspace, you automatically become a member of the Workspace Admin role.</p>"},{"location":"labs/19-secure-data-access%20copy/#create-a-data-warehouse","title":"Create a data warehouse","text":"<p>Next, create a data warehouse in the workspace you created:</p> <ol> <li> <p>Click + New Item. On the New item page, under the Store Data section, select Sample warehouse and create a new data warehouse with a name of your choice.</p> <p>After a minute or so, a new warehouse will be created:</p> <p></p> </li> </ol>"},{"location":"labs/19-secure-data-access%20copy/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Next, create a lakehouse in the workspace you created.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> <li> <p>Select the Start with sample data tile and then select the Public holidays sample. After a minute or so, the lakehouse will be populated with data.</p> </li> </ol>"},{"location":"labs/19-secure-data-access%20copy/#apply-workspace-access-controls","title":"Apply workspace access controls","text":"<p>Workspace roles are used to control access to workspaces and the content within them. Workspace roles can be assigned when users need to see all items in a workspace, when they need to manage workspace access, or create new Fabric items, or when they need specific permissions to view, modify or share content in the workspace.</p> <p>In this exercise, you add a user to a workspace role, apply permissions and, see what is viewable when each set of permissions is applied. You will also be added to another users workspace and you will be able to view their workspace items.</p> <p>In your workspace you'll be a Workspace Admin and in the other workspaces you'll be a less privileged user.</p> <ol> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Next select the workspace you created.</p> </li> <li> <p>Select on Manage access on the top of the screen.</p> <p>You'll see the user you're logged, who is a a member of the Workspace Admin role because you created the workspace.</p> <ul> <li>No other users are assigned access to the workspace yet.</li> </ul> </li> <li> <p>Next, you'll see what a user without permissions on the workspace can view. In your browser, open an InPrivate window. In the Microsoft Edge browser, select the ellipse at the top right corner and select New InPrivate Window.</p> </li> <li> <p>Enter https://app.fabric.microsoft.com and sign-in as the second user you're using for testing.</p> </li> </ol> <p>On the bottom left corner of your screen, select Microsoft Fabric and then select Data Warehouse. Next select Workspaces (the icon looks similar to \ud83d\uddc7).</p> <p>Note: The second user doesn't have access to the workspace, so it's not viewable.</p> <p>Next, you assign the Workspace Viewer role to the second user and see that the role grants read access to the warehouse in the workspace. Return to the browser window where you're logged in as the Workspace Admin. Ensure you're still on the page that shows the workspace you created. It should have your new workspace items, and the sample warehouse and lakehouse, listed at the bottom of the page. Select Manage access at the top right of the screen. Select Add people or groups. Enter the email of the second user you're testing with. Select Add to assign the user to the workspace Viewer role. Return to the InPrivate browser window where you're logged in as the second user and select refresh button on the browser to refresh session permissions assigned to the second user. Select the Workspaces icon on the left menu bar (the icon looks similar to \ud83d\uddc7) and select on the workspace name you created as the Workspace Admin user. The second user can now see all of the items in the workspace because they were assigned the Workspace Viewer role.</p> <p>Screenshot of workspace items in Fabric.</p> <p>Select the warehouse and open it. Select the Date table and wait for the rows to be loaded. You can see the rows because as a member of the Workspace Viewer role, you have CONNECT and ReadData permission on tables in the warehouse. For more information on permissions granted to the Workspace Viewer role, see Workspace roles. Next, select the Workspaces icon on the left menu bar, then select the lakehouse. When the lakehouse opens, click on the dropdown box at the top right corner of the screen that says Lakehouse and select SQL analytics endpoint. Select the publicholidays table and wait for the data to be displayed. Data in the lakehouse table is readable from the SQL analytics endpoint because the user is a member of the Workspace Viewer role that grants read permissions on the SQL analytics endpoint. Apply item access control Item permissions control access to individual Fabric items within a workspace, like warehouses, lakehouses and semantic models. In this exercise, you remove the Workspace Viewer permissions applied in the previous exercise and then apply item level permissions on the warehouse so a less privileged user can only view the warehouse data, not the lakehouse data.</p> <p>Return to the browser window where you're logged in as the Workspace Admin. Select Workspaces from the left navigation pane. Select the workspace that you created to open it. Select Manage access from the top of the screen. Select the word Viewer under the name of the second user. On the menu that appears, select Remove.</p> <p>Screenshot of workspace access dropdown in Fabric.</p> <p>Close the Manage access section. In the workspace, hover over the name of your warehouse and an ellipse (\u2026) will appear. Select the ellipse and select Manage permissions</p> <p>Select Add user and enter the name of the second user. In the box that appears, under Additional permissions check Read all data using SQL (ReadData) and uncheck all other boxes.</p> <p>Screenshot of warehouse permissions being granted in Fabric.</p> <p>Select Grant</p> <p>Return to the browser window where you're logged in as the second user. Refresh the browser view.</p> <p>The second user no longer has access to the workspace and instead has access to only the warehouse. You can no longer browse workspaces on the left navigation pane to find the warehouse. Select OneLake on the left navigation menu to find the warehouse.</p> <p>Select the warehouse. On the screen that appears, select Open from the top menu bar.</p> <p>When the warehouse view appears, select the Date table to view table data. The rows are viewable because the user still has read access to the warehouse because ReadData permissions were applied by using item permissions on the warehouse. Apply OneLake data access roles in a Lakehouse OneLake data access roles let you create custom roles within a Lakehouse and grant read permissions to folders you specify. OneLake data access roles is currently a Preview feature.</p> <p>In this exercise, you assign an item permission and create a OneLake data access role and experiment with how they work together to restrict access to data in a Lakehouse.</p> <p>Stay in the browser where you're logged in as the second user. Select OneLake on the left navigation bar. The second user doesn't see the lakehouse. Return to the browser where you're logged in as the Workspace Admin. Select Workspaces on the left menu and select your workspace. Hover over the name of the lakehouse. Select on the ellipse (\u2026) to the right of the ellipse and select Manage permissions</p> <p>Screenshot of setting permissions on a lakehouse in Fabric.</p> <p>On the screen that appears, select Add user. Assign the second user to the lakehouse and ensure none of the checkboxes on the Grant People Access window are checked.</p> <p>Screenshot of the grant access lakehouse window in Fabric.</p> <p>Select Grant. The second user now has read permissions on the lakehouse. Read permission only allows the user to see metadata for the lakehouse but not the underlying data. Next we'll validate this. Return to the browser where you're logged in as the second user. Refresh the browser. Select OneLake in the left navigation pane. Select the lakehouse and open it. Select Open on the top menu bar. You're unable to expand the tables or files even though read permission was granted. Next, you grant the second user access to a specific folder using OneLake data access permissions. Return to the browser where you're logged in as the workspace administrator. Select Workspaces from the left navigation bar. Select your workspace name. Select the lakehouse. When the lakehouse opens, select Manage OneLake data access on the top menu bar and enable the feature by clicking the Continue button.</p> <p>Screenshot of the Manage OneLake data access (preview) feature on the menu bar in Fabric.</p> <p>Select new role on the Manage OneLake data access (preview) screen that appears.</p> <p>Screenshot of the new role functionality in the manage OneLake data access feature.</p> <p>Create a new role called publicholidays that can only access the publicholidays folder as shown in the screenshot below.</p> <p>Screenshot of the folder assignment in the manage OneLake data access feature.</p> <p>When the role finishes creating, select Assign role and assign the role to your second user, select Add and, select Save.</p> <p>Screenshot of the folder assignment in the manage OneLake data access feature.</p> <p>Return to the browser where you're logged in as the second user. Ensure you're still on the page where the lakehouse is open. Refresh the browser. Select the publicholidays table and wait for the data to load. Only the data in the publicholidays table is accessible to the user because the user was assigned to the custom OneLake data access role. The role permits them to see only the data in the publicholidays table, not data in any of the other tables, files, or folders.</p>"},{"location":"labs/19-secure-data-access%20copy/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you secured data using workspace access controls, item access controls and, OneLake data access roles.</p> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/19-secure-data-access.html </p>"},{"location":"labs/19-secure-data-access/","title":"Lab: Secure Data Access in Microsoft Fabric","text":"<p>Microsoft Fabric has a multi-layer security model for managing data access. Security can be set for an entire workspace, for individual items, or through granular permissions in each Fabric engine. In this exercise, you secure data using workspace, and item access controls and OneLake data access roles.</p> <p>This lab takes approximately 45 minutes to complete.</p>"},{"location":"labs/19-secure-data-access/#create-a-workspace","title":"Create a workspace","text":"<ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol> <p>When you create a workspace, you automatically become a member of the Workspace Admin role.</p>"},{"location":"labs/19-secure-data-access/#create-a-data-warehouse","title":"Create a data warehouse","text":"<p>Next, create a data warehouse in the workspace you created:</p> <ol> <li> <p>Click + New Item. On the New item page, under the Store Data section, select Sample warehouse and create a new data warehouse with a name of your choice.</p> <p>After a minute or so, a new warehouse will be created:</p> <p></p> </li> </ol>"},{"location":"labs/19-secure-data-access/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Next, create a lakehouse in the workspace you created.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> <li> <p>Select the Start with sample data tile and then select the Public holidays sample. After a minute or so, the lakehouse will be populated with data.</p> </li> </ol>"},{"location":"labs/19-secure-data-access/#apply-workspace-access-controls","title":"Apply workspace access controls","text":"<p>To continue this lab ... use the MS Learn instructions</p> <p>https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/19-secure-data-access.html</p>"},{"location":"labs/21-deployment-pipelines%20copy/","title":"Lab: Implement Deployment Pipelines","text":"<p>Implement deployment pipelines in Microsoft Fabric Deployment pipelines in Microsoft Fabric let you automate the process of copying changes made to the content in Fabric items between environments like development, test, and production. You can use deployment pipelines to develop and test content before it reaches end users. In this exercise, you create a deployment pipeline, and assign stages to the pipeline. Then you create some content in a development workspace and use deployment pipelines to deploy it between the Development, Test and Production pipeline stages.</p> <p>Note: To complete this exercise, you need to be an member of the Fabric workspace admin role. To assign roles see Roles in workspaces in Microsoft Fabric.</p> <p>This lab takes approximately 20 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> </ol> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p>"},{"location":"labs/21-deployment-pipelines%20copy/#create-workspaces","title":"Create workspaces","text":"<p>Create three workspaces.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a new workspace named: Development</p> </li> <li> <p>Create a new workspace named: Test</p> </li> <li> <p>Create a new workspace named: Production</p> </li> <li> <p>Select the Workspaces icon on the menu bar on the left and confirm that there are three workspaces named: Development, Test, and Production</p> <p>Note</p> <p>If you are prompted to enter a unique name for the workspaces, append one or more random numbers to the words: Development, Test, or Production.</p> </li> </ol>"},{"location":"labs/21-deployment-pipelines%20copy/#create-a-deployment-pipeline","title":"Create a deployment pipeline","text":"<p>Next, create a deployment pipeline.</p> <ol> <li> <p>In the menu bar on the left, select Workspaces.</p> </li> <li> <p>Select Deployment Pipelines, then New pipeline.</p> </li> <li> <p>In the Add a new deployment pipeline window, give the pipeline a unique name and select Next.</p> </li> <li> <p>In the new pipeline window, select Create and continue.</p> </li> </ol>"},{"location":"labs/21-deployment-pipelines%20copy/#assign-workspaces-to-stages-of-a-deployment-pipeline","title":"Assign workspaces to stages of a deployment pipeline","text":"<p>Assign workspaces to the stages of the deployment pipeline.</p> <ol> <li> <p>On the left menu bar, select the pipeline you created.</p> </li> <li> <p>In the window that appears, expand the options under Assign a workspace on each deployment stage and select the name of the workspace that matches the name of the stage.</p> </li> <li> <p>Select the check mark Assign for each deployment stage.</p> <p></p> </li> </ol>"},{"location":"labs/21-deployment-pipelines%20copy/#create-content","title":"Create content","text":"<p>Fabric items haven\u2019t been created in your workspaces yet. Next, create a lakehouse in the development workspace.</p> <p>In the menu bar on the left, select Workspaces. Select the Development workspace. Select New Item. In the window that appears, select Lakehouse and in the New lakehouse window, name the lakehouse, LabLakehouse. Select Create. In the Lakehouse Explorer window, select Start with sample data to populate the new lakehouse with data. Screenshot of Lakehouse Explorer.</p> <p>Select the sample NYCTaxi. In the menu bar on the left, select the pipeline you created. Select the Development stage, and under the deployment pipeline canvas you can see the lakehouse you created as a stage item. In the left edge of the Test stage, there\u2019s an X within a circle. The X indicates that the Development and Test stages aren\u2019t synchronized. Select the Test stage and under the deployment pipeline canvas you can see that the lakehouse you created is only a stage item in the source, which in this case refers to the Development stage. Screenshot the deployment pipeline showing content mismatches between stages.</p> <p>Deploy content between stages Deploy the lakehouse from the Development stage to the Test and Production stages.</p> <p>Select the Test stage in the deployment pipeline canvas. Under the deployment pipeline canvas, select the checkbox next to the Lakehouse item. Then select the Deploy button to copy the lakehouse in its current state to the Test stage. In the Deploy to next stage window that appears, select Deploy. There is now an X in a circle in the Production stage in the deployment pipeline canvas. The lakehouse exists in the Development and Test stages but not yet in the Production stage. Select the Production stage in the deployment canvas. Under the deployment pipeline canvas, select the checkbox next to the Lakehouse item. Then select the Deploy button to copy the lakehouse in its current state to the Production stage. In the Deploy to next stage window that appears, select Deploy. The green check marks between the stages indicates that all stages in sync and contain the same content. Using deployment pipelines to deploy between stages also updates the content in the workspaces corresponding to the deployment stage. Let\u2019s confirm. In the menu bar on the left, select Workspaces. Select the Test workspace. The lakehouse was copied there. Open the Production workspace from the Workspaces icon on the left menu. The lakehouse was copied to the Production workspace too. Clean up In this exercise, you created a deployment pipeline, and assigned stages to the pipeline. Then you created content in a development workspace and deployed it between pipeline stages using deployment pipelines.</p> <p>In the left navigation bar, select the icon for each workspace to view all of the items it contains. In the menu on the top toolbar, select Workspace settings. In the General section, select Remove this workspace.</p> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/21-implement-cicd.html </p>"},{"location":"labs/21-deployment-pipelines/","title":"Lab: Implement Deployment Pipelines","text":"<p>Implement deployment pipelines in Microsoft Fabric Deployment pipelines in Microsoft Fabric let you automate the process of copying changes made to the content in Fabric items between environments like development, test, and production. You can use deployment pipelines to develop and test content before it reaches end users. In this exercise, you create a deployment pipeline, and assign stages to the pipeline. Then you create some content in a development workspace and use deployment pipelines to deploy it between the Development, Test and Production pipeline stages.</p> <p>Note: To complete this exercise, you need to be an member of the Fabric workspace admin role. To assign roles see Roles in workspaces in Microsoft Fabric.</p> <p>This lab takes approximately 20 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> </ol> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p>"},{"location":"labs/21-deployment-pipelines/#create-workspaces","title":"Create workspaces","text":"<p>Create three workspaces.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a new workspace named: Development</p> </li> <li> <p>Create a new workspace named: Test</p> </li> <li> <p>Create a new workspace named: Production</p> </li> <li> <p>Select the Workspaces icon on the menu bar on the left and confirm that there are three workspaces named: Development, Test, and Production</p> <p>Note</p> <p>If you are prompted to enter a unique name for the workspaces, append one or more random numbers to the words: Development, Test, or Production.</p> </li> </ol>"},{"location":"labs/21-deployment-pipelines/#create-a-deployment-pipeline","title":"Create a deployment pipeline","text":"<p>To continue this lab ... use the MS Learn instructions</p> <p>https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/21-implement-cicd.html</p>"}]}