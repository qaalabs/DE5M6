{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DE5: Module 6 ~ Overview","text":""},{"location":"#day-1-operational-monitoring-performance","title":"Day 1: Operational Monitoring &amp; Performance","text":"<ul> <li>Focus: Learn to monitor when data operations work correctly</li> <li>Activities: Run data pipelines, set up monitoring dashboards, establish performance baselines</li> </ul>"},{"location":"#day-2-operational-incident-response","title":"Day 2: Operational Incident Response","text":"<ul> <li>Focus: Learn to respond when data operations fail</li> <li>Activities: Handle pipeline failures, set up automated alerts, practice incident response</li> </ul>"},{"location":"#day-3-operational-quality-governance","title":"Day 3: Operational Quality &amp; Governance","text":"<ul> <li>Focus: Learn to maintain data standards and governance</li> <li>Activities: Implement data quality patterns, manage data access and security</li> </ul>"},{"location":"#day-4-operational-improvement-value","title":"Day 4: Operational Improvement &amp; Value","text":"<ul> <li>Focus: Learn to evolve and optimize data operations</li> <li>Activities: Deployment pipelines, continuous improvement, demonstrating business value</li> </ul>"},{"location":"#day-1-foundation-context","title":"Day 1: Foundation &amp; Context","text":"<ul> <li>Acquisition: DataOps principles and service management frameworks</li> <li>Investigation: Research real-world DataOps case studies</li> <li>Discussion: Compare approaches across different organisations</li> </ul>"},{"location":"#day-2-hands-on-operations","title":"Day 2: Hands-on Operations","text":"<ul> <li>Practice: Monitoring setup, incident simulation exercises</li> <li>Collaboration: Team-based troubleshooting scenarios</li> <li>Production: Build monitoring dashboards</li> </ul>"},{"location":"#day-3-advanced-operations-improvement","title":"Day 3: Advanced Operations &amp; Improvement","text":"<ul> <li>Practice: Root cause analysis workshops</li> <li>Production: Create deployment pipelines</li> <li>Reflection: Evaluate operational maturity</li> </ul>"},{"location":"#day-4-integration-presentation","title":"Day 4: Integration &amp; Presentation","text":"<ul> <li>Collaboration: Cross-functional team exercises</li> <li>Production: Final presentations of operational solutions</li> <li>Peer Teaching: Learners present different aspects to each other</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/","title":"Day 1: Cloud Monitoring Comparison","text":""},{"location":"day1/cloud-monitoring-comparison/#overview","title":"Overview","text":"<p>This investigation activity allows learners to explore how major cloud providers approach data monitoring, comparing different tools and philosophies. This builds on their hands-on Fabric experience and prepares them for creating their own monitoring strategy.</p>"},{"location":"day1/cloud-monitoring-comparison/#session-structure","title":"Session Structure","text":""},{"location":"day1/cloud-monitoring-comparison/#setup-instructions-3-minutes","title":"Setup Instructions (3 minutes)","text":""},{"location":"day1/cloud-monitoring-comparison/#platform-assignments","title":"Platform Assignments:","text":"<p>Assign each group one primary platform to research:</p> <p>Group 1: AWS Monitoring - CloudWatch (metrics, logs, dashboards) - AWS X-Ray (distributed tracing) - AWS Data Pipeline monitoring</p> <p>Group 2: Google Cloud Monitoring - Cloud Monitoring (formerly Stackdriver) - Cloud Logging - Data pipeline monitoring in Cloud Composer/Dataflow</p> <p>Group 3: Azure Monitoring (beyond Fabric) - Azure Monitor - Application Insights - Data Factory monitoring vs Fabric monitoring</p> <p>Group 4: Fabric Deep Dive (if enough groups) - Advanced Fabric monitoring features - Integration with Azure Monitor - Comparison with other Azure data services</p>"},{"location":"day1/cloud-monitoring-comparison/#research-phase-15-minutes","title":"Research Phase (15 minutes)","text":""},{"location":"day1/cloud-monitoring-comparison/#investigation-framework","title":"Investigation Framework:","text":"<p>Provide each group with these structured research questions:</p>"},{"location":"day1/cloud-monitoring-comparison/#core-monitoring-capabilities","title":"Core Monitoring Capabilities:","text":"<ol> <li>What can you monitor?</li> <li>Data pipeline performance?</li> <li>Data quality metrics?</li> <li>Infrastructure metrics?</li> <li> <p>Cost/resource usage?</p> </li> <li> <p>How do you set up monitoring?</p> </li> <li>Automatic vs manual configuration?</li> <li>Built-in dashboards vs custom creation?</li> <li> <p>Ease of setup for data pipelines?</p> </li> <li> <p>Alerting and Notifications:</p> </li> <li>What triggers can you set?</li> <li>How are alerts delivered? (email, SMS, Slack, etc.)</li> <li>Can you set up automated responses?</li> </ol>"},{"location":"day1/cloud-monitoring-comparison/#data-specific-questions","title":"Data-Specific Questions:","text":"<ol> <li>Data Pipeline Monitoring:</li> <li>Can you track data freshness/latency?</li> <li>Schema change detection?</li> <li>Data volume monitoring?</li> <li> <p>Failed job notifications?</p> </li> <li> <p>Integration and Ecosystem:</p> </li> <li>How well does it integrate with other tools?</li> <li>APIs for custom monitoring?</li> <li> <p>Third-party tool support?</p> </li> <li> <p>Cost and Complexity:</p> </li> <li>What's the pricing model?</li> <li>How complex is it to maintain?</li> <li>Required expertise level?</li> </ol>"},{"location":"day1/cloud-monitoring-comparison/#research-resources","title":"Research Resources:","text":"<p>Point groups to these starting points: - AWS: Search \"CloudWatch data pipeline monitoring\" - GCP: Search \"Cloud Monitoring dataflow pipeline\" - Azure: Search \"Azure Monitor data factory\" - Fabric: Microsoft Learn documentation on monitoring</p>"},{"location":"day1/cloud-monitoring-comparison/#research-strategy-tips","title":"Research Strategy Tips:","text":"<ul> <li>Look for documentation, tutorials, and real-world examples</li> <li>Focus on data/analytics monitoring, not just general infrastructure</li> <li>Take screenshots of interesting dashboards or features</li> <li>Note what seems easy vs complex to implement</li> </ul>"},{"location":"day1/cloud-monitoring-comparison/#report-back-session-7-minutes","title":"Report Back Session (7 minutes)","text":""},{"location":"day1/cloud-monitoring-comparison/#presentation-format","title":"Presentation Format:","text":"<p>Each group has 90 seconds to share their key findings using this structure:</p> <p>30 seconds - Platform Overview: - \"The main monitoring tool for platform X is...\" - \"The best feature for data monitoring is...\"</p> <p>30 seconds - Strengths: - \"What this platform does really well is...\" - \"The standout capability is...\"</p> <p>30 seconds - Challenges/Gaps: - \"The biggest limitation we found is...\" - \"You'd struggle with this platform if...\"</p>"},{"location":"day1/cloud-monitoring-comparison/#key-questions-for-discussion","title":"Key Questions for Discussion:","text":"<p>After all groups present, facilitate quick discussion: - \"Which approach seems most similar to what you've used in your workplace?\" - \"What surprised you about the differences between platforms?\" - \"If you had to choose one for a new project, what would drive your decision?\"</p>"},{"location":"day1/cloud-monitoring-comparison/#expected-outcomes","title":"Expected Outcomes:","text":"<p>By the end of this activity, learners should:</p> <ol> <li>Understand that different platforms have different monitoring philosophies</li> <li>Recognize common patterns across cloud monitoring tools  </li> <li>Have concrete examples of monitoring capabilities beyond what they've seen</li> <li>Be prepared to make informed choices about monitoring approaches</li> </ol>"},{"location":"day1/foundation-purpose/","title":"Day 1: Foundation &amp; Purpose","text":""},{"location":"day1/foundation-purpose/#overview","title":"Overview","text":"<p>This session establishes the foundational understanding of why monitoring matters in data operations by connecting personal experiences with business impact and examining real-world communication during system failures.</p>"},{"location":"day1/foundation-purpose/#session-structure","title":"Session Structure","text":""},{"location":"day1/foundation-purpose/#1-discussion-system-failure-stories-15-minutes","title":"1. Discussion: System Failure Stories (15 minutes)","text":"<p>Learning Type: Discussion Format: Group discussion with individual sharing</p>"},{"location":"day1/foundation-purpose/#facilitator-instructions","title":"Facilitator Instructions:","text":"<ol> <li>Opening Question (2 minutes):</li> <li> <p>\"Think of a time when a system failure impacted you personally - could be at work, online shopping, banking, social media. What happened?\"</p> </li> <li> <p>Individual Reflection (3 minutes):</p> </li> <li>Give learners time to think of their example</li> <li> <p>Encourage them to consider: What broke? How did they find out? What was the impact?</p> </li> <li> <p>Sharing Round (8 minutes):</p> </li> <li>Each learner shares their story (1-2 minutes each)</li> <li> <p>As facilitator, listen for common themes:</p> <ul> <li>How they discovered the problem</li> <li>Lack of communication about the issue</li> <li>Business/personal impact</li> <li>Resolution time</li> </ul> </li> <li> <p>Pattern Recognition (2 minutes):</p> </li> <li>Summarise common themes you heard</li> <li>\"What patterns do you notice in these stories?\"</li> </ol>"},{"location":"day1/foundation-purpose/#key-questions-to-guide-discussion","title":"Key Questions to Guide Discussion:","text":"<ul> <li>How did you first realise something was wrong?</li> <li>Did anyone tell you what was happening?</li> <li>How long did it take to resolve?</li> <li>What would have made the experience better?</li> </ul>"},{"location":"day1/foundation-purpose/#expected-outcomes","title":"Expected Outcomes:","text":"<ul> <li>Learners connect personally with system reliability challenges</li> <li>Common themes emerge: detection, communication, resolution</li> <li>Natural lead-in to why monitoring matters</li> </ul>"},{"location":"day1/foundation-purpose/#2-acquisition-why-monitor-10-minutes","title":"2. Acquisition: Why Monitor? (10 minutes)","text":"<p>Learning Type: Acquisition Format: Trainer presentation with interactive elements</p>"},{"location":"day1/foundation-purpose/#key-points-to-cover","title":"Key Points to Cover:","text":"<p>From Personal Pain to Business Impact (3 minutes):</p> <ul> <li>Personal frustration = lost customers/revenue for businesses</li> <li>System downtime costs: Amazon loses \u00a32.8M per minute of downtime</li> <li>Reputation damage can last longer than the outage</li> </ul> <p>The Monitoring Pyramid (4 minutes): <pre><code>Business Impact     (Customer complaints, lost revenue)\n        \u2191\nService Impact      (Application slow/down, data unavailable)\n        \u2191\nTechnical Issues    (CPU high, disk full, network timeout)\n        \u2191\nMonitoring Data     (Metrics, logs, alerts)\n</code></pre></p> <p>Key Monitoring Questions (3 minutes):</p> <ol> <li>Is it working? (Availability monitoring)</li> <li>Is it working well? (Performance monitoring)  </li> <li>Will it keep working? (Predictive monitoring)</li> <li>When it breaks, how do we know? (Alerting)</li> <li>When it breaks, what do we tell people? (Communication)</li> </ol>"},{"location":"day1/foundation-purpose/#interactive-elements","title":"Interactive Elements:","text":"<ul> <li>Ask: \"Which level of the pyramid do you think most organisations focus on?\"</li> <li>Quick poll: \"Hands up if your workplace monitors at each level\"</li> </ul>"},{"location":"day1/foundation-purpose/#3-investigation-status-page-analysis-15-minutes","title":"3. Investigation: Status Page Analysis (15 minutes)","text":"<p>Learning Type: Investigation Format: Pairs research with report back</p>"},{"location":"day1/foundation-purpose/#setup-instructions-2-minutes","title":"Setup Instructions (2 minutes):","text":"<ol> <li>Form pairs - mix experience levels if possible</li> <li>Assign each pair 1-2 status pages from the list below</li> <li>Research task: 10 minutes investigation + 3 minutes prep for sharing</li> </ol>"},{"location":"day1/foundation-purpose/#status-pages-to-research","title":"Status Pages to Research:","text":"<p>Choose from these well-known status pages:</p> <ul> <li>AWS: https://health.aws.amazon.com/health/status</li> <li>Microsoft Azure: https://status.azure.com/</li> <li>Google Cloud: https://status.cloud.google.com/</li> <li>GitHub: https://www.githubstatus.com/</li> <li>Slack: https://status.slack.com/</li> <li>Stripe: https://status.stripe.com/</li> <li>Zoom: https://status.zoom.us/</li> </ul>"},{"location":"day1/foundation-purpose/#investigation-questions-for-pairs","title":"Investigation Questions for Pairs:","text":"<p>Provide each pair with these guiding questions:</p> <p>Current Status:</p> <ul> <li>What's the overall system health right now?</li> <li>How is information organised? (by service, region, etc.)</li> </ul> <p>Historical Incidents:</p> <ul> <li>Look at 1-2 recent incidents - what information is provided?</li> <li>How detailed are the updates?</li> <li>How frequently were updates posted?</li> </ul> <p>Communication Style:</p> <ul> <li>How technical is the language?</li> <li>What do they tell users about impact?</li> <li>Do they explain what they're doing to fix it?</li> </ul> <p>What Works Well:</p> <ul> <li>What would be helpful if you were a customer?</li> <li>What builds confidence that they're handling it?</li> </ul>"},{"location":"day1/foundation-purpose/#report-back-session-3-minutes","title":"Report Back Session (3 minutes):","text":"<p>Each pair shares one key insight in 30 seconds:</p> <ul> <li>\"The most interesting thing we noticed was...\"</li> <li>\"If we were designing a status page, we'd make sure to...\"</li> </ul>"},{"location":"day1/foundation-purpose/#facilitator-notes","title":"Facilitator Notes:","text":"<ul> <li>If internet issues: Have screenshots of status pages prepared as backup</li> <li>Time management: Keep sharing tight - use a timer</li> <li>Follow-up questions: \"How does this connect to monitoring?\" \"What would happen without these status pages?\"</li> </ul>"},{"location":"day1/foundation-purpose/#transition-to-next-session","title":"Transition to Next Session","text":"<p>Bridge to Lab 5 (2 minutes):</p> <ul> <li>\"We've talked about why monitoring matters and how companies communicate when things go wrong\"</li> <li>\"Now let's start building something we can monitor\"</li> <li>\"Our first lab will introduce you to Microsoft Fabric and create a simple data process\"</li> </ul>"},{"location":"day1/foundation-purpose/#key-takeaways","title":"Key Takeaways:","text":"<p>By the end of this session, learners should understand:</p> <ol> <li>System failures have real business and personal impact</li> <li>Monitoring helps detect, communicate, and resolve issues faster</li> <li>Good communication during incidents builds trust</li> <li>We're going to learn operational skills to handle these challenges</li> </ol>"},{"location":"day1/gallery-walk/","title":"Sharing Strategy ~ Gallery Walk","text":""},{"location":"day1/gallery-walk/#strategy-sharing-feedback-7-minutes","title":"Strategy Sharing &amp; Feedback (7 minutes)","text":""},{"location":"day1/gallery-walk/#rapid-strategy-showcase-5-minutes","title":"Rapid Strategy Showcase (5 minutes):","text":"<ul> <li>Each pair screen shares for 60 seconds</li> <li>Focus on \"one key decision we made and why\"</li> <li>Other pairs post ONE insight in chat</li> </ul>"},{"location":"day1/gallery-walk/#group-reflection-2-minutes","title":"Group Reflection (2 minutes):","text":"<ul> <li>Facilitator reads chat highlights</li> <li>\"What common themes do you notice?\"</li> <li>\"What surprised you about different approaches?\"</li> </ul>"},{"location":"day1/introduction/","title":"Day 1 - Monitoring &amp; Performance","text":""},{"location":"day1/introduction/#agenda","title":"Agenda","text":""},{"location":"day1/introduction/#explore-end-to-end-analytics-with-microsoft-fabric","title":"Explore end-to-end analytics with Microsoft Fabric","text":"<ul> <li>Microsoft Fabric</li> <li>Data teams and Fabric</li> <li>Enable and use Microsoft Fabric</li> </ul>"},{"location":"day1/introduction/#get-started-with-lakehouses-in-microsoft-fabric","title":"Get started with lakehouses in Microsoft Fabric","text":"<ul> <li>What is a lakehouse</li> <li>Work with a Fabric lakehouse</li> <li>Load data into a lakehouse</li> <li>Explore, transform, and visualise data in the lakehouse</li> </ul>"},{"location":"day1/introduction/#exercise-create-and-ingest-data","title":"Exercise: Create and ingest data","text":"<p>labs/01-lakehouse.md</p>"},{"location":"day1/introduction/#use-apache-spark-in-microsoft-fabric","title":"Use Apache Spark in Microsoft Fabric","text":"<ul> <li>What is Apache Spark?</li> <li>How to use Apache Spark</li> <li>Prepare to use Apache Spark</li> <li>Configure the Spark environment</li> <li>Run Spark in Fabric</li> <li>Ingest data with Spark</li> <li>Load data in a Spark Dataframe</li> <li>Transform data in a Spark Dataframe</li> </ul>"},{"location":"day1/introduction/#exercise-apache-spark-in-fabric","title":"Exercise: Apache Spark in Fabric","text":"<p>labs/02-analyze-spark.html</p> <ul> <li>Create a workspace</li> <li>Create a lakehouse and upload files</li> <li>Create a notebook</li> <li>Create a DataFrame</li> <li>Explore data in a DataFrame</li> <li>Use Spark to transform data files</li> </ul>"},{"location":"day1/introduction/#ingest-data-with-dataflow-gen2","title":"Ingest data with Dataflow Gen2","text":"<ul> <li>Understand Dataflow Gen2</li> <li>Dataflow Gen2 benefits and limitations</li> <li>Explore Dataflow Gen2</li> </ul>"},{"location":"day1/monitoring-strategy-creation/","title":"Day 1: Monitoring Strategy Creation","text":""},{"location":"day1/monitoring-strategy-creation/#overview","title":"Overview","text":"<p>This production activity synthesizes everything learners have experienced today - the labs, status page analysis, and cloud platform research - into a comprehensive monitoring strategy. Working in pairs, they create a tangible monitoring framework that could be applied in real-world scenarios.</p>"},{"location":"day1/monitoring-strategy-creation/#session-structure","title":"Session Structure","text":""},{"location":"day1/monitoring-strategy-creation/#setup-instructions-3-minutes","title":"Setup Instructions (3 minutes)","text":""},{"location":"day1/monitoring-strategy-creation/#scenario-assignment","title":"Scenario Assignment:","text":"<p>Give each pair a realistic scenario to make their strategy concrete:</p> <p>Scenario A: E-commerce Data Platform</p> <ul> <li>Real-time customer behaviour tracking</li> <li>Daily sales reporting pipelines  </li> <li>Inventory management data feeds</li> <li>Customer impact: Revenue loss if data is stale/incorrect</li> </ul> <p>Scenario B: Healthcare Analytics</p> <ul> <li>Patient data processing pipelines</li> <li>Clinical reporting systems</li> <li>Research data aggregation</li> <li>Customer impact: Patient safety and regulatory compliance</li> </ul> <p>Scenario C: Financial Services</p> <ul> <li>Transaction processing monitoring</li> <li>Risk calculation pipelines</li> <li>Regulatory reporting systems</li> <li>Customer impact: Financial losses and regulatory penalties</li> </ul> <p>Scenario D: Manufacturing IoT</p> <ul> <li>Sensor data ingestion from factory floor</li> <li>Predictive maintenance pipelines</li> <li>Quality control data processing</li> <li>Customer impact: Production delays and safety risks</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#strategy-development-phase-20-minutes","title":"Strategy Development Phase (20 minutes)","text":""},{"location":"day1/monitoring-strategy-creation/#framework-template","title":"Framework Template:","text":"<p>Provide pairs with this structured template to complete:</p>"},{"location":"day1/monitoring-strategy-creation/#1-what-we-monitor-5-minutes","title":"1. What We Monitor (5 minutes)","text":"<p>Define the key monitoring areas:</p> <p>Data Pipeline Health:</p> <ul> <li> Pipeline execution status (success/failure)</li> <li> Processing time/latency</li> <li> Data volume trends</li> <li> Error rates and types</li> </ul> <p>Data Quality:</p> <ul> <li> Completeness (missing records)</li> <li> Accuracy (data validation failures)  </li> <li> Freshness (data age/staleness)</li> <li> Schema changes/drift</li> </ul> <p>Infrastructure &amp; Performance:</p> <ul> <li> Resource utilization (CPU, memory, storage)</li> <li> Cost monitoring</li> <li> Scalability metrics</li> <li> Security events</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#2-how-we-monitor-5-minutes","title":"2. How We Monitor (5 minutes)","text":"<p>Choose tools and methods:</p> <p>Monitoring Tools:</p> <ul> <li>Primary platform: ________________</li> <li>Dashboard tools: ________________</li> <li>Alerting method: ________________</li> <li>Log aggregation: ________________</li> </ul> <p>Monitoring Approach:</p> <ul> <li> Real-time monitoring</li> <li> Batch/scheduled checks</li> <li> Trend analysis</li> <li> Predictive monitoring</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#3-when-we-alert-5-minutes","title":"3. When We Alert (5 minutes)","text":"<p>Define alert thresholds and escalation:</p> <p>Critical Alerts (immediate response required):</p> <ul> <li>Trigger: _________________________</li> <li>Notify: __________________________</li> <li>Response time: ___________________</li> </ul> <p>Warning Alerts (monitor closely):</p> <ul> <li>Trigger: _________________________</li> <li>Notify: __________________________</li> <li>Response time: ___________________</li> </ul> <p>Information Alerts (awareness only):</p> <ul> <li>Trigger: _________________________</li> <li>Notify: __________________________</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#4-who-responds-3-minutes","title":"4. Who Responds (3 minutes)","text":"<p>Define roles and responsibilities:</p> <p>First Response Team:</p> <ul> <li>Role: ____________________________</li> <li>Responsibilities: _________________</li> </ul> <p>Escalation Path:</p> <ul> <li>Level 1: _________________________</li> <li>Level 2: _________________________</li> <li>Level 3: _________________________</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#5-communication-plan-2-minutes","title":"5. Communication Plan (2 minutes)","text":"<p>How you communicate during incidents:</p> <p>Internal Communication:</p> <ul> <li>Method: __________________________</li> <li>Frequency: _______________________</li> </ul> <p>External Communication:</p> <ul> <li>Stakeholders: ____________________</li> <li>Method: __________________________</li> <li>Message template: ________________</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#strategy-sharing-feedback-7-minutes","title":"Strategy Sharing &amp; Feedback (7 minutes)","text":""},{"location":"day1/monitoring-strategy-creation/#rapid-strategy-showcase-5-minutes","title":"Rapid Strategy Showcase (5 minutes):","text":"<ul> <li>Each pair screen shares for 60 seconds</li> <li>Focus on \"one key decision we made and why\"</li> <li>Other pairs post ONE insight in chat</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#group-reflection-2-minutes","title":"Group Reflection (2 minutes):","text":"<ul> <li>Facilitator reads chat highlights</li> <li>\"What common themes do you notice?\"</li> <li>\"What surprised you about different approaches?\"---</li> </ul>"},{"location":"day1/monitoring-strategy-creation/#expected-outputs","title":"Expected Outputs:","text":"<p>Each pair should produce:</p> <ol> <li>Completed monitoring strategy template</li> <li>Understanding of monitoring complexity</li> <li>Appreciation for different monitoring approaches</li> <li>Foundation for tomorrow's incident response activities</li> </ol>"},{"location":"day1/share-monitoring-strategies/","title":"Day 1: Wrap-up","text":""},{"location":"day1/share-monitoring-strategies/#strategy-sharing-feedback-7-minutes","title":"Strategy Sharing &amp; Feedback (7 minutes)","text":""},{"location":"day1/share-monitoring-strategies/#rapid-strategy-showcase-5-minutes","title":"Rapid Strategy Showcase (5 minutes):","text":"<ul> <li>Each pair screen shares for 60 seconds</li> <li>Focus on \"one key decision we made and why\"</li> <li>Other pairs post ONE insight in chat</li> </ul>"},{"location":"day1/share-monitoring-strategies/#group-reflection-2-minutes","title":"Group Reflection (2 minutes):","text":"<ul> <li>Facilitator reads chat highlights</li> <li>\"What common themes do you notice?\"</li> <li>\"What surprised you about different approaches?\"</li> </ul>"},{"location":"day2/breaking-things/","title":"Day 2: Breaking Things","text":""},{"location":"day2/breaking-things/#overview","title":"Overview","text":"<p>This hands-on session introduces failures into working data pipelines. You will practice controlled troubleshooting while observing how MS Fabric responds to different types of failures.</p>"},{"location":"day2/breaking-things/#part-1-establish-working-baseline","title":"Part 1: Establish Working Baseline","text":""},{"location":"day2/breaking-things/#setup-instructions","title":"Setup Instructions","text":"<p>Technical Setup:</p> <ul> <li>Start the QA Platform Lab enviroment</li> <li>Redo Lab 1 - Ingest Data from yesterday</li> <li>Verify that the data flows through successfully</li> </ul>"},{"location":"day2/breaking-things/#part-2-systematic-breaking-learning","title":"Part 2: Systematic Breaking &amp; Learning","text":""},{"location":"day2/breaking-things/#break-cycle-process","title":"Break Cycle Process","text":"<p>For each break type, follow this 4-step cycle:</p> <ol> <li>Break   - introduce the failure</li> <li>Observe - run pipeline, note error messages</li> <li>Discuss - what does the error tell us?</li> <li>Fix     - restore to working state</li> </ol>"},{"location":"day2/breaking-things/#break-1-file-not-found","title":"Break 1: File Not Found","text":"<p>Break Instructions:</p> <ul> <li>Navigate to your source data file</li> <li>Rename it - add '_broken' to the filename</li> <li>Now try to run your pipeline</li> </ul> <p>Observation Points:</p> <ul> <li>How quickly can you see the error?</li> <li>What does Fabric's error message say?</li> <li>Does the error message help you identify the problem?</li> <li>Is it clear what needs to be fixed?</li> </ul> <p>Discussion:</p> <p>Fix &amp; Verify:</p> <ul> <li>Rename the file back to original name</li> <li>Run pipeline again to confirm it works</li> </ul>"},{"location":"day2/breaking-things/#break-2-schema-mismatch","title":"Break 2: Schema Mismatch","text":"<p>Break Instructions:</p> <ul> <li>Open your source CSV file</li> <li>Delete one of the column headers (not the data, just the header)</li> <li>Save the file and run your pipeline</li> </ul> <p>Observation Points:</p> <ul> <li>Does the error message clearly indicate schema issues?</li> <li>How long does it take to identify the problem?</li> <li>Is the difference clear between this and the file error?</li> </ul> <p>Discussion:</p> <p>Fix &amp; Verify:</p> <ul> <li>Add the column header back</li> <li>Verify pipeline works again</li> </ul>"},{"location":"day2/breaking-things/#break-3-data-quality-issues","title":"Break 3: Data Quality Issues","text":"<p>Break Instructions:</p> <ul> <li>In your CSV, find a numeric column</li> <li>Change some numbers to text (like 'ERROR' or 'N/A')</li> <li>Save and run pipeline</li> </ul> <p>Observation Points: - Does Fabric handle data type mismatches gracefully? - What happens to the bad data - does it get skipped or cause total failure?</p> <p>Discussion:</p> <p>Fix &amp; Verify: - Fix the data values back to numbers - Confirm everything works</p>"},{"location":"day2/complex-breaking/","title":"Day 2: Advanced Breaking Scenarios","text":""},{"location":"day2/complex-breaking/#overview","title":"Overview","text":"<p>This session introduces more complex failure scenarios that mirror real-world production issues.</p>"},{"location":"day2/complex-breaking/#scenario-1-performance-issues","title":"Scenario 1: Performance Issues","text":""},{"location":"day2/complex-breaking/#setup","title":"Setup","text":"<p>Context: \"Large files and complex processing can cause timeouts and resource issues\"</p> <p>Breaking Method:</p> <ul> <li>Find or create a much larger dataset than used in the previos lab</li> <li>Options:</li> <li>Duplicate your existing file multiple times to create a large CSV</li> <li>Use online sample datasets (sales data, sensor data, etc.)</li> <li>Create synthetic data with repeated rows</li> </ul> <p>Target: File should be significantly larger (aim for 10x+ size)</p>"},{"location":"day2/complex-breaking/#execution-observation","title":"Execution &amp; Observation","text":"<p>Break Process:</p> <ol> <li>Replace your pipeline's source with the large file</li> <li>Run the pipeline and monitor performance</li> <li>Observe behavior: Does it slow down? Timeout? Complete successfully?</li> <li>Check resource usage if visible in Fabric interface</li> </ol> <p>Key Observation Points:</p> <ul> <li>How long does processing take compared to small files?</li> <li>Does Fabric show any warnings or performance indicators?</li> <li>Are there any timeout errors or resource constraint messages?</li> <li>How does the monitoring from Lab 11 respond to slower processing?</li> </ul>"},{"location":"day2/complex-breaking/#documentation","title":"Documentation","text":"<p>Quick Notes: Write down what you observed for later discussion</p>"},{"location":"day2/complex-breaking/#scenario-2-data-corruption-scenarios","title":"Scenario 2: Data Corruption Scenarios","text":""},{"location":"day2/complex-breaking/#setup_1","title":"Setup","text":"<p>Context: \"Real data often contains mixed quality - some good records, some problematic ones\"</p> <p>Breaking Method Options (choose 1-2 to try):</p> <p>Option A: Mixed Data Types</p> <ul> <li>In a numeric column, replace some values with text (\"ERROR\", \"NULL\", \"N/A\")</li> <li>Leave other values as valid numbers</li> <li>See if Fabric processes partially or fails completely</li> </ul> <p>Option B: Encoding Issues</p> <ul> <li>Add special characters or emojis to text fields</li> <li>Try different character encodings if possible</li> <li>See how Fabric handles non-standard characters</li> </ul> <p>Option C: Incomplete Records</p> <ul> <li>Delete some values in the middle of rows (creating gaps)</li> <li>Remove entire columns from some rows</li> <li>Create inconsistent row lengths</li> </ul>"},{"location":"day2/complex-breaking/#execution-observation_1","title":"Execution &amp; Observation","text":"<p>Break Process:</p> <ol> <li>Choose one corruption type and modify your data file</li> <li>Run the pipeline and carefully observe results</li> <li>Check output data - did bad records get processed, skipped, or cause failure?</li> <li>Try a second corruption type if time allows</li> </ol> <p>Key Observation Points: - Does the pipeline fail completely or continue with warnings? - How does Fabric handle the corrupted data? - What error messages or warnings appear? - Can you identify which specific records caused problems? - How would a business user know there was an issue?</p>"},{"location":"day2/complex-breaking/#documentation_1","title":"Documentation","text":"<p>Record findings: What happened with each corruption type?</p>"},{"location":"day2/complex-breaking/#scenario-3-resource-constraints-timing-issues","title":"Scenario 3: Resource Constraints &amp; Timing Issues","text":""},{"location":"day2/complex-breaking/#setup_2","title":"Setup","text":"<p>Context: \"Production systems often have multiple processes competing for resources\"</p> <p>Breaking Method Options:</p> <p>Option A: Multiple Simultaneous Pipelines</p> <ul> <li>Create duplicate copies of your pipeline</li> <li>Run multiple instances at the same time</li> <li>See how Fabric handles concurrent resource usage</li> </ul> <p>Option B: Complex Transformations</p> <ul> <li>Add complex calculations or data transformations to your pipeline</li> <li>Create joins between multiple large datasets if possible</li> <li>See if processing becomes resource-intensive</li> </ul> <p>Option C: Rapid Successive Runs</p> <ul> <li>Run the same pipeline repeatedly in quick succession</li> <li>Don't wait for completion before starting the next run</li> <li>Observe queuing, conflicts, or resource competition</li> </ul>"},{"location":"day2/complex-breaking/#execution-observation_2","title":"Execution &amp; Observation","text":"<p>Break Process:</p> <ol> <li>Choose one resource constraint scenario</li> <li>Execute your chosen approach</li> <li>Monitor system behavior and any queue/resource indicators</li> <li>Try to overwhelm the system (within reasonable limits)</li> </ol> <p>Key Observation Points:</p> <ul> <li>How does Fabric manage multiple concurrent operations?</li> <li>Are there visible queue indicators or resource monitors?</li> <li>Do operations slow down, queue up, or fail?</li> <li>How long before the system recovers to normal performance?</li> </ul>"},{"location":"day2/complex-breaking/#documentation_2","title":"Documentation","text":"<p>Note observations: System behavior under load</p>"},{"location":"day2/fire-drill-briefing/","title":"Day 2: Fire Drill Briefing","text":"<p>Trainer Instructions</p>"},{"location":"day2/fire-drill-briefing/#briefing-script","title":"Briefing Script","text":""},{"location":"day2/fire-drill-briefing/#opening","title":"Opening","text":"<p>Dramatic tone: \"Right, preparation time is over. We have a LIVE INCIDENT situation developing with our data platform. You are now the incident response teams for our organization.\"</p> <p>Set the scene: \"It's a typical workday, you're focused on your regular tasks, when suddenly...\"</p>"},{"location":"day2/fire-drill-briefing/#rules-logistics","title":"Rules &amp; Logistics","text":"<p>Team Coordination: - Your team has a war room - your breakout room for internal discussion - You can plan, debate, and coordinate freely in your team room - BUT - all official communication must go through Slack</p> <p>Slack Communication Rules: - Use your Slack channel for incident updates to stakeholders - This is how you communicate with customers, executives, and other teams - Write professionally - these messages will be reviewed during debrief - First action should be incident announcement to the channel</p> <p>Timing: - You have 25 minutes to handle this incident - I'll be monitoring your Slack communications - You may recive additional updates or pressure via Slack as the situation develops</p>"},{"location":"day2/fire-drill-briefing/#scenario-distribution-1-minute","title":"Scenario Distribution (1 minute)","text":"<p>Hand out scenario cards:</p> <ul> <li>Read your scenario and understand the situation</li> <li>Then immediately post your first incident update to Slack</li> <li>Remember: stakeholders are waiting to hear from you</li> </ul> <p>Guidelines:</p> <ul> <li>Don't watch the other teams </li> <li>Focus on what your team needs to do!</li> </ul>"},{"location":"day2/fire-drill-briefing/#success-criteria","title":"Success Criteria","text":"<p>What we're looking for:</p> <ul> <li>Clear, professional communication via Slack</li> <li>Systematic approach to incident response</li> <li>Good coordination within your team</li> <li>Balancing technical fixes with stakeholder communication</li> </ul> <p>Important:</p> <ul> <li>There's no single 'right' answer - we're practicing process and communication</li> <li>Real incidents are messy and uncertain - embrace that</li> <li>Use what you learned from the major incident examples</li> </ul>"},{"location":"day2/fire-drill-briefing/#facilitator-actions-during-briefing","title":"Facilitator Actions During Briefing","text":""},{"location":"day2/fire-drill-briefing/#before-starting","title":"Before Starting:","text":"<ul> <li>Ensure breakout rooms are ready for each team</li> <li>Confirm Slack channels are set up and all learners can access</li> <li>Have scenario cards ready to distribute</li> <li>Set 25-minute timer visible to all</li> </ul>"},{"location":"day2/fire-drill-briefing/#during-briefing","title":"During Briefing:","text":"<ul> <li>Maintain dramatic energy - this is a real incident simulation</li> <li>Check for questions but keep momentum high</li> <li>Emphasize Slack communication - this is key learning objective</li> <li>Watch for any technical issues with breakout rooms or Slack</li> </ul>"},{"location":"day2/fire-drill-briefing/#after-briefing","title":"After Briefing:","text":"<ul> <li>Send teams to breakout rooms immediately after scenario distribution</li> <li>Monitor Slack channels for their first posts</li> <li>Prepare to send escalation messages during the 25-minute period</li> </ul>"},{"location":"day2/fire-drill-briefing/#optional-pressure-elements","title":"Optional Pressure Elements","text":""},{"location":"day2/fire-drill-briefing/#during-the-25-minute-drill-you-can-add-realism-by-sending-these-messages","title":"During the 25-minute drill, you can add realism by sending these messages:","text":"<p>5 minutes in:</p> <p>\"Customer calls are increasing - service desk getting overwhelmed\"</p> <p>10 minutes in:</p> <p>\"Social media mentions of our service issues are trending\"</p> <p>15 minutes in:</p> <p>\"CEO's office is asking for status update\"</p> <p>20 minutes in:</p> <p>\"Major client just called - they're considering switching providers\"</p> <p>Use sparingly - only if teams seem to be handling the basic scenario well.</p>"},{"location":"day2/fire-drill-briefing/#what-to-watch-for","title":"What to Watch For","text":""},{"location":"day2/fire-drill-briefing/#during-the-drill","title":"During the Drill:","text":"<p>Slack Communication Quality:</p> <ul> <li>Are first posts professional and informative?</li> <li>Do they provide regular updates?</li> <li>Are messages clear for non-technical audiences?</li> </ul> <p>Team Coordination:</p> <ul> <li>Are they using breakout rooms effectively?</li> <li>Is there clear role separation?</li> <li>Are decisions being made efficiently?</li> </ul> <p>Incident Response Process:</p> <ul> <li>Do they follow systematic approach?</li> <li>Are they balancing technical fixes with communication?</li> <li>How do they handle uncertainty?</li> </ul>"},{"location":"day2/fire-drill-briefing/#common-issues-to-note","title":"Common Issues to Note:","text":"<p>Communication Problems:</p> <ul> <li>Too technical for general audience</li> <li>Infrequent updates or radio silence</li> <li>Unclear or confusing messaging</li> </ul> <p>Coordination Issues:</p> <ul> <li>All talking at once, no clear leadership</li> <li>Forgetting to update Slack while focused on technical discussion</li> <li>Paralysis when facing uncertain information</li> </ul> <p>These become great debrief discussion points!</p>"},{"location":"day2/fire-drill-briefing/#wrap-up-at-25-minutes","title":"Wrap-up at 25 Minutes","text":""},{"location":"day2/fire-drill-briefing/#end-the-drill","title":"End the Drill:","text":"<p>Announcement: \"Time! Step back from your incident response. Well done everyone.\"</p> <p>Immediate Actions:</p> <ul> <li>Bring all teams back to main room</li> <li>Keep Slack channels open for review</li> <li>Prepare for 10-minute quick debrief</li> </ul>"},{"location":"day2/fire-drill-briefing/#transition","title":"Transition:","text":"<p>\"Before we break, let's do a quick review of your Slack communication and initial responses. Then we'll take a break before the next escalation phase.\"</p>"},{"location":"day2/fire-drill-briefing/#success-indicators","title":"Success Indicators","text":"<p>Good Simulation if:</p> <ul> <li>Teams immediately start coordinating in breakout rooms</li> <li>Slack shows professional incident updates within first 5 minutes</li> <li>Teams balance technical discussion with communication needs</li> <li>Energy level is high and focused</li> </ul> <p>Adjust if:</p> <ul> <li>Teams seem confused about process (provide clarification)</li> <li>Slack communication is sparse (remind about stakeholder updates)</li> <li>Teams are too stressed (reduce pressure elements)</li> <li>Teams finish too quickly (add complexity via Slack updates)</li> </ul>"},{"location":"day2/fire-drill-prep/","title":"Day 2: Fire Drill Preparation","text":""},{"location":"day2/fire-drill-prep/#whats-coming-next","title":"What's Coming Next","text":"<p>In 20 minutes, you'll participate in incident response fire drills simulating real data platform failures. You'll work in teams to coordinate technical responses and stakeholder communication under time pressure.</p>"},{"location":"day2/fire-drill-prep/#your-preparation-tasks","title":"Your Preparation Tasks","text":""},{"location":"day2/fire-drill-prep/#1-form-your-incident-response-team","title":"1. Form Your Incident Response Team","text":"<p>Team Size: 3-4 people depending on class size</p> <p>Assign Roles:</p> <ul> <li>Incident Commander: Coordinates overall response, makes key decisions</li> <li>Technical Lead: Investigates root cause, determines fix options  </li> <li>Communications Lead: Handles stakeholder updates and messaging</li> <li>Business Impact Analyst: Assesses downstream effects (if 4 people)</li> </ul>"},{"location":"day2/fire-drill-prep/#2-real-incident-examples","title":"2. Real Incident Examples","text":"<p>Scan these major incidents to understand what works (and what doesn't) in incident response:</p> <p>Cloudflare Global DNS Outage - June 21, 2022</p> <ul> <li>27-minute global DNS failure affecting millions of websites</li> <li>Link: https://blog.cloudflare.com/cloudflare-outage-on-june-21-2022/</li> <li>Key lesson: Speed vs. accuracy in technical communication</li> </ul> <p>AWS us-east-1 Outage - December 7, 2021 </p> <ul> <li>5+ hour power-related outage affecting thousands of services</li> <li>Search: \"AWS us-east-1 outage December 2021 post-mortem\"</li> <li>Key lesson: Cascading failures and dependency management</li> </ul> <p>GitHub Database Incident - October 21, 2018</p> <ul> <li>24+ hour database cluster failure during maintenance</li> <li>Search: \"GitHub October 2018 incident post-mortem database\"</li> <li>Key lesson: Incident escalation and technical decision-making</li> </ul> <p>Meta/Facebook Global Outage - October 4, 2021</p> <ul> <li>6+ hour BGP configuration error taking down all Meta services</li> <li>Search: \"Facebook outage October 2021 BGP routing\"</li> <li>Key lesson: When monitoring systems fail too</li> </ul>"},{"location":"day2/fire-drill-prep/#3-extract-key-principles","title":"3. Extract Key Principles","text":"<p>Focus your research on these questions</p> <p>Detection &amp; Assessment:</p> <ul> <li>How quickly did they recognize the problem?</li> <li>What information did they gather before acting?</li> <li>How did they assess business impact?</li> </ul> <p>Technical Response:</p> <ul> <li>Did they implement quick fixes or wait for proper solutions?</li> <li>How did they coordinate multiple teams?</li> <li>What tools and processes helped/hindered them?</li> </ul> <p>Communication Strategy:</p> <ul> <li>When did they first communicate publicly?</li> <li>How detailed were their updates?</li> <li>How did they handle uncertainty in their messaging?</li> <li>What different messages did they send to different audiences?</li> </ul> <p>Coordination &amp; Decision-Making:</p> <ul> <li>Who made key decisions?</li> <li>How did they balance speed vs. accuracy?</li> <li>What escalation triggers did they use?</li> </ul>"},{"location":"day2/fire-drill-prep/#research-strategy-tips","title":"Research Strategy Tips","text":"<p>Don't try to read everything! Focus on:</p> <ul> <li>Executive summaries and key timeline points</li> <li>Communication examples - actual status updates they published</li> <li>Lessons learned sections in post-mortems</li> <li>Decision points - when they chose one approach over another</li> </ul> <p>Look for patterns:</p> <ul> <li>What's common across different incidents?</li> <li>What approaches consistently work or fail?</li> <li>How do companies handle uncertainty in their communication?</li> </ul>"},{"location":"day2/fire-drill-prep/#team-coordination","title":"Team Coordination","text":"<ul> <li>Share your findings with your team members</li> <li>Agree on communication approach - who will handle what during the drill</li> <li>Establish team coordination method - how will you make quick decisions together?</li> </ul> <p>The goal isn't deep expertise - it's rapid preparation for practical application!</p>"},{"location":"day2/fire-drill-scenario1/","title":"Fire Drill Scenario 1: Customer Dashboard Failure","text":""},{"location":"day2/fire-drill-scenario1/#scenario","title":"Scenario","text":"<p>View the incident on the Company status page:</p> <ul> <li>INCIDENT: Customer-facing dashboard showing stale data</li> </ul>"},{"location":"day2/fire-drill-scenario1/#your-mission","title":"Your Mission","text":"<ul> <li>You are the incident response team.</li> <li>Coordinate your response, investigate the root cause, monitor fixes, and manage stakeholder communication.</li> </ul> <p>NOTE: All official communications must go through the official channel.</p>"},{"location":"day2/fire-drill-scenario2/","title":"Fire Drill Scenario 2: Incident Escalation Update","text":""},{"location":"day2/fire-drill-scenario2/#scenario","title":"Scenario","text":"<p>View the incident on the Company status page:</p> <ul> <li>INCIDENT: Customer Dashboard ~ Incident Escalation Update</li> </ul>"},{"location":"day2/fire-drill-scenario2/#your-mission","title":"Your Mission","text":"<ul> <li>You are the incident response team.</li> <li>Coordinate your response, investigate the root cause, monitor fixes, and manage stakeholder communication.</li> </ul> <p>NOTE: All official communications must go through the official channel.</p>"},{"location":"day2/scenario1-debrief/","title":"Scenario1 debrief","text":""},{"location":"day2/scenario1-debrief/#scenario-1-debrief-elements-you-can-use","title":"Scenario 1 Debrief Elements You Can Use:","text":"<p>From the Scenario Card: - Customer Dashboard Failure - stale data since 6 AM - Multiple stakeholder pressure - Sales Director, Customer Success, IT Director - Time pressure - Board meeting at 10 AM (45 minutes) - Technical context - Fabric pipeline failed, vendor data source changes</p> <p>Observable Team Behaviors: - First response approach - technical investigation vs. immediate communication - Stakeholder prioritization - who did they contact first? - Communication timing - how quickly did they update your status site? - Status update quality - professional vs. technical language</p> <p>Optional Pressure Elements You Added: - Customer service call volume doubling - Premium client Acme Corp threatening contract review - Sales Director urgency about board meeting - IT Director asking for technical details</p>"},{"location":"day2/scenario1-debrief/#debrief-questions-that-work","title":"Debrief Questions That Work:","text":"<p>Decision Comparison: - \"Team A focused on customer communication first, Team B went straight to technical investigation - what are the pros/cons?\" - \"Which approach would work better in your workplace?\"</p> <p>Communication Analysis: - \"Which status updates would have reassured you as a customer?\" - \"How did teams balance technical details with business language?\"</p> <p>Pressure Response: - \"How did the escalating pressure messages affect your decision-making?\" - \"When did you feel most/least confident in your approach?\"</p> <p>Yes, you definitely have enough material for a solid 10-minute Scenario 1 debrief!</p>"},{"location":"day2/scenario2-debrief/","title":"Scenario2 debrief","text":""},{"location":"day2/scenario2-debrief/#scenario-2-debrief-elements-you-can-use","title":"Scenario 2 Debrief Elements You Can Use:","text":"<p>From the Escalation Card: - Partial success complexity - pipeline running but 40% data still missing - CEO involvement - board meeting happened with incomplete data - Multiple new stakeholders - Legal, Data Protection Officer, Marketing Director - External pressures - Media inquiry, competitor actions, formal client complaints - Complex decisions - technical vs. workaround, legal/compliance implications</p> <p>CEO Briefing Observations: - Communication effectiveness - which teams translated technical issues clearly? - Executive presence - how did teams handle direct CEO questioning? - Decision confidence - who gave firm commitments vs. hedged their answers? - Resource requests - what support did teams ask for?</p> <p>Cross-Team Comparison: - Risk assessment approaches - legal, competitive, technical priorities - Communication strategies - public vs. internal messaging decisions - Timeline commitments - conservative vs. aggressive promises - Escalation handling - how teams managed multiple senior stakeholders</p>"},{"location":"day2/scenario2-debrief/#rich-debrief-questions","title":"Rich Debrief Questions:","text":"<p>CEO Briefing Analysis: - \"Which team's CEO briefing felt most/least confident?\" - \"How did teams balance honesty about problems with leadership expectations?\" - \"What made some technical explanations clearer than others?\"</p> <p>Decision-Making Under Pressure: - \"How did teams handle the legal/compliance questions?\" - \"Which risk assessments seemed most comprehensive?\" - \"How did competitive pressure affect your communication decisions?\"</p> <p>Stakeholder Juggling: - \"How did teams prioritize between CEO, Legal, Marketing, and technical needs?\" - \"Which external pressures (media, competitors) influenced your approach most?\"</p> <p>Scenario 2 gives you excellent material for a 15-20 minute debrief! The CEO briefing element alone provides great cross-team comparison opportunities.</p>"},{"location":"day2/welcome-context/","title":"Day 2: Welcome &amp; Context Setting","text":""},{"location":"day2/welcome-context/#overview","title":"Overview","text":"<p>This session transitions from Day 1's \"when things work\" to Day 2's \"when things break\" by collecting workplace failure experiences and establishing the foundational concepts of incident vs problem management. This creates the context for today's hands-on breaking and responding activities.</p>"},{"location":"day2/welcome-context/#session-structure","title":"Session Structure","text":""},{"location":"day2/welcome-context/#1-discussion-system-failure-stories-workplace-edition-12-minutes","title":"1. Discussion: System Failure Stories - Workplace Edition (12 minutes)","text":"<p>Learning Type: Discussion Format: Structured sharing with experience capture</p>"},{"location":"day2/welcome-context/#sharing-structure-8-minutes","title":"Sharing Structure (8 minutes):","text":"<p>Individual Reflection (2 minutes): Think of a workplace example:</p> <ul> <li>System failure, data pipeline break, application outage</li> <li>Focus on the organizational response, not just the technical issue</li> </ul> <p>Round-Robin Sharing (6 minutes): Each learner shares briefly (45-60 seconds each):</p> <ul> <li>What broke? (brief technical context)</li> <li>How did people find out? (detection method)</li> <li>Who got involved? (response team/escalation)</li> <li>How long to fix? (resolution time)</li> <li>What was learned? (if anything)</li> </ul>"},{"location":"day2/welcome-context/#2-acquisition-incident-vs-problem-framework-8-minutes","title":"2. Acquisition: Incident vs Problem Framework (8 minutes)","text":"<p>Learning Type: Acquisition Format: Interactive presentation with real examples</p>"},{"location":"day2/welcome-context/#core-definitions-3-minutes","title":"Core Definitions (3 minutes):","text":"<p>Incident:</p> <ul> <li>Definition: Unplanned interruption or reduction in quality of service</li> <li>Focus: Restore service as quickly as possible</li> <li>Timeframe: Minutes to hours</li> <li>Example: \"Data pipeline failed at 2am, sales dashboard showing yesterday's data\"</li> </ul> <p>Problem:</p> <ul> <li>Definition: Root cause of one or more incidents</li> <li>Focus: Prevent incidents from recurring</li> <li>Timeframe: Days to weeks</li> <li>Example: \"Why do our pipelines keep failing when the source system releases schema changes?\"</li> </ul>"},{"location":"day2/welcome-context/#the-response-hierarchy-3-minutes","title":"The Response Hierarchy (3 minutes):","text":"<pre><code>INCIDENT MANAGEMENT           PROBLEM MANAGEMENT\n\u2193                            \u2193\nDetect \u2192 Respond \u2192 Restore   Analyze \u2192 Identify \u2192 Prevent\n\u2193                            \u2193\n\"Make it work now\"           \"Stop it happening again\"\n</code></pre> <p>Key Insight: You often need to do both simultaneously</p> <ul> <li>Immediate team: Fix the incident (restore service)</li> <li>Analysis team: Investigate the problem (prevent recurrence)</li> </ul>"},{"location":"day2/welcome-context/#real-world-application-2-minutes","title":"Real-World Application (2 minutes):","text":"<p>Scenario Walkthrough:</p> <p>\"A data pipeline feeding the customer dashboard fails at 6am. Customers start calling at 8am saying they can't see their orders.\"</p>"},{"location":"day3/governance/","title":"Day 3: Security &amp; Access Governance","text":""},{"location":"day3/governance/#overview","title":"Overview","text":"<p>This afternoon session shifts from data quality to data governance, focusing on security and access control. Through hands-on Lab 19 and structured discussion, learners explore how to balance data accessibility with security requirements, understanding governance as an operational discipline that enables rather than restricts business value.</p>"},{"location":"day3/governance/#session-structure","title":"Session Structure","text":""},{"location":"day3/governance/#phase-1-secure-data-access-lab-45-minutes","title":"Phase 1: Secure Data Access Lab (45 minutes)","text":""},{"location":"day3/governance/#lab-19-secure-data-access-in-microsoft-fabric-45-minutes","title":"Lab 19: Secure Data Access in Microsoft Fabric (45 minutes)","text":"<p>Microsoft Learn Lab: Secure data access in Microsoft Fabric</p> <p>Key Learning Focus for Today: While completing the lab, learners should pay attention to: - Workspace permissions: Who can access what at the workspace level? - Item-level security: Granular control over individual data assets - OneLake data access roles: File-level security and data lake permissions - Row-level security: Controlling access to specific data records</p> <p>Governance Mindset During Lab: As learners work through security configurations, encourage them to think: - Business justification: Why would you restrict this access? - User experience: How do security controls affect data consumers? - Operational impact: What happens when security is too restrictive vs. too permissive?</p> <p>Facilitator Notes: - Emphasize business context over technical configuration - Connect to real scenarios: \"When would you use row-level security?\" - Highlight trade-offs: \"How does security affect data accessibility?\" - Time management: Focus on understanding concepts over perfect setup</p>"},{"location":"day3/governance/#phase-2-security-vs-accessibility-analysis-20-minutes","title":"Phase 2: Security vs. Accessibility Analysis (20 minutes)","text":""},{"location":"day3/governance/#governance-trade-offs-discussion-20-minutes","title":"Governance Trade-offs Discussion (20 minutes)","text":"<p>Setup (2 minutes): Facilitator Frame: - \"You've just implemented multiple layers of data security\" - \"Good governance requires balancing security with business needs\" - \"Let's analyze the trade-offs and decision frameworks you just experienced\"</p>"},{"location":"day3/governance/#structured-analysis-15-minutes","title":"Structured Analysis (15 minutes):","text":"<p>Individual Reflection (5 minutes): Give learners this framework based on their lab experience:</p> <pre><code>SECURITY &amp; ACCESSIBILITY TRADE-OFFS\n\nWORKSPACE-LEVEL SECURITY:\n\u25a1 What business scenarios require workspace restrictions?\n\u25a1 How would this affect collaborative analytics work?\n\u25a1 What problems could overly restrictive workspace access cause?\n\nITEM-LEVEL SECURITY:\n\u25a1 When would you restrict access to specific datasets?\n\u25a1 How does this impact self-service analytics capabilities?\n\u25a1 What governance challenges arise with granular permissions?\n\nROW-LEVEL SECURITY:\n\u25a1 What business cases justify filtering data by user?\n\u25a1 How does this affect performance and user experience?\n\u25a1 What alternatives exist to row-level restrictions?\n\nGOVERNANCE DECISION FRAMEWORK:\n\u25a1 How do you decide what level of security to apply?\n\u25a1 Who should make these access control decisions?\n\u25a1 How do you balance compliance requirements with business agility?\n</code></pre> <p>Pair Discussion (5 minutes): - Form pairs (mix from different lab experiences if possible) - Share perspectives on security trade-offs observed - Focus question: \"What's the biggest governance challenge you identified?\"</p> <p>Group Discussion (5 minutes): Facilitator-led synthesis:</p> <p>Key Questions: - \"Where did you see the biggest tension between security and accessibility?\" - \"How would these security controls affect data scientists or analysts in your organization?\" - \"What governance decisions require business stakeholder input vs. technical judgment?\"</p> <p>Expected Insights to Highlight: - Over-restriction kills innovation - too much security prevents valuable analysis - Under-restriction creates compliance risk - inadequate security exposes sensitive data - User experience matters - complex security creates workarounds and shadow IT - Governance is about enabling business value safely, not just preventing access</p>"},{"location":"day3/governance/#bridge-to-governance-principles-3-minutes","title":"Bridge to Governance Principles (3 minutes):","text":"<p>Facilitator: - \"Security controls are just one part of data governance\" - \"Good governance creates frameworks for making these trade-off decisions consistently\" - \"Next we'll investigate governance approaches that balance multiple competing priorities\"</p>"},{"location":"day3/governance/#phase-3-governance-approaches-investigation-10-minutes","title":"Phase 3: Governance Approaches Investigation (10 minutes)","text":""},{"location":"day3/governance/#governance-framework-research-10-minutes","title":"Governance Framework Research (10 minutes)","text":"<p>Setup (1 minute): Research Focus: \"Investigate how organizations structure governance to make consistent, business-aligned decisions about data access, quality, and usage\"</p> <p>Team Formation: - Pairs or small groups (depending on class size) - Each group investigates a different governance approach</p>"},{"location":"day3/governance/#governance-approaches-to-research","title":"Governance Approaches to Research:","text":"<p>Group 1: Data Mesh Governance - Concept: Decentralized governance with domain ownership - Research focus: How do domain teams balance autonomy with consistency? - Key question: What governance decisions stay centralized vs. distributed?</p> <p>Group 2: Centralized Data Governance - Concept: Enterprise-wide standards and controls - Research focus: How do central teams ensure compliance while enabling innovation? - Key question: What are the benefits and challenges of centralized control?</p> <p>Group 3: Federated Data Governance - Concept: Shared governance across business units - Research focus: How do organizations coordinate governance across multiple stakeholders? - Key question: How do you resolve conflicts between different business needs?</p> <p>Group 4: Agile Data Governance - Concept: Iterative, flexible governance that evolves with business needs - Research focus: How do you maintain control while adapting quickly? - Key question: What governance elements can be lightweight vs. must be rigorous?</p>"},{"location":"day3/governance/#research-questions-8-minutes","title":"Research Questions (8 minutes):","text":"<p>For each governance approach, investigate:</p> <p>Decision-Making Structure: - Who makes governance decisions (access, quality standards, retention)? - How are business priorities balanced with technical constraints? - What escalation paths exist for governance conflicts?</p> <p>Implementation Practical: - How would this approach handle the security scenarios from Lab 19? - What tools and processes support this governance model? - How does this approach scale with organizational growth?</p> <p>Business Alignment: - How does this governance approach enable vs. restrict business value? - What cultural changes are required for successful implementation? - How do you measure governance effectiveness?</p>"},{"location":"day3/governance/#quick-sharing-1-minute","title":"Quick Sharing (1 minute):","text":"<p>Rapid insights: Each group shares one key finding (15 seconds each): - \"This governance approach works best when...\" - \"The biggest challenge with this model is...\" - \"This would solve the problem of...\"</p>"},{"location":"day3/governance/#transition-to-session-4","title":"Transition to Session 4","text":""},{"location":"day3/governance/#bridge-to-governance-framework-design-1-minute","title":"Bridge to Governance Framework Design (1 minute):","text":"<p>Facilitator: - \"You've seen different governance philosophies and experienced security trade-offs hands-on\" - \"After the break, we'll design a practical governance framework\" - \"We'll focus on creating decision-making processes that balance competing priorities\"</p>"},{"location":"day3/governance/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/governance/#pre-session-preparation","title":"Pre-Session Preparation:","text":"<ul> <li>Complete Lab 19 yourself to understand security configuration options</li> <li>Research governance models briefly to guide groups if needed</li> <li>Prepare real-world examples of governance trade-offs to reference</li> </ul>"},{"location":"day3/governance/#managing-the-lab-phase","title":"Managing the Lab Phase:","text":"<p>Emphasize Business Context: - Connect security to scenarios: \"When would a sales team need row-level security?\" - Highlight user impact: \"How would these restrictions affect a data analyst's daily work?\" - Discuss implementation costs: \"What's the operational overhead of maintaining these permissions?\"</p> <p>Common Lab Challenges: - Technical complexity: Focus on understanding concepts over perfect configuration - Permission errors: Use as learning opportunities about governance hierarchy - Time management: Ensure core security concepts are understood, skip advanced features if needed</p>"},{"location":"day3/governance/#facilitating-trade-offs-discussion","title":"Facilitating Trade-offs Discussion:","text":"<p>Drawing Out Business Thinking: - Use specific examples: \"In the row-level security setup, what business rule determined the filtering?\" - Challenge assumptions: \"Is restricting access always the answer to data security?\" - Connect to workplace: \"How do these decisions get made in your organization?\"</p> <p>Managing Governance Research: - Keep research practical: Focus on decision-making processes, not theoretical frameworks - Guide struggling groups: Suggest looking for case studies or implementation examples - Time box tightly: 8 minutes is quick research - aim for key insights, not comprehensive analysis</p>"},{"location":"day3/governance/#expected-learning-outcomes","title":"Expected Learning Outcomes:","text":"<p>Governance Mindset: - Security as business enabler - not just risk prevention - Trade-off thinking - balancing competing priorities systematically - Stakeholder coordination - governance involves business and technical perspectives</p> <p>Practical Skills: - Security implementation - hands-on experience with access controls - Decision frameworks - understanding different approaches to governance choices - Business communication - translating technical security to business impact</p>"},{"location":"day3/governance/#common-discussion-themes","title":"Common Discussion Themes:","text":"<p>Security Complexity: - Difficulty determining appropriate security levels - User experience degradation with complex permissions - Operational overhead of maintaining granular controls</p> <p>Governance Challenges: - Balancing centralized control with business agility - Coordinating decisions across multiple stakeholders - Measuring effectiveness of governance processes</p> <p>Business Alignment: - Ensuring governance supports rather than hinders business goals - Managing conflicts between security and accessibility - Evolving governance as business needs change</p>"},{"location":"day3/governance/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>S13: Use data systems securely to meet requirements and in line with organizational procedures</li> <li>K10: Concepts of data governance, including regulatory requirements, data privacy, security</li> <li>S22: Develop collaborative relationships with stakeholders (business vs. technical needs)</li> <li>K13: Implications of security, scalability, compliance regarding local, remote or distributed solutions</li> <li>B2: Works collaboratively with stakeholders to achieve common goals</li> </ul>"},{"location":"day3/governance/#time-management-tips","title":"Time Management Tips:","text":"<ul> <li>Lab phase: Monitor progress, assist with conceptual understanding over technical details</li> <li>Discussion phase: Use structured reflection to maximize learning from lab experience</li> <li>Research phase: Encourage rapid insight gathering over comprehensive research</li> <li>Keep transitions crisp: Each phase builds toward practical governance framework design</li> </ul>"},{"location":"day3/medallion-architecture/","title":"Day 3: Medallion Architecture &amp; Quality Patterns","text":""},{"location":"day3/medallion-architecture/#overview","title":"Overview","text":"<p>This session combines hands-on experience with medallion architecture (Lab 3b) and structured analysis of how quality patterns are implemented in data architecture. Learners see how the DMBOK quality dimensions they studied earlier are addressed through bronze, silver, and gold layer design, then investigate modern quality tools and approaches.</p>"},{"location":"day3/medallion-architecture/#session-structure","title":"Session Structure","text":""},{"location":"day3/medallion-architecture/#phase-1-medallion-architecture-lab-45-minutes","title":"Phase 1: Medallion Architecture Lab (45 minutes)","text":""},{"location":"day3/medallion-architecture/#lab-3b-create-a-medallion-architecture-45-minutes","title":"Lab 3b: Create a Medallion Architecture (45 minutes)","text":"<p>Microsoft Learn Lab: Create a medallion architecture in a Fabric lakehouse</p> <p>Key Learning Focus for Today: While completing the lab, learners should pay attention to: - Bronze layer: Raw data ingestion - what quality issues exist here? - Silver layer: Data cleaning and validation - what quality improvements happen? - Gold layer: Business-ready data - what quality standards are enforced?</p> <p>Facilitator Notes: - Circulate during lab to highlight quality aspects as learners work - Point out quality transformations: \"Notice how the silver layer handles missing values\" - Connect to DMBOK: \"This validation step addresses which quality dimension?\" - Time management: Keep lab moving - focus on completion over perfection</p>"},{"location":"day3/medallion-architecture/#phase-2-quality-patterns-analysis-20-minutes","title":"Phase 2: Quality Patterns Analysis (20 minutes)","text":""},{"location":"day3/medallion-architecture/#medallion-quality-mapping-discussion-20-minutes","title":"Medallion Quality Mapping Discussion (20 minutes)","text":"<p>Setup (2 minutes): Facilitator Frame: - \"You've just built a medallion architecture hands-on\" - \"Now let's analyze how this pattern addresses the DMBOK quality dimensions you studied earlier\" - \"We'll map specific quality improvements to each layer\"</p>"},{"location":"day3/medallion-architecture/#structured-quality-analysis-15-minutes","title":"Structured Quality Analysis (15 minutes):","text":"<p>Individual Reflection (5 minutes): Give learners this framework to complete based on their lab experience:</p> <pre><code>MEDALLION QUALITY MAPPING\n\nBRONZE LAYER - Raw Data:\n\u25a1 What quality issues did you observe in the raw data?\n\u25a1 Which DMBOK dimensions had problems here?\n\u25a1 What was preserved \"as-is\" and why?\n\nSILVER LAYER - Cleaned Data:  \n\u25a1 What quality transformations happened in silver?\n\u25a1 Which DMBOK dimensions were improved?\n\u25a1 What validation rules were applied?\n\nGOLD LAYER - Business-Ready Data:\n\u25a1 What additional quality standards were enforced?\n\u25a1 Which DMBOK dimensions reached \"production quality\"?\n\u25a1 What business rules ensured fitness for purpose?\n\nQUALITY PATTERNS OBSERVED:\n\u25a1 How does this layered approach handle different quality requirements?\n\u25a1 What are the trade-offs between layers?\n\u25a1 Where would you add additional quality checks?\n</code></pre> <p>Pair Discussion (5 minutes): - Share findings with a partner (different from morning DMBOK pairs) - Compare observations about quality transformations - Identify one quality improvement they found most effective</p> <p>Group Discussion (5 minutes): Facilitator-led analysis:</p> <p>Key Questions: - \"Which DMBOK dimensions were most improved by the silver layer transformations?\" - \"What quality trade-offs did you notice between preserving raw data and cleaning it?\" - \"Where would you add additional quality checks if this was a production system?\"</p> <p>Expected Insights to Highlight: - Bronze preserves completeness but may sacrifice accuracy - Silver improves accuracy and consistency through validation and standardization - Gold ensures fitness for purpose through business rule application - Different layers serve different quality needs - exploration vs. reporting vs. analytics</p>"},{"location":"day3/medallion-architecture/#bridge-to-quality-tools-3-minutes","title":"Bridge to Quality Tools (3 minutes):","text":"<p>Facilitator: - \"The medallion pattern gives you architecture for quality improvement\" - \"But you need tools to implement and monitor these quality standards\" - \"Next we'll investigate modern data quality tools that can automate what you just did manually\"</p>"},{"location":"day3/medallion-architecture/#phase-3-quality-tools-investigation-10-minutes","title":"Phase 3: Quality Tools Investigation (10 minutes)","text":""},{"location":"day3/medallion-architecture/#modern-data-quality-tools-research-10-minutes","title":"Modern Data Quality Tools Research (10 minutes)","text":"<p>Setup (1 minute): Research Focus: \"Investigate tools that could automate the quality checks you just implemented in the medallion architecture\"</p> <p>Team Formation: - Pairs or individuals (depending on class size) - Each pair/person takes a different tool category</p>"},{"location":"day3/medallion-architecture/#tool-categories-to-research","title":"Tool Categories to Research:","text":"<p>Category 1: Data Quality Frameworks - Great Expectations: Python-based data validation framework - dbt tests: Built-in and custom data quality tests - Focus: How do these tools implement quality checks similar to your silver layer transformations?</p> <p>Category 2: Cloud-Native Quality Tools - Azure Data Quality (in Fabric/Synapse): Built-in quality monitoring - AWS Glue DataBrew: Visual data quality profiling - Focus: How do cloud platforms handle quality monitoring and alerting?</p> <p>Category 3: Quality Monitoring &amp; Observability - Monte Carlo: Data observability and quality monitoring - Datadog Data Streams: Real-time quality monitoring - Focus: How do these tools detect quality issues automatically?</p> <p>Category 4: Open Source Quality Solutions - Apache Griffin: Data quality platform - DataHub: Data discovery with quality insights - Focus: How do open source tools provide cost-effective quality solutions?</p>"},{"location":"day3/medallion-architecture/#research-questions-8-minutes","title":"Research Questions (8 minutes):","text":"<p>For each tool category, investigate:</p> <p>Implementation Approach: - How would this tool integrate with a medallion architecture? - What quality checks could be automated? - How does it compare to manual validation?</p> <p>Quality Dimensions Covered: - Which DMBOK dimensions does this tool address best? - What types of quality issues would it catch/miss? - How does it handle quality monitoring vs. quality improvement?</p> <p>Practical Considerations: - What skills/resources needed for implementation? - How does it fit with Microsoft Fabric/Azure ecosystem? - What would be the first quality check you'd implement?</p>"},{"location":"day3/medallion-architecture/#quick-sharing-1-minute","title":"Quick Sharing (1 minute):","text":"<p>Rapid insights: Each pair/person shares one key finding (15 seconds each): - \"The most interesting capability we found was...\" - \"This tool would solve the problem of...\" - \"The biggest implementation challenge would be...\"</p>"},{"location":"day3/medallion-architecture/#transition-to-afternoon","title":"Transition to Afternoon","text":""},{"location":"day3/medallion-architecture/#bridge-to-security-governance-1-minute","title":"Bridge to Security &amp; Governance (1 minute):","text":"<p>Facilitator: - \"This morning you've focused on data quality - ensuring data meets standards\" - \"This afternoon we'll explore governance - ensuring the right people can access the right data safely\" - \"Quality and governance work together - high-quality data that's not properly secured and governed can't deliver business value\"</p>"},{"location":"day3/medallion-architecture/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/medallion-architecture/#pre-session-preparation","title":"Pre-Session Preparation:","text":"<ul> <li>Complete Lab 3b yourself to understand quality transformation points</li> <li>Research quality tools briefly to guide learners if they struggle</li> <li>Prepare quality examples from the lab to highlight during discussion</li> </ul>"},{"location":"day3/medallion-architecture/#managing-the-lab-phase","title":"Managing the Lab Phase:","text":"<p>Keep Quality Focus: - Point out quality improvements as they happen: \"Notice how this addresses accuracy\" - Connect to DMBOK: Reference morning's framework throughout - Time management: Ensure lab completion within 45 minutes</p> <p>Common Lab Issues: - If learners struggle with technical steps: Focus on understanding the quality patterns rather than perfect execution - If lab runs long: Guide them to complete bronze and silver layers, skip gold if necessary - If learners finish early: Have them analyze additional quality improvement opportunities</p>"},{"location":"day3/medallion-architecture/#facilitating-quality-discussion","title":"Facilitating Quality Discussion:","text":"<p>Drawing Out Insights: - Use specific examples from the lab: \"What happened to the invalid dates in silver layer?\" - Connect to workplace: \"How does this compare to quality processes in your organization?\" - Highlight trade-offs: \"Why keep raw data in bronze if it has quality issues?\"</p> <p>Managing Tools Research: - Keep research focused on practical implementation rather than deep technical details - Guide struggling researchers: Suggest looking for case studies or getting-started guides - Connect to medallion: \"How would this tool fit into the architecture you just built?\"</p>"},{"location":"day3/medallion-architecture/#expected-learning-outcomes","title":"Expected Learning Outcomes:","text":"<p>Practical Understanding: - Quality patterns in action - seeing DMBOK dimensions addressed through architecture - Tool awareness - knowing what modern quality solutions exist - Implementation thinking - connecting theory to practical application</p> <p>Quality Mindset: - Layered quality approach - different standards for different purposes - Automation possibilities - tools can implement quality checks at scale - Integration considerations - quality tools work within broader data architecture</p>"},{"location":"day3/medallion-architecture/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K4: Frameworks for data quality (DMBOK applied to architecture)</li> <li>S26: Identify data quality metrics and track them (quality tools investigation)</li> <li>S6: Systematically clean, validate, and describe data at ETL stages (medallion lab)</li> <li>S25: Assess gaps in existing tools and technologies (quality tools research)</li> <li>B3: Quality focus that promotes continuous improvement (throughout session)</li> </ul>"},{"location":"day3/medallion-architecture/#time-management-tips","title":"Time Management Tips:","text":"<ul> <li>Lab phase: Use timer, keep groups moving together</li> <li>Discussion phase: Limit individual reflection to 5 minutes max</li> <li>Research phase: Encourage \"good enough\" research over perfect analysis</li> <li>Sharing: Use strict time limits for rapid knowledge sharing</li> </ul>"},{"location":"day3/quality-foundation/","title":"Day 3: Welcome &amp; Quality Foundation","text":""},{"location":"day3/quality-foundation/#overview","title":"Overview","text":"<p>This opening session transitions from Day 2's \"when systems break\" to Day 3's \"when systems work but data doesn't meet standards.\" Through discussion and concept introduction, learners explore what quality means in operational contexts and why data quality is a different type of operational challenge requiring similar systematic approaches.</p>"},{"location":"day3/quality-foundation/#session-structure","title":"Session Structure","text":""},{"location":"day3/quality-foundation/#discussion-what-does-good-data-mean-12-minutes","title":"Discussion: What Does \"Good Data\" Mean? (12 minutes)","text":""},{"location":"day3/quality-foundation/#opening-frame-2-minutes","title":"Opening Frame (2 minutes):","text":"<p>Facilitator Introduction: - \"Yesterday we focused on incident response - when technical systems break\" - \"Today we're exploring a different operational challenge: when systems work fine, but the data doesn't meet our standards\" - \"This is about quality and governance - making sure data is 'good enough' for its intended use\"</p>"},{"location":"day3/quality-foundation/#individual-reflection-3-minutes","title":"Individual Reflection (3 minutes):","text":"<p>Question for learners: \"Think about data you use in your work - what makes data 'good' versus 'bad'? What frustrates you most about poor quality data?\"</p> <p>Reflection prompts: - Think of a time when you couldn't trust data you were given - What made you lose confidence in a report or dashboard? - When have you had to spend time cleaning or fixing data before you could use it?</p>"},{"location":"day3/quality-foundation/#group-sharing-5-minutes","title":"Group Sharing (5 minutes):","text":"<p>Structured sharing: Each learner shares one example (30-45 seconds): - \"Bad data for me is when...\" - \"I lose confidence in data when...\" - \"The most frustrating data quality issue I face is...\"</p> <p>Facilitator role: - Listen for patterns and capture themes on screen/flip chart - Group similar responses: accuracy issues, completeness problems, timeliness concerns - Note the business impact - not just technical problems but work impact</p>"},{"location":"day3/quality-foundation/#pattern-recognition-2-minutes","title":"Pattern Recognition (2 minutes):","text":"<p>Facilitator synthesis: - \"I'm hearing themes around accuracy, completeness, timing...\" - \"Notice how quality problems affect your ability to do your job\" - \"These aren't just technical issues - they have real business consequences\"</p> <p>Expected themes to emerge: - Accuracy: Wrong numbers, outdated information - Completeness: Missing records, partial data - Consistency: Same thing measured differently across systems - Timeliness: Data arriving too late to be useful - Relevance: Right data but not quite what's needed - Trust: Uncertainty about whether data can be relied upon</p>"},{"location":"day3/quality-foundation/#acquisition-data-quality-as-operational-challenge-8-minutes","title":"Acquisition: Data Quality as Operational Challenge (8 minutes)","text":""},{"location":"day3/quality-foundation/#reframe-quality-as-operations-3-minutes","title":"Reframe Quality as Operations (3 minutes):","text":"<p>Connect to Yesterday: - \"Yesterday you handled incidents - systems failing completely\" - \"Today's challenge is more subtle: systems work, but output isn't meeting standards\" - \"This requires the same systematic thinking, but different detection and response approaches\"</p> <p>Quality vs. Incidents: <pre><code>SYSTEM INCIDENTS          vs.    DATA QUALITY ISSUES\n\u2193                                \u2193\nPipeline stops working           Pipeline produces unreliable data\nClear failure signal           Subtle degradation signal\nImmediate impact               Gradual impact\nTechnical fix needed           Process/standards fix needed\n</code></pre></p>"},{"location":"day3/quality-foundation/#why-quality-matters-operationally-3-minutes","title":"Why Quality Matters Operationally (3 minutes):","text":"<p>Business Impact Examples: - Decision-making: Executives making strategic decisions based on inaccurate data - Customer experience: Personalization systems using outdated customer preferences - Compliance: Regulatory reports with incomplete or inconsistent data - Efficiency: Teams spending time validating data instead of analyzing it</p> <p>The Quality Challenge: - Detection is harder: Systems appear to be working fine - Standards are subjective: \"Good enough\" varies by use case - Prevention requires governance: Processes, policies, and culture change - Impact is often delayed: Problems discovered weeks or months later</p>"},{"location":"day3/quality-foundation/#operational-quality-dimensions-2-minutes","title":"Operational Quality Dimensions (2 minutes):","text":"<p>Quick introduction to key concepts: - Fitness for purpose: Data quality depends on how it will be used - Quality standards: Need clear, measurable criteria - Quality monitoring: Ongoing measurement, not one-time assessment - Quality improvement: Systematic processes for addressing issues</p> <p>Preview today's approach: - \"We'll define quality standards using proven frameworks\" - \"See how to implement quality patterns in data architectures\" - \"Practice balancing quality requirements with practical constraints\"</p>"},{"location":"day3/quality-foundation/#transition-to-next-session","title":"Transition to Next Session","text":""},{"location":"day3/quality-foundation/#bridge-to-dmbok-investigation-1-minute","title":"Bridge to DMBOK Investigation (1 minute):","text":"<p>Facilitator: - \"You've identified quality challenges from your experience\" - \"Now we'll explore a professional framework for thinking about data quality\" - \"The DMBOK (Data Management Body of Knowledge) provides six specific dimensions for measuring and improving data quality\" - \"This will give you systematic language for the quality issues you've experienced\"</p>"},{"location":"day3/quality-foundation/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/quality-foundation/#managing-the-discussion","title":"Managing the Discussion:","text":"<p>If learners struggle to think of examples: - \"Think about spreadsheets you've received that had problems\" - \"Consider dashboards or reports where you questioned the numbers\" - \"What about customer data that seemed outdated or wrong?\"</p> <p>If examples get too technical: - \"Focus on the impact on your work, not the technical cause\" - \"How did the quality problem affect your decision-making?\"</p> <p>If discussion goes long: - Capture themes quickly and move to acquisition - \"I'm hearing great examples - let's put framework around these experiences\"</p>"},{"location":"day3/quality-foundation/#common-quality-issues-to-expect","title":"Common Quality Issues to Expect:","text":"<p>Accuracy Problems: - Wrong customer contact information - Incorrect financial calculations - Outdated product pricing</p> <p>Completeness Issues: - Missing customer records - Partial transaction data - Incomplete survey responses</p> <p>Consistency Problems: - Different systems showing different customer names - Conflicting sales figures across reports - Varying date formats</p> <p>Timeliness Issues: - Reports showing yesterday's data for real-time decisions - Batch processing delays affecting morning meetings - Historical data needed but not available</p>"},{"location":"day3/quality-foundation/#key-insights-to-highlight","title":"Key Insights to Highlight:","text":"<p>Quality is Contextual: - Data that's \"good enough\" for one purpose may be inadequate for another - Quality requirements change based on business criticality - Perfect data quality is often neither necessary nor cost-effective</p> <p>Quality is Operational: - Requires ongoing monitoring, not just initial assessment - Needs clear standards and measurement processes - Benefits from the same systematic thinking as incident response</p> <p>Quality is Organizational: - Often involves multiple teams and systems - Requires governance processes and clear accountability - Culture and processes matter as much as technology</p>"},{"location":"day3/quality-foundation/#expected-outcomes","title":"Expected Outcomes:","text":"<p>By the end of this session, learners should: 1. Connect personal experience with formal data quality concepts 2. Understand quality as operational challenge requiring systematic approaches 3. Appreciate the business impact of data quality issues 4. Be prepared to explore structured frameworks for defining and measuring quality</p>"},{"location":"day3/quality-foundation/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K4: Frameworks for data quality covering dimensions such as accuracy, completeness, consistency</li> <li>S26: Identify data quality metrics and track them to ensure quality, accuracy and reliability</li> <li>K5: The inherent risks of data such as incomplete data, ethical data sources</li> <li>B3: Quality focus that promotes continuous improvement</li> </ul>"},{"location":"day3/quality-patterns/","title":"Day 3: Quality Patterns","text":""},{"location":"day3/quality-patterns/#overview","title":"Overview","text":"<p>This session combines hands-on experience with medallion architecture (Lab 3b) and structured analysis of how quality patterns are implemented in data architecture. Learners see how the DMBOK quality dimensions they studied earlier are addressed through bronze, silver, and gold layer design, then investigate modern quality tools and approaches.</p> <p>Key Learning Focus for Today: While completing the lab, learners should pay attention to: - Bronze layer: Raw data ingestion - what quality issues exist here? - Silver layer: Data cleaning and validation - what quality improvements happen? - Gold layer: Business-ready data - what quality standards are enforced?</p> <p>Facilitator Notes: - Circulate during lab to highlight quality aspects as learners work - Point out quality transformations: \"Notice how the silver layer handles missing values\" - Connect to DMBOK: \"This validation step addresses which quality dimension?\" - Time management: Keep lab moving - focus on completion over perfection</p>"},{"location":"day3/quality-patterns/#phase-2-quality-patterns-analysis-20-minutes","title":"Phase 2: Quality Patterns Analysis (20 minutes)","text":""},{"location":"day3/quality-patterns/#medallion-quality-mapping-discussion-20-minutes","title":"Medallion Quality Mapping Discussion (20 minutes)","text":"<p>Setup (2 minutes): Facilitator Frame: - \"You've just built a medallion architecture hands-on\" - \"Now let's analyze how this pattern addresses the DMBOK quality dimensions you studied earlier\" - \"We'll map specific quality improvements to each layer\"</p>"},{"location":"day3/quality-patterns/#structured-quality-analysis-15-minutes","title":"Structured Quality Analysis (15 minutes):","text":"<p>Individual Reflection (5 minutes): Give learners this framework to complete based on their lab experience:</p> <pre><code>MEDALLION QUALITY MAPPING\n\nBRONZE LAYER - Raw Data:\n\u25a1 What quality issues did you observe in the raw data?\n\u25a1 Which DMBOK dimensions had problems here?\n\u25a1 What was preserved \"as-is\" and why?\n\nSILVER LAYER - Cleaned Data:  \n\u25a1 What quality transformations happened in silver?\n\u25a1 Which DMBOK dimensions were improved?\n\u25a1 What validation rules were applied?\n\nGOLD LAYER - Business-Ready Data:\n\u25a1 What additional quality standards were enforced?\n\u25a1 Which DMBOK dimensions reached \"production quality\"?\n\u25a1 What business rules ensured fitness for purpose?\n\nQUALITY PATTERNS OBSERVED:\n\u25a1 How does this layered approach handle different quality requirements?\n\u25a1 What are the trade-offs between layers?\n\u25a1 Where would you add additional quality checks?\n</code></pre> <p>Pair Discussion (5 minutes): - Share findings with a partner (different from morning DMBOK pairs) - Compare observations about quality transformations - Identify one quality improvement they found most effective</p> <p>Group Discussion (5 minutes): Facilitator-led analysis:</p> <p>Key Questions: - \"Which DMBOK dimensions were most improved by the silver layer transformations?\" - \"What quality trade-offs did you notice between preserving raw data and cleaning it?\" - \"Where would you add additional quality checks if this was a production system?\"</p> <p>Expected Insights to Highlight: - Bronze preserves completeness but may sacrifice accuracy - Silver improves accuracy and consistency through validation and standardization - Gold ensures fitness for purpose through business rule application - Different layers serve different quality needs - exploration vs. reporting vs. analytics</p>"},{"location":"day3/quality-patterns/#bridge-to-quality-tools-3-minutes","title":"Bridge to Quality Tools (3 minutes):","text":"<p>Facilitator: - \"The medallion pattern gives you architecture for quality improvement\" - \"But you need tools to implement and monitor these quality standards\" - \"Next we'll investigate modern data quality tools that can automate what you just did manually\"</p>"},{"location":"day3/quality-tools/","title":"Quality tools","text":""},{"location":"day3/quality-tools/#phase-3-quality-tools-investigation-10-minutes","title":"Phase 3: Quality Tools Investigation (10 minutes)","text":""},{"location":"day3/quality-tools/#modern-data-quality-tools-research-10-minutes","title":"Modern Data Quality Tools Research (10 minutes)","text":"<p>Setup (1 minute): Research Focus: \"Investigate tools that could automate the quality checks you just implemented in the medallion architecture\"</p> <p>Team Formation: - Pairs or individuals (depending on class size) - Each pair/person takes a different tool category</p>"},{"location":"day3/quality-tools/#tool-categories-to-research","title":"Tool Categories to Research:","text":"<p>Category 1: Data Quality Frameworks - Great Expectations: Python-based data validation framework - dbt tests: Built-in and custom data quality tests - Focus: How do these tools implement quality checks similar to your silver layer transformations?</p> <p>Category 2: Cloud-Native Quality Tools - Azure Data Quality (in Fabric/Synapse): Built-in quality monitoring - AWS Glue DataBrew: Visual data quality profiling - Focus: How do cloud platforms handle quality monitoring and alerting?</p> <p>Category 3: Quality Monitoring &amp; Observability - Monte Carlo: Data observability and quality monitoring - Datadog Data Streams: Real-time quality monitoring - Focus: How do these tools detect quality issues automatically?</p> <p>Category 4: Open Source Quality Solutions - Apache Griffin: Data quality platform - DataHub: Data discovery with quality insights - Focus: How do open source tools provide cost-effective quality solutions?</p>"},{"location":"day3/quality-tools/#research-questions-8-minutes","title":"Research Questions (8 minutes):","text":"<p>For each tool category, investigate:</p> <p>Implementation Approach: - How would this tool integrate with a medallion architecture? - What quality checks could be automated? - How does it compare to manual validation?</p> <p>Quality Dimensions Covered: - Which DMBOK dimensions does this tool address best? - What types of quality issues would it catch/miss? - How does it handle quality monitoring vs. quality improvement?</p> <p>Practical Considerations: - What skills/resources needed for implementation? - How does it fit with Microsoft Fabric/Azure ecosystem? - What would be the first quality check you'd implement?</p>"},{"location":"day3/quality-tools/#quick-sharing-1-minute","title":"Quick Sharing (1 minute):","text":"<p>Rapid insights: Each pair/person shares one key finding (15 seconds each): - \"The most interesting capability we found was...\" - \"This tool would solve the problem of...\" - \"The biggest implementation challenge would be...\"</p>"},{"location":"day3/quality-tools/#transition-to-afternoon","title":"Transition to Afternoon","text":""},{"location":"day3/quality-tools/#bridge-to-security-governance-1-minute","title":"Bridge to Security &amp; Governance (1 minute):","text":"<p>Facilitator: - \"This morning you've focused on data quality - ensuring data meets standards\" - \"This afternoon we'll explore governance - ensuring the right people can access the right data safely\" - \"Quality and governance work together - high-quality data that's not properly secured and governed can't deliver business value\"</p>"},{"location":"day3/quality-tools/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day3/quality-tools/#pre-session-preparation","title":"Pre-Session Preparation:","text":"<ul> <li>Complete Lab 3b yourself to understand quality transformation points</li> <li>Research quality tools briefly to guide learners if they struggle</li> <li>Prepare quality examples from the lab to highlight during discussion</li> </ul>"},{"location":"day3/quality-tools/#managing-the-lab-phase","title":"Managing the Lab Phase:","text":"<p>Keep Quality Focus: - Point out quality improvements as they happen: \"Notice how this addresses accuracy\" - Connect to DMBOK: Reference morning's framework throughout - Time management: Ensure lab completion within 45 minutes</p> <p>Common Lab Issues: - If learners struggle with technical steps: Focus on understanding the quality patterns rather than perfect execution - If lab runs long: Guide them to complete bronze and silver layers, skip gold if necessary - If learners finish early: Have them analyze additional quality improvement opportunities</p>"},{"location":"day3/quality-tools/#facilitating-quality-discussion","title":"Facilitating Quality Discussion:","text":"<p>Drawing Out Insights: - Use specific examples from the lab: \"What happened to the invalid dates in silver layer?\" - Connect to workplace: \"How does this compare to quality processes in your organization?\" - Highlight trade-offs: \"Why keep raw data in bronze if it has quality issues?\"</p> <p>Managing Tools Research: - Keep research focused on practical implementation rather than deep technical details - Guide struggling researchers: Suggest looking for case studies or getting-started guides - Connect to medallion: \"How would this tool fit into the architecture you just built?\"</p>"},{"location":"day3/quality-tools/#expected-learning-outcomes","title":"Expected Learning Outcomes:","text":"<p>Practical Understanding: - Quality patterns in action - seeing DMBOK dimensions addressed through architecture - Tool awareness - knowing what modern quality solutions exist - Implementation thinking - connecting theory to practical application</p> <p>Quality Mindset: - Layered quality approach - different standards for different purposes - Automation possibilities - tools can implement quality checks at scale - Integration considerations - quality tools work within broader data architecture</p>"},{"location":"day3/quality-tools/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K4: Frameworks for data quality (DMBOK applied to architecture)</li> <li>S26: Identify data quality metrics and track them (quality tools investigation)</li> <li>S6: Systematically clean, validate, and describe data at ETL stages (medallion lab)</li> <li>S25: Assess gaps in existing tools and technologies (quality tools research)</li> <li>B3: Quality focus that promotes continuous improvement (throughout session)</li> </ul>"},{"location":"day3/quality-tools/#time-management-tips","title":"Time Management Tips:","text":"<ul> <li>Lab phase: Use timer, keep groups moving together</li> <li>Discussion phase: Limit individual reflection to 5 minutes max</li> <li>Research phase: Encourage \"good enough\" research over perfect analysis</li> <li>Sharing: Use strict time limits for rapid knowledge sharing</li> </ul>"},{"location":"day4/introduction/","title":"Day 4 - Improvement &amp; Value","text":""},{"location":"day4/introduction/#introduction","title":"Introduction","text":""},{"location":"day4/welcome/","title":"Day 4: Welcome &amp; Improvement Mindset","text":"<p>Time: 9:30-9:50 (20 minutes) Learning Types: Discussion \u2192 Acquisition</p>"},{"location":"day4/welcome/#overview","title":"Overview","text":"<p>This final day focuses on operational evolution - moving from reactive problem-solving to proactive improvement and optimization. The welcome session establishes the mindset shift from \"fixing what's broken\" to \"making good systems even better\" while connecting to the systematic thinking developed over the previous three days.</p>"},{"location":"day4/welcome/#session-structure","title":"Session Structure","text":""},{"location":"day4/welcome/#discussion-from-reactive-to-proactive-operations-12-minutes","title":"Discussion: From Reactive to Proactive Operations (12 minutes)","text":""},{"location":"day4/welcome/#opening-frame-2-minutes","title":"Opening Frame (2 minutes):","text":"<p>Facilitator Introduction: - \"Welcome to Day 4 - our final day together on operational data engineering\" - \"Days 1-3 focused on building, monitoring, breaking, and governing data operations\" - \"Today we shift from reactive to proactive - how do you continuously improve and evolve data systems?\"</p>"},{"location":"day4/welcome/#learning-journey-reflection-3-minutes","title":"Learning Journey Reflection (3 minutes):","text":"<p>Quick recap with learners: - Day 1: \"What was the key insight about monitoring?\" - Day 2: \"What surprised you most about incident response?\" - Day 3: \"What's the biggest quality/governance challenge you identified?\"</p> <p>Connect the progression: - \"You've built operational skills for when things work, when they break, and when they need governing\" - \"Today: How do you make them work even better?\"</p>"},{"location":"day4/welcome/#individual-reflection-4-minutes","title":"Individual Reflection (4 minutes):","text":"<p>Question for learners: \"Think about a system or process in your workplace that works 'well enough' but could be better. What would 'better' look like? What stops you from improving it?\"</p> <p>Reflection prompts: - Data pipeline that works but is slow/expensive - Process that functions but requires manual intervention - System that delivers results but is hard to maintain - Tool that does the job but frustrates users</p>"},{"location":"day4/welcome/#group-sharing-3-minutes","title":"Group Sharing (3 minutes):","text":"<p>Structured sharing: Each learner shares one example (20-30 seconds): - \"Something that works but could be better is...\" - \"Better would mean...\" - \"The main obstacle to improvement is...\"</p> <p>Facilitator role: - Listen for improvement themes: performance, cost, maintainability, user experience - Note barriers: time, resources, prioritization, technical debt - Capture on screen: Common improvement opportunities and obstacles</p>"},{"location":"day4/welcome/#acquisition-continuous-improvement-framework-8-minutes","title":"Acquisition: Continuous Improvement Framework (8 minutes)","text":""},{"location":"day4/welcome/#the-improvement-mindset-3-minutes","title":"The Improvement Mindset (3 minutes):","text":"<p>Operational Maturity Levels: <pre><code>REACTIVE OPERATIONS          PROACTIVE OPERATIONS\n\u2193                           \u2193\nFix when broken      \u2192      Prevent problems\nRespond to requests  \u2192      Anticipate needs  \nManual processes     \u2192      Automated workflows\nIndividual heroics   \u2192      Systematic improvement\n\"If it works, don't touch\"  \"If it works, make it work better\"\n</code></pre></p> <p>Why Continuous Improvement Matters: - Technical debt accumulates - small problems become big problems - Business needs evolve - today's solution may not fit tomorrow's requirements - Technology advances - better tools and approaches become available - Cost optimization - efficient systems save money and resources</p>"},{"location":"day4/welcome/#improvement-opportunity-categories-3-minutes","title":"Improvement Opportunity Categories (3 minutes):","text":"<p>Performance &amp; Efficiency: - Faster processing, lower latency, reduced resource usage - Automation of manual tasks, elimination of bottlenecks</p> <p>Reliability &amp; Maintainability: - Easier troubleshooting, better error handling, simplified architecture - Reduced dependency on specific people or knowledge</p> <p>User Experience &amp; Business Value: - More intuitive interfaces, self-service capabilities, better data accessibility - Enhanced analytics capabilities, real-time insights</p> <p>Cost &amp; Resource Optimization: - Lower infrastructure costs, improved resource utilization - Reduced operational overhead, simplified maintenance</p>"},{"location":"day4/welcome/#todays-improvement-framework-2-minutes","title":"Today's Improvement Framework (2 minutes):","text":"<p>The Day 4 Approach: 1. Assess current state - where are we now? (gap analysis) 2. Explore possibilities - what's available? (technology assessment) 3. Plan systematically - how do we get there? (deployment and change management) 4. Measure and iterate - how do we know it's better? (continuous improvement)</p> <p>Key Questions We'll Answer: - How do you identify improvement opportunities systematically? - What new technologies could enhance your data operations? - How do you plan and execute operational improvements safely? - How do you build a culture of continuous improvement?</p>"},{"location":"day4/welcome/#transition-to-session-1","title":"Transition to Session 1","text":""},{"location":"day4/welcome/#bridge-to-technology-assessment-1-minute","title":"Bridge to Technology Assessment (1 minute):","text":"<p>Facilitator: - \"You've identified improvement opportunities from your experience\" - \"Now we'll take a systematic approach to assessing what's possible\" - \"We'll start with gap analysis - understanding the difference between current state and desired state\" - \"Then explore emerging technologies that could bridge those gaps\"</p>"},{"location":"day4/welcome/#resources-for-trainers","title":"Resources for Trainers","text":""},{"location":"day4/welcome/#managing-the-discussion","title":"Managing the Discussion:","text":"<p>If learners focus only on technical improvements: - \"What about user experience or business process improvements?\" - \"How would non-technical stakeholders benefit from these changes?\"</p> <p>If examples are too specific/narrow: - \"Think broader - process improvements, tool upgrades, automation opportunities\" - \"Consider improvements that affect multiple people or systems\"</p> <p>If discussion gets pessimistic about obstacles: - \"Those are real constraints - how do you work within them to still make progress?\" - \"What small improvements could you make without requiring major resources?\"</p>"},{"location":"day4/welcome/#common-improvement-themes-to-expect","title":"Common Improvement Themes to Expect:","text":"<p>Technical Improvements: - Performance optimization, automation opportunities - Tool upgrades, architecture simplification - Monitoring and alerting enhancements</p> <p>Process Improvements: - Workflow streamlining, self-service capabilities - Documentation and knowledge sharing - Collaboration and communication enhancements</p> <p>Business Value Improvements: - Faster decision-making, better data accessibility - New analytical capabilities, real-time insights - Cost reduction, resource optimization</p>"},{"location":"day4/welcome/#expected-outcomes","title":"Expected Outcomes:","text":"<p>By the end of this session, learners should: 1. Shift mindset from reactive to proactive operational thinking 2. Identify improvement opportunities systematically rather than reactively 3. Understand improvement categories and how they create business value 4. Be prepared for structured assessment of current state vs. desired state</p>"},{"location":"day4/welcome/#connection-to-ksbs","title":"Connection to KSBs:","text":"<ul> <li>K28: Continuous improvement including how to capture good practice and lessons learned</li> <li>S21: Identify and remediate technical debt, assess for updates and obsolescence</li> <li>S25: Assess and identify gaps in existing tools and technologies</li> <li>B3: Quality focus that promotes continuous improvement</li> <li>B6: Keeps abreast of developments in emerging technologies</li> </ul>"},{"location":"labs/01-lakehouse/","title":"Lab: Create a Microsoft Fabric Lakehouse","text":"<p>Large-scale data analytics solutions have traditionally been built around a data warehouse, in which data is stored in relational tables and queried using SQL. The growth in \u201cbig data\u201d (characterized by high volumes, variety, and velocity of new data assets) together with the availability of low-cost storage and cloud-scale distributed compute technologies has led to an alternative approach to analytical data storage; the data lake. In a data lake, data is stored as files without imposing a fixed schema for storage. Increasingly, data engineers and analysts seek to benefit from the best features of both of these approaches by combining them in a data lakehouse; in which data is stored in files in a data lake and a relational schema is applied to them as a metadata layer so that they can be queried using traditional SQL semantics.</p> <p>In Microsoft Fabric, a lakehouse provides highly scalable file storage in a OneLake store (built on Azure Data Lake Store Gen2) with a metastore for relational objects such as tables and views based on the open source Delta Lake table format. Delta Lake enables you to define a schema of tables in your lakehouse that you can query using SQL.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"labs/01-lakehouse/#signing-in-to-microsoft-fabric","title":"Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> </ol> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p>"},{"location":"labs/01-lakehouse/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> </ol> <p>When your new workspace opens, it should be empty.</p> <p></p>"},{"location":"labs/01-lakehouse/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> <li> <p>View the new lakehouse, and note that the Lakehouse explorer pane on the left enables you to browse tables and files in the lakehouse:</p> <ul> <li> <p>The Tables folder contains tables that you can query using SQL semantics. Tables in a Microsoft Fabric lakehouse are based on the open source Delta Lake file format, commonly used in Apache Spark.</p> </li> <li> <p>The Files folder contains data files in the OneLake storage for the lakehouse that aren't associated with managed delta tables. You can also create shortcuts in this folder to reference data that is stored externally.</p> </li> </ul> </li> </ol> <p>Currently, there are no tables or files in this lakehouse.</p>"},{"location":"labs/01-lakehouse/#upload-a-file","title":"Upload a file","text":"<p>Fabric provides multiple ways to load data into the lakehouse, including built-in support for pipelines that copy data from external sources and data flows (Gen 2) that you can define using visual tools based on Power Query. However one of the simplest ways to ingest small amounts of data is to upload files or folders from your local computer (or lab VM if applicable).</p> <ol> <li> <p>Download the sales.csv file from https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv, </p> <ul> <li>Save it as <code>sales.csv</code> on your local computer (or lab VM if applicable).</li> </ul> <p>Note</p> <ul> <li>To download the file, open a new tab in the browser and paste in the URL.</li> <li>Right click anywhere on the page containing the data and select \"Save as\" to save the data as a CSV file.</li> </ul> </li> <li> <p>Return to the web browser tab containing your lakehouse</p> <ul> <li>Click the <code>...</code> menu for the Files folder in the Explorer pane select New subfolder</li> <li>Name the new subfolder: <code>data</code></li> <li>Click Create</li> </ul> </li> <li> <p>In the <code>...</code> menu for the new data folder, select Upload and Upload files.</p> <ul> <li>Then upload the sales.csv file from your local computer (or lab VM if applicable).</li> </ul> </li> <li> <p>After the file has been uploaded, select the Files/data folder and verify that the sales.csv file has been uploaded, as shown here:</p> <p></p> </li> <li> <p>Select the sales.csv file to see a preview of its contents.</p> <p>If the sales.csv file does not automatically appear, in the <code>...</code> menu for the data folder, select Refresh.</p> </li> </ol>"},{"location":"labs/01-lakehouse/#explore-shortcuts","title":"Explore shortcuts","text":"<p>In many scenarios, the data you need to work with in your lakehouse may be stored in some other location. While there are many ways to ingest data into the OneLake storage for your lakehouse, another option is to instead create a shortcut. Shortcuts enable you to include externally sourced data in your analytics solution without the overhead and risk of data inconsistency associated with copying it.</p> <ol> <li> <p>In the <code>...</code> menu for the Files folder, select New shortcut.</p> </li> <li> <p>View the available data source types for shortcuts.</p> <ul> <li>Then close the New shortcut dialog box without creating a shortcut.</li> </ul> </li> </ol>"},{"location":"labs/01-lakehouse/#load-file-data-into-a-table","title":"Load file data into a table","text":"<p>The sales data you uploaded is in a file, which you can work with directly by using Apache Spark code. However, in many scenarios you may want to load the data from the file into a table so that you can query it using SQL.</p> <ol> <li> <p>In the Explorer pane, select the Files/data folder so you can see the sales.csv file it contains.</p> </li> <li> <p>In the <code>...</code> menu for the sales.csv file, select Load to Tables &gt; New table.</p> <p></p> </li> <li> <p>In Load to table dialog box, set the table name to sales and confirm the load operation.</p> <ul> <li>Then wait for the table to be created and loaded.</li> </ul> <p>If the sales table does not automatically appear, in the <code>...</code> menu for the Tables folder, select Refresh.</p> </li> <li> <p>In the Explorer pane, select the sales table that has been created to view the data:</p> <p></p> </li> <li> <p>In the <code>...</code> menu for the sales table, select View files to see the underlying files for this table:</p> <p></p> <p>Files for a delta table are stored in Parquet format, and include a subfolder named <code>_delta_log</code> in which details of transactions applied to the table are logged.</p> </li> </ol>"},{"location":"labs/01-lakehouse/#use-sql-to-query-tables","title":"Use SQL to query tables","text":"<p>When you create a lakehouse and define tables in it, a SQL endpoint is automatically created through which the tables can be queried using SQL <code>SELECT</code> statements.</p> <ol> <li> <p>At the top-right of the Lakehouse page, switch from Lakehouse to SQL analytics endpoint.</p> <ul> <li>Then wait a short time until the SQL analytics endpoint for your lakehouse opens in a visual interface from which you can query its tables.</li> </ul> </li> <li> <p>Use the New SQL query button to open a new query editor, and enter the following SQL query:</p> </li> </ol> <pre><code>SELECT Item, SUM(Quantity * UnitPrice) AS Revenue\nFROM sales\nGROUP BY Item\nORDER BY Revenue DESC;\n</code></pre> <ol> <li> <p>Use the  Run button to run the query and view the results, which should show the total revenue for each product.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#create-a-visual-query","title":"Create a visual query","text":"<p>While many data professionals are familiar with SQL, those with Power BI experience can apply their Power Query skills to create visual queries.</p> <ol> <li> <p>On the toolbar, expand the New SQL query option and select New visual query.</p> </li> <li> <p>Drag the sales table (under dbo &gt; Tables) to the new visual query editor pane that opens to create a Power Query as shown here:</p> <p></p> </li> <li> <p>In the Manage columns menu, select Choose columns.</p> <ul> <li>Then select only the SalesOrderNumber and SalesOrderLineNumber columns. Click OK</li> </ul> <p></p> </li> <li> <p>in the Transform menu, select Group by. Then group the data by using the following Basic settings:</p> <ul> <li>Group by: SalesOrderNumber</li> <li>New column name: <code>LineItems</code></li> <li>Operation: Count distinct values</li> <li>Column: SalesOrderLineNumber (if not greyed out)</li> </ul> <p>When you're done, the results pane under the visual query shows the number of line items for each sales order.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse and imported data into it. You\u2019ve seen how a lakehouse consists of files and tables stored in a OneLake data store. The managed tables can be queried using SQL, and are included in a default semantic model to support data visualizations.</p> <p>If you've finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>You can now close the incognito/private mode browser tab, and click Submit on the Microsoft Fabric Playground hands-on lab.</p>"},{"location":"labs/01-lakehouse/#source-httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs01-lakehousehtml","title":"Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/01-lakehouse.html","text":""},{"location":"labs/03b-medallion-lakehouse/","title":"Lab: Create Medallion Architecture in a Fabric Lakehouse","text":"<p>In this exercise you will build out a medallion architecture in a Fabric lakehouse using notebooks. You will create a workspace, create a lakehouse, upload data to the bronze layer, transform the data and load it to the silver Delta table, transform the data further and load it to the gold Delta tables, and then explore the semantic model and create relationships.</p> <p>This exercise should take approximately 45 minutes to complete</p> <p>You need access to a Microsoft Fabric tenant to complete this exercise.</p>"},{"location":"labs/03b-medallion-lakehouse/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page at https://app.fabric.microsoft.com/home?experience=fabric-developer in a browser and sign in with your Fabric credentials.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7). Create a new workspace with a name of your choice, selecting a licensing mode in the Advanced section that includes Fabric capacity (Trial, Premium, or Fabric).</p> </li> <li>When your new workspace opens, it should be empty.</li> </ol> <p></p> <ol> <li>Navigate to the workspace settings and verify that the Data model settings preview feature is enabled. This will enable you to create relationships between tables in your lakehouse using a Power BI semantic model.</li> </ol> <p></p> <p>You may need to refresh the browser tab after enabling the preview feature.</p>"},{"location":"labs/03b-medallion-lakehouse/#create-a-lakehouse-and-upload-data-to-bronze-layer","title":"Create a lakehouse and upload data to bronze layer","text":"<p>Now that you have a workspace, it\u2019s time to create a data lakehouse for the data you\u2019re going to analyze.</p> <ol> <li>In the workspace you just created, create a new Lakehouse named Sales by selecting the + New item button.</li> </ol> <p>After a minute or so, a new empty lakehouse will be created. Next, you\u2019ll ingest some data into the data lakehouse for analysis. There are multiple ways to do this, but in this exercise you\u2019ll simply download a text file to your local computer (or lab VM if applicable) and then upload it to your lakehouse.</p> <ol> <li> <p>Download the data file for this exercise from <code>https://github.com/MicrosoftLearning/dp-data/blob/main/orders.zip</code> Extract the files and save them with their original names on your local computer (or lab VM if applicable). There should be 3 files containing sales data for 3 years: 2019.csv, 2020.csv, and 2021.csv</p> </li> <li> <p>Return to the web browser tab containing your lakehouse, and in the <code>...</code> menu for the Files folder in the Explorer pane, select New subfolder and create a folder named bronze.</p> </li> <li> <p>In the <code>...</code> menu for the bronze folder, select Upload and Upload files, and then upload the 3 files (2019.csv, 2020.csv, and 2021.csv) from your local computer (or lab VM if applicable) to the lakehouse. Use the shift key to upload all 3 files at once.</p> </li> <li> <p>After the files have been uploaded, select the bronze folder; and verify that the files have been uploaded, as shown here:</p> </li> </ol> <p>Screenshot of uploaded products.csv file in a lakehouse.</p>"},{"location":"labs/03b-medallion-lakehouse/#transform-data-and-load-to-silver-delta-table","title":"Transform data and load to silver Delta table","text":"<p>Now that you have some data in the bronze layer of your lakehouse, you can use a notebook to transform the data and load it to a delta table in the silver layer.</p> <p>On the Home page while viewing the contents of the bronze folder in your data lake, in the Open notebook menu, select New notebook.</p> <p>After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).</p> <p>When the notebook opens, rename it to Transform data for Silver by selecting the Notebook xxxx text at the top left of the notebook and entering the new name.</p> <p>Screenshot of a new notebook named Transform data for silver.</p> <p>Select the existing cell in the notebook, which contains some simple commented-out code. Highlight and delete these two lines - you will not need this code.</p> <p>Note: Notebooks enable you to run code in a variety of languages, including Python, Scala, and SQL. In this exercise, you\u2019ll use PySpark and SQL. You can also add markdown cells to provide formatted text and images to document your code.</p> <p>Paste the following code into the cell:</p> <p>code from pyspark.sql.types import *</p>"},{"location":"labs/03b-medallion-lakehouse/#create-the-schema-for-the-table","title":"Create the schema for the table","text":"<p>orderSchema = StructType([     StructField(\"SalesOrderNumber\", StringType()),     StructField(\"SalesOrderLineNumber\", IntegerType()),     StructField(\"OrderDate\", DateType()),     StructField(\"CustomerName\", StringType()),     StructField(\"Email\", StringType()),     StructField(\"Item\", StringType()),     StructField(\"Quantity\", IntegerType()),     StructField(\"UnitPrice\", FloatType()),     StructField(\"Tax\", FloatType())     ])</p>"},{"location":"labs/03b-medallion-lakehouse/#import-all-files-from-bronze-folder-of-lakehouse","title":"Import all files from bronze folder of lakehouse","text":"<p>df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(orderSchema).load(\"Files/bronze/*.csv\")</p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(df.head(10)) Use the \u25b7 (Run cell) button on the left of the cell to run the code.</p> <p>Note: Since this is the first time you\u2019ve run any Spark code in this notebook, a Spark session must be started. This means that the first run can take a minute or so to complete. Subsequent runs will be quicker.</p> <p>When the cell command has completed, review the output below the cell, which should look similar to this:</p> <p>Index   SalesOrderNumber    SalesOrderLineNumber    OrderDate   CustomerName    Email   Item    Quantity    UnitPrice   Tax 1   SO49172 1   2021-01-01  Brian Howard    brian23@adventure-works.com Road-250 Red, 52    1   2443.35 195.468 2   SO49173 1   2021-01-01  Linda Alvarez   linda19@adventure-works.com Mountain-200 Silver, 38 1   2071.4197   165.7136 \u2026   \u2026   \u2026   \u2026   \u2026   \u2026   \u2026   \u2026   \u2026   \u2026 The code you ran loaded the data from the CSV files in the bronze folder into a Spark dataframe, and then displayed the first few rows of the dataframe.</p> <p>Note: You can clear, hide, and auto-resize the contents of the cell output by selecting the \u2026 menu at the top left of the output pane.</p> <p>Now you\u2019ll add columns for data validation and cleanup, using a PySpark dataframe to add columns and update the values of some of the existing columns. Use the + Code button to add a new code block and add the following code to the cell:</p> <p>code from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name</p>"},{"location":"labs/03b-medallion-lakehouse/#add-columns-isflagged-createdts-and-modifiedts","title":"Add columns IsFlagged, CreatedTS and ModifiedTS","text":"<p>df = df.withColumn(\"FileName\", input_file_name()) \\     .withColumn(\"IsFlagged\", when(col(\"OrderDate\") &lt; '2019-08-01',True).otherwise(False)) \\     .withColumn(\"CreatedTS\", current_timestamp()).withColumn(\"ModifiedTS\", current_timestamp())</p>"},{"location":"labs/03b-medallion-lakehouse/#update-customername-to-unknown-if-customername-null-or-empty","title":"Update CustomerName to \"Unknown\" if CustomerName null or empty","text":"<p>df = df.withColumn(\"CustomerName\", when((col(\"CustomerName\").isNull() | (col(\"CustomerName\")==\"\")),lit(\"Unknown\")).otherwise(col(\"CustomerName\"))) The first line of the code imports the necessary functions from PySpark. You\u2019re then adding new columns to the dataframe so you can track the source file name, whether the order was flagged as being a before the fiscal year of interest, and when the row was created and modified.</p> <p>Finally, you\u2019re updating the CustomerName column to \u201cUnknown\u201d if it\u2019s null or empty.</p> <p>Run the cell to execute the code using the \u25b7 (Run cell) button.</p> <p>Next, you\u2019ll define the schema for the sales_silver table in the sales database using Delta Lake format. Create a new code block and add the following code to the cell:</p> <p>code</p>"},{"location":"labs/03b-medallion-lakehouse/#define-the-schema-for-the-sales_silver-table","title":"Define the schema for the sales_silver table","text":"<p>from pyspark.sql.types import * from delta.tables import *</p> <p>DeltaTable.createIfNotExists(spark) \\     .tableName(\"sales.sales_silver\") \\     .addColumn(\"SalesOrderNumber\", StringType()) \\     .addColumn(\"SalesOrderLineNumber\", IntegerType()) \\     .addColumn(\"OrderDate\", DateType()) \\     .addColumn(\"CustomerName\", StringType()) \\     .addColumn(\"Email\", StringType()) \\     .addColumn(\"Item\", StringType()) \\     .addColumn(\"Quantity\", IntegerType()) \\     .addColumn(\"UnitPrice\", FloatType()) \\     .addColumn(\"Tax\", FloatType()) \\     .addColumn(\"FileName\", StringType()) \\     .addColumn(\"IsFlagged\", BooleanType()) \\     .addColumn(\"CreatedTS\", DateType()) \\     .addColumn(\"ModifiedTS\", DateType()) \\     .execute() Run the cell to execute the code using the \u25b7 (Run cell) button.</p> <p>Select the \u2026 in the Tables section of the Explorer pane and select Refresh. You should now see the new sales_silver table listed. The \u25b2 (triangle icon) indicates that it\u2019s a Delta table.</p> <p>Note: If you don\u2019t see the new table, wait a few seconds and then select Refresh again, or refresh the entire browser tab.</p> <p>Now you\u2019re going to perform an upsert operation on a Delta table, updating existing records based on specific conditions and inserting new records when no match is found. Add a new code block and paste the following code:</p> <p>code</p>"},{"location":"labs/03b-medallion-lakehouse/#update-existing-records-and-insert-new-ones-based-on-a-condition-defined-by-the-columns-salesordernumber-orderdate-customername-and-item","title":"Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.","text":"<p>from delta.tables import *</p> <p>deltaTable = DeltaTable.forPath(spark, 'Tables/sales_silver')</p> <p>dfUpdates = df</p> <p>deltaTable.alias('silver') \\   .merge(     dfUpdates.alias('updates'),     'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'   ) \\    .whenMatchedUpdate(set =     {</p> <pre><code>}\n</code></pre> <p>) \\  .whenNotMatchedInsert(values =     {       \"SalesOrderNumber\": \"updates.SalesOrderNumber\",       \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",       \"OrderDate\": \"updates.OrderDate\",       \"CustomerName\": \"updates.CustomerName\",       \"Email\": \"updates.Email\",       \"Item\": \"updates.Item\",       \"Quantity\": \"updates.Quantity\",       \"UnitPrice\": \"updates.UnitPrice\",       \"Tax\": \"updates.Tax\",       \"FileName\": \"updates.FileName\",       \"IsFlagged\": \"updates.IsFlagged\",       \"CreatedTS\": \"updates.CreatedTS\",       \"ModifiedTS\": \"updates.ModifiedTS\"     }   ) \\   .execute() Run the cell to execute the code using the \u25b7 (Run cell) button.</p> <p>This operation is important because it enables you to update existing records in the table based on the values of specific columns, and insert new records when no match is found. This is a common requirement when you\u2019re loading data from a source system that may contain updates to existing and new records.</p> <p>You now have data in your silver delta table that is ready for further transformation and modeling.</p> <p>Explore data in the silver layer using the SQL endpoint Now that you have data in your silver layer, you can use the SQL analytics endpoint to explore the data and perform some basic analysis. This is useful if you\u2019re familiar with SQL and want to do some basic exploration of your data. In this exercise we\u2019re using the SQL endpoint view in Fabric, but you can use other tools like SQL Server Management Studio (SSMS) and Azure Data Explorer.</p> <p>Navigate back to your workspace and notice that you now have several items listed. Select the Sales SQL analytics endpoint to open your lakehouse in the SQL analytics endpoint view.</p> <p>Screenshot of the SQL endpoint in a lakehouse.</p> <p>Select New SQL query from the ribbon, which will open a SQL query editor. Note that you can rename your query using the \u2026 menu item next to the existing query name in the Explorer pane.</p> <p>Next, you\u2019ll run two sql queries to explore the data.</p> <p>Paste the following query into the query editor and select Run:</p> <p>sql SELECT YEAR(OrderDate) AS Year     , CAST (SUM(Quantity * (UnitPrice + Tax)) AS DECIMAL(12, 2)) AS TotalSales FROM sales_silver GROUP BY YEAR(OrderDate)  ORDER BY YEAR(OrderDate) This query calculates the total sales for each year in the sales_silver table. Your results should look like this:</p> <p>Screenshot of the results of a SQL query in a lakehouse.</p> <p>Next you\u2019ll review which customers are purchasing the most (in terms of quantity). Paste the following query into the query editor and select Run:</p> <p>sql SELECT TOP 10 CustomerName, SUM(Quantity) AS TotalQuantity FROM sales_silver GROUP BY CustomerName ORDER BY TotalQuantity DESC This query calculates the total quantity of items purchased by each customer in the sales_silver table, and then returns the top 10 customers in terms of quantity.</p> <p>Data exploration at the silver layer is useful for basic analysis, but you\u2019ll need to transform the data further and model it into a star schema to enable more advanced analysis and reporting. You\u2019ll do that in the next section.</p> <p>Transform data for gold layer You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you\u2019ll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.</p> <p>You could have done all of this in a single notebook, but for this exercise you\u2019re using separate notebooks to demonstrate the process of transforming data from bronze to silver and then from silver to gold. This can help with debugging, troubleshooting, and reuse.</p> <p>Return to the workspace home page and create a new notebook called Transform data for Gold.</p> <p>In the Explorer pane, add your Sales lakehouse by selecting Add data items and then selecting the Sales lakehouse you created earlier. You should see the sales_silver table listed in the Tables section of the explorer pane.</p> <p>In the existing code block, remove the commented text and add the following code to load data to your dataframe and start building your star schema, then run it:</p> <p>code</p>"},{"location":"labs/03b-medallion-lakehouse/#load-data-to-the-dataframe-as-a-starting-point-to-create-the-gold-layer","title":"Load data to the dataframe as a starting point to create the gold layer","text":"<p>df = spark.read.table(\"Sales.sales_silver\") Add a new code block and paste the following code to create your date dimension table and run it:</p> <p>code from pyspark.sql.types import * from delta.tables import*</p>"},{"location":"labs/03b-medallion-lakehouse/#define-the-schema-for-the-dimdate_gold-table","title":"Define the schema for the dimdate_gold table","text":"<p>DeltaTable.createIfNotExists(spark) \\     .tableName(\"sales.dimdate_gold\") \\     .addColumn(\"OrderDate\", DateType()) \\     .addColumn(\"Day\", IntegerType()) \\     .addColumn(\"Month\", IntegerType()) \\     .addColumn(\"Year\", IntegerType()) \\     .addColumn(\"mmmyyyy\", StringType()) \\     .addColumn(\"yyyymm\", StringType()) \\     .execute() Note: You can run the display(df) command at any time to check the progress of your work. In this case, you\u2019d run \u2018display(dfdimDate_gold)\u2019 to see the contents of the dimDate_gold dataframe.</p> <p>In a new code block, add and run the following code to create a dataframe for your date dimension, dimdate_gold:</p> <p>code from pyspark.sql.functions import col, dayofmonth, month, year, date_format</p>"},{"location":"labs/03b-medallion-lakehouse/#create-dataframe-for-dimdate_gold","title":"Create dataframe for dimDate_gold","text":"<p>dfdimDate_gold = df.dropDuplicates([\"OrderDate\"]).select(col(\"OrderDate\"), \\         dayofmonth(\"OrderDate\").alias(\"Day\"), \\         month(\"OrderDate\").alias(\"Month\"), \\         year(\"OrderDate\").alias(\"Year\"), \\         date_format(col(\"OrderDate\"), \"MMM-yyyy\").alias(\"mmmyyyy\"), \\         date_format(col(\"OrderDate\"), \"yyyyMM\").alias(\"yyyymm\"), \\     ).orderBy(\"OrderDate\")</p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data_1","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(dfdimDate_gold.head(10)) You\u2019re separating the code out into new code blocks so that you can understand and watch what\u2019s happening in the notebook as you transform the data. In another new code block, add and run the following code to update the date dimension as new data comes in:</p> <p>code from delta.tables import *</p> <p>deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')</p> <p>dfUpdates = dfdimDate_gold</p> <p>deltaTable.alias('gold') \\   .merge(     dfUpdates.alias('updates'),     'gold.OrderDate = updates.OrderDate'   ) \\    .whenMatchedUpdate(set =     {</p> <pre><code>}\n</code></pre> <p>) \\  .whenNotMatchedInsert(values =     {       \"OrderDate\": \"updates.OrderDate\",       \"Day\": \"updates.Day\",       \"Month\": \"updates.Month\",       \"Year\": \"updates.Year\",       \"mmmyyyy\": \"updates.mmmyyyy\",       \"yyyymm\": \"updates.yyyymm\"     }   ) \\   .execute() The date dimension is now set up. Now you\u2019ll create your customer dimension.</p> <p>To build out the customer dimension table, add a new code block, paste and run the following code:</p> <p>code from pyspark.sql.types import * from delta.tables import *</p>"},{"location":"labs/03b-medallion-lakehouse/#create-customer_gold-dimension-delta-table","title":"Create customer_gold dimension delta table","text":"<p>DeltaTable.createIfNotExists(spark) \\     .tableName(\"sales.dimcustomer_gold\") \\     .addColumn(\"CustomerName\", StringType()) \\     .addColumn(\"Email\",  StringType()) \\     .addColumn(\"First\", StringType()) \\     .addColumn(\"Last\", StringType()) \\     .addColumn(\"CustomerID\", LongType()) \\     .execute() In a new code block, add and run the following code to drop duplicate customers, select specific columns, and split the \u201cCustomerName\u201d column to create \u201cFirst\u201d and \u201cLast\u201d name columns:</p> <p>code from pyspark.sql.functions import col, split</p>"},{"location":"labs/03b-medallion-lakehouse/#create-customer_silver-dataframe","title":"Create customer_silver dataframe","text":"<p>dfdimCustomer_silver = df.dropDuplicates([\"CustomerName\",\"Email\"]).select(col(\"CustomerName\"),col(\"Email\")) \\     .withColumn(\"First\",split(col(\"CustomerName\"), \" \").getItem(0)) \\     .withColumn(\"Last\",split(col(\"CustomerName\"), \" \").getItem(1)) </p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data_2","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(dfdimCustomer_silver.head(10)) Here you have created a new DataFrame dfdimCustomer_silver by performing various transformations such as dropping duplicates, selecting specific columns, and splitting the \u201cCustomerName\u201d column to create \u201cFirst\u201d and \u201cLast\u201d name columns. The result is a DataFrame with cleaned and structured customer data, including separate \u201cFirst\u201d and \u201cLast\u201d name columns extracted from the \u201cCustomerName\u201d column.</p> <p>Next we\u2019ll create the ID column for our customers. In a new code block, paste and run the following:</p> <p>code from pyspark.sql.functions import monotonically_increasing_id, col, when, coalesce, max, lit</p> <p>dfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")</p> <p>MAXCustomerID = dfdimCustomer_temp.select(coalesce(max(col(\"CustomerID\")),lit(0)).alias(\"MAXCustomerID\")).first()[0]</p> <p>dfdimCustomer_gold = dfdimCustomer_silver.join(dfdimCustomer_temp,(dfdimCustomer_silver.CustomerName == dfdimCustomer_temp.CustomerName) &amp; (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email), \"left_anti\")</p> <p>dfdimCustomer_gold = dfdimCustomer_gold.withColumn(\"CustomerID\",monotonically_increasing_id() + MAXCustomerID + 1)</p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data_3","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(dfdimCustomer_gold.head(10)) Here you\u2019re cleaning and transforming customer data (dfdimCustomer_silver) by performing a left anti join to exclude duplicates that already exist in the dimCustomer_gold table, and then generating unique CustomerID values using the monotonically_increasing_id() function.</p> <p>Now you\u2019ll ensure that your customer table remains up-to-date as new data comes in. In a new code block, paste and run the following:</p> <p>code from delta.tables import *</p> <p>deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')</p> <p>dfUpdates = dfdimCustomer_gold</p> <p>deltaTable.alias('gold') \\   .merge(     dfUpdates.alias('updates'),     'gold.CustomerName = updates.CustomerName AND gold.Email = updates.Email'   ) \\    .whenMatchedUpdate(set =     {</p> <pre><code>}\n</code></pre> <p>) \\  .whenNotMatchedInsert(values =     {       \"CustomerName\": \"updates.CustomerName\",       \"Email\": \"updates.Email\",       \"First\": \"updates.First\",       \"Last\": \"updates.Last\",       \"CustomerID\": \"updates.CustomerID\"     }   ) \\   .execute() Now you\u2019ll repeat those steps to create your product dimension. In a new code block, paste and run the following:</p> <p>code from pyspark.sql.types import * from delta.tables import *</p> <p>DeltaTable.createIfNotExists(spark) \\     .tableName(\"sales.dimproduct_gold\") \\     .addColumn(\"ItemName\", StringType()) \\     .addColumn(\"ItemID\", LongType()) \\     .addColumn(\"ItemInfo\", StringType()) \\     .execute() Add another code block to create the product_silver dataframe.</p> <p>code from pyspark.sql.functions import col, split, lit, when</p>"},{"location":"labs/03b-medallion-lakehouse/#create-product_silver-dataframe","title":"Create product_silver dataframe","text":"<p>dfdimProduct_silver = df.dropDuplicates([\"Item\"]).select(col(\"Item\")) \\     .withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\     .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) </p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data_4","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(dfdimProduct_silver.head(10)) Now you\u2019ll create IDs for your dimProduct_gold table. Add the following syntax to a new code block and run it:</p> <p>code from pyspark.sql.functions import monotonically_increasing_id, col, lit, max, coalesce</p>"},{"location":"labs/03b-medallion-lakehouse/#dfdimproduct_temp-dfdimproduct_silver","title":"dfdimProduct_temp = dfdimProduct_silver","text":"<p>dfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")</p> <p>MAXProductID = dfdimProduct_temp.select(coalesce(max(col(\"ItemID\")),lit(0)).alias(\"MAXItemID\")).first()[0]</p> <p>dfdimProduct_gold = dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName == dfdimProduct_temp.ItemName) &amp; (dfdimProduct_silver.ItemInfo == dfdimProduct_temp.ItemInfo), \"left_anti\")</p> <p>dfdimProduct_gold = dfdimProduct_gold.withColumn(\"ItemID\",monotonically_increasing_id() + MAXProductID + 1)</p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data_5","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(dfdimProduct_gold.head(10)) This calculates the next available product ID based on the current data in the table, assigns these new IDs to the products, and then displays the updated product information.</p> <p>Similar to what you\u2019ve done with your other dimensions, you need to ensure that your product table remains up-to-date as new data comes in. In a new code block, paste and run the following:</p> <p>code from delta.tables import *</p> <p>deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')</p> <p>dfUpdates = dfdimProduct_gold</p> <p>deltaTable.alias('gold') \\   .merge(         dfUpdates.alias('updates'),         'gold.ItemName = updates.ItemName AND gold.ItemInfo = updates.ItemInfo'         ) \\         .whenMatchedUpdate(set =         {</p> <pre><code>    }\n    ) \\\n    .whenNotMatchedInsert(values =\n     {\n      \"ItemName\": \"updates.ItemName\",\n      \"ItemInfo\": \"updates.ItemInfo\",\n      \"ItemID\": \"updates.ItemID\"\n      }\n      ) \\\n      .execute()\n</code></pre> <p>Now that you have your dimensions built out, the final step is to create the fact table.</p> <p>In a new code block, paste and run the following code to create the fact table:</p> <p>code from pyspark.sql.types import * from delta.tables import *</p> <p>DeltaTable.createIfNotExists(spark) \\     .tableName(\"sales.factsales_gold\") \\     .addColumn(\"CustomerID\", LongType()) \\     .addColumn(\"ItemID\", LongType()) \\     .addColumn(\"OrderDate\", DateType()) \\     .addColumn(\"Quantity\", IntegerType()) \\     .addColumn(\"UnitPrice\", FloatType()) \\     .addColumn(\"Tax\", FloatType()) \\     .execute() In a new code block, paste and run the following code to create a new dataframe to combine sales data with customer and product information include customer ID, item ID, order date, quantity, unit price, and tax:</p> <p>code from pyspark.sql.functions import col</p> <p>dfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\") dfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")</p> <p>df = df.withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\     .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \\</p>"},{"location":"labs/03b-medallion-lakehouse/#create-sales_gold-dataframe","title":"Create Sales_gold dataframe","text":"<p>dffactSales_gold = df.alias(\"df1\").join(dfdimCustomer_temp.alias(\"df2\"),(df.CustomerName == dfdimCustomer_temp.CustomerName) &amp; (df.Email == dfdimCustomer_temp.Email), \"left\") \\         .join(dfdimProduct_temp.alias(\"df3\"),(df.ItemName == dfdimProduct_temp.ItemName) &amp; (df.ItemInfo == dfdimProduct_temp.ItemInfo), \"left\") \\     .select(col(\"df2.CustomerID\") \\         , col(\"df3.ItemID\") \\         , col(\"df1.OrderDate\") \\         , col(\"df1.Quantity\") \\         , col(\"df1.UnitPrice\") \\         , col(\"df1.Tax\") \\     ).orderBy(col(\"df1.OrderDate\"), col(\"df2.CustomerID\"), col(\"df3.ItemID\"))</p>"},{"location":"labs/03b-medallion-lakehouse/#display-the-first-10-rows-of-the-dataframe-to-preview-your-data_6","title":"Display the first 10 rows of the dataframe to preview your data","text":"<p>display(dffactSales_gold.head(10)) Now you\u2019ll ensure that sales data remains up-to-date by running the following code in a new code block:</p> <p>code from delta.tables import *</p> <p>deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')</p> <p>dfUpdates = dffactSales_gold</p> <p>deltaTable.alias('gold') \\   .merge(     dfUpdates.alias('updates'),     'gold.OrderDate = updates.OrderDate AND gold.CustomerID = updates.CustomerID AND gold.ItemID = updates.ItemID'   ) \\    .whenMatchedUpdate(set =     {</p> <pre><code>}\n</code></pre> <p>) \\  .whenNotMatchedInsert(values =     {       \"CustomerID\": \"updates.CustomerID\",       \"ItemID\": \"updates.ItemID\",       \"OrderDate\": \"updates.OrderDate\",       \"Quantity\": \"updates.Quantity\",       \"UnitPrice\": \"updates.UnitPrice\",       \"Tax\": \"updates.Tax\"     }   ) \\   .execute() Here you\u2019re using Delta Lake\u2019s merge operation to synchronize and update the factsales_gold table with new sales data (dffactSales_gold). The operation compares the order date, customer ID, and item ID between the existing data (silver table) and the new data (updates DataFrame), updating matching records and inserting new records as needed.</p> <p>You now have a curated, modeled gold layer that can be used for reporting and analysis.</p> <p>Create a semantic model In your workspace, you can now use the gold layer to create a report and analyze the data. You can access the semantic model directly in your workspace to create relationships and measures for reporting.</p> <p>Note that you can\u2019t use the default semantic model that is automatically created when you create a lakehouse. You must create a new semantic model that includes the gold tables you created in this exercise, from the Explorer.</p> <p>In your workspace, navigate to your Sales lakehouse. Select New semantic model from the ribbon of the Explorer view. Assign the name Sales_Gold to your new semantic model. Select your transformed gold tables to include in your semantic model and select Confirm. dimdate_gold dimcustomer_gold dimproduct_gold factsales_gold This will open the semantic model in Fabric where you can create relationships and measures, as shown here:</p> <p>Screenshot of a semantic model in Fabric.</p> <p>From here, you or other members of your data team can create reports and dashboards based on the data in your lakehouse. These reports will be connected directly to the gold layer of your lakehouse, so they\u2019ll always reflect the latest data.</p> <p>Clean up resources In this exercise, you\u2019ve learned how to create a medallion architecture in a Microsoft Fabric lakehouse.</p> <p>If you\u2019ve finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains. In the \u2026 menu on the toolbar, select Workspace settings. In the General section, select Remove this workspace.</p>"},{"location":"labs/03b-medallion-lakehouse/#httpsdocsmicrosoftcomlearnmodulescreate-medallion-architecture-fabric-lakehouse","title":"https://docs.microsoft.com/learn/modules/create-medallion-architecture-fabric-lakehouse/","text":""},{"location":"labs/03b-medallion-lakehouse/#httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs03b-medallion-lakehousehtml","title":"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03b-medallion-lakehouse.html","text":""},{"location":"labs/04-ingest-pipeline/","title":"Lab: Ingest Data with a Pipeline in Microsoft Fabric","text":"<p>A data lakehouse is a common analytical data store for cloud-scale analytics solutions. One of the core tasks of a data engineer is to implement and manage the ingestion of data from multiple operational data sources into the lakehouse. In Microsoft Fabric, you can implement extract, transform, and load (ETL) or extract, load, and transform (ELT) solutions for data ingestion through the creation of pipelines.</p> <p>Fabric also supports Apache Spark, enabling you to write and run code to process data at scale. By combining the pipeline and Spark capabilities in Fabric, you can implement complex data ingestion logic that copies data from external sources into the OneLake storage on which the lakehouse is based, and then uses Spark code to perform custom data transformations before loading it into tables for analysis.</p> <p>This lab will take approximately 45 minutes to complete.</p> <p>You need access to a Microsoft Fabric tenant to complete this exercise.</p>"},{"location":"labs/04-ingest-pipeline/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in a browser and sign in with your Fabric credentials.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a new workspace with a name of your choice, selecting a licensing mode in the Advanced section that includes Fabric capacity (Trial, Premium, or Fabric).</p> </li> <li> <p>When your new workspace opens, it should be empty.</p> </li> </ol> <p></p>"},{"location":"labs/04-ingest-pipeline/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it\u2019s time to create a data lakehouse into which you will ingest data.</p> <ol> <li>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse. Give it a unique name of your choice.</li> </ol> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new lakehouse with no Tables or Files will be created.</p> <ol> <li>On the Explorer pane on the left, in the <code>...</code> menu for the Files node, select New subfolder and create a subfolder named <code>new_data</code></li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-pipeline","title":"Create a pipeline","text":"<p>A simple way to ingest data is to use a Copy Data activity in a pipeline to extract the data from a source and copy it to a file in the lakehouse.</p> <ol> <li> <p>On the Home page for your lakehouse, select Get data and then select New data pipeline, and create a new data pipeline named <code>Ingest Sales Data</code></p> </li> <li> <p>If the Copy Data wizard doesn\u2019t open automatically, select Copy Data &gt; Use copy assistant in the pipeline editor page.</p> </li> <li> <p>In the Copy Data wizard, on the Choose data source page, type HTTP in the search bar and then select HTTP in the New sources section.</p> </li> </ol> <p></p> <ol> <li> <p>In the Connect to data source pane, enter the following settings for the connection to your data source:</p> <ul> <li>URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv</li> <li>Connection: Create new connection</li> <li>Connection name: Specify a unique name</li> <li>Data gateway: (none)</li> <li>Authentication kind: Anonymous</li> </ul> </li> <li> <p>Select Next. Then ensure the following settings are selected:</p> <ul> <li>Relative URL: Leave blank</li> <li>Request method: GET</li> <li>Additional headers: Leave blank</li> <li>Binary copy: Unselected</li> <li>Request timeout: Leave blank</li> <li>Max concurrent connections: Leave blank</li> </ul> </li> <li> <p>Select Next, and wait for the data to be sampled and then ensure that the following settings are selected:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>First row as header: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>Select Preview data to see a sample of the data that will be ingested. Then close the data preview and select Next.</p> </li> <li> <p>On the Connect to data destination page, set the following data destination options, and then select Next:</p> <ul> <li>Root folder: Files</li> <li>Folder path name: new_data</li> <li>File name: sales.csv</li> <li>Copy behavior: None</li> </ul> </li> <li> <p>Set the following file format options and then select Next:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>Add header to file: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>On the Copy summary page, review the details of your copy operation and then select Save + Run.</p> </li> </ol> <p>A new pipeline containing a Copy Data activity is created, as shown here:</p> <p>![Screenshot of a pipeline with a Copy Data activity.]</p> <ol> <li> <p>When the pipeline starts to run, you can monitor its status in the Output pane under the pipeline designer. Use the \u21bb (Refresh) icon to refresh the status, and wait until it has succeeeded.</p> </li> <li> <p>In the menu bar on the left, select your lakehouse.</p> </li> <li> <p>On the Home page, in the Explorer pane, expand Files and select the new_data folder to verify that the sales.csv file has been copied.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-notebook","title":"Create a notebook","text":"<ol> <li>On the Home page for your lakehouse, in the Open notebook menu, select New notebook.</li> </ol> <p>After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).</p> <ol> <li>Select the existing cell in the notebook, which contains some simple code, and then replace the default code with the following variable declaration.</li> </ol> <pre><code>table_name = \"sales\"\n</code></pre> <ol> <li> <p>In the <code>...</code> menu for the cell (at its top-right) select Toggle parameter cell. This configures the cell so that the variables declared in it are treated as parameters when running the notebook from a pipeline.</p> </li> <li> <p>Under the parameters cell, use the + Code button to add a new code cell. Then add the following code to it:</p> </li> </ol> <pre><code>from pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n</code></pre> <p>This code loads the data from the sales.csv file that was ingested by the Copy Data activity, applies some transformation logic, and saves the transformed data as a table - appending the data if the table already exists.</p> <ol> <li>Verify that your notebooks looks similar to this, and then use the \u25b6 Run all button on the toolbar to run all of the cells it contains.</li> </ol> <p>![Screenshot of a notebook with a parameters cell and code to transform data.]</p> <p>Note</p> <ul> <li>Since this is the first time you\u2019ve run any Spark code in this session, the Spark pool must be started.</li> <li>This means that the first cell can take a minute or so to complete.</li> </ul> <ol> <li> <p>When the notebook run has completed, in the Explorer pane on the left, in the <code>...</code> menu for Tables select Refresh and verify that a sales table has been created.</p> </li> <li> <p>In the notebook menu bar, use the \u2699\ufe0f Settings icon to view the notebook settings. Then set the Name of the notebook to <code>Load Sales</code> and close the settings pane.</p> </li> <li> <p>In the hub menu bar on the left, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, refresh the view. Then expand Tables, and select the sales table to see a preview of the data it contains.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#modify-the-pipeline","title":"Modify the pipeline","text":"<p>Now that you\u2019ve implemented a notebook to transform data and load it into a table, you can incorporate the notebook into a pipeline to create a reusable ETL process.</p> <ol> <li> <p>In the hub menu bar on the left select the Ingest Sales Data pipeline you created previously.</p> </li> <li> <p>On the Activities tab, in the All activities list, select Delete data. Then position the new Delete data activity to the left of the Copy data activity and connect its On completion output to the Copy data activity, as shown here:</p> </li> </ol> <p>![Screenshot of a pipeline with Delete data and Copy data activities.]</p> <ol> <li> <p>Select the Delete data activity, and in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Delete old files</code></li> </ul> </li> <li> <p>Source:</p> <ul> <li>Connection: Your lakehouse</li> <li>File path type: Wildcard file path</li> <li>Folder path: Files / new_data</li> <li>Wildcard file name: <code>*.csv</code></li> <li>Recursively: Selected</li> </ul> </li> <li> <p>Logging settings:</p> <ul> <li>Enable logging: Unselected</li> </ul> </li> </ul> </li> </ol> <p>These settings will ensure that any existing .csv files are deleted before copying the sales.csv file.</p> <ol> <li> <p>In the pipeline designer, on the Activities tab, select Notebook to add a Notebook activity to the pipeline.</p> </li> <li> <p>Select the Copy data activity and then connect its On Completion output to the Notebook activity as shown here:</p> </li> </ol> <p>![Screenshot of a pipeline with Copy Data and Notebook activities.]</p> <ol> <li> <p>Select the Notebook activity, and then in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Load Sales notebook</code></li> </ul> </li> <li> <p>Settings:</p> <ul> <li>Notebook: Load Sales</li> <li>Base parameters: Add a new parameter with the following properties:</li> </ul> Name Type Value table_name String new_sales </li> </ul> </li> </ol> <p>The table_name parameter will be passed to the notebook and override the default value assigned to the table_name variable in the parameters cell.</p> <ol> <li>On the Home tab, use the \ud83d\uddab (Save) icon to save the pipeline. Then use the \u25b6 Run button to run the pipeline, and wait for all of the activities to complete.</li> </ol> <p>![Screenshot of a pipeline with a Dataflow activity.]</p> <p>If you see an error message</p> <ul> <li>In case you receive the error message Spark SQL queries are only possible in the context of a lakehouse.</li> <li>Please attach a lakehouse to proceed:</li> <li>Open your notebook, select the lakehouse you created on the left pane,</li> <li>select Remove all Lakehouses and then add it again.</li> <li>Go back to the pipeline designer and select \u25b6 Run.</li> </ul> <ol> <li> <p>In the hub menu bar on the left edge of the portal, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, expand Tables and select the new_sales table to see a preview of the data it contains. This table was created by the notebook when it was run by the pipeline.</p> </li> </ol> <p>In this exercise, you implemented a data ingestion solution that uses a pipeline to copy data to your lakehouse from an external source, and then uses a Spark notebook to transform the data and load it into a table.</p>"},{"location":"labs/04-ingest-pipeline/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you\u2019ve learned how to implement a pipeline in Microsoft Fabric.</p> <p>If you\u2019ve finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs04-ingest-pipelinehtml","title":"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/04-ingest-pipeline.html","text":""},{"location":"labs/05-dataflows-gen2/","title":"Lab: Create and use Dataflows (Gen2) in Microsoft Fabric","text":"<p>In Microsoft Fabric, Dataflows (Gen2) connect to various data sources and perform transformations in Power Query Online. They can then be used in Data Pipelines to ingest data into a lakehouse or other analytical store, or to define a dataset for a Power BI report.</p> <p>This lab is designed to introduce the different elements of Dataflows (Gen2), and not create a complex solution that may exist in an enterprise. This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> </ol> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p>"},{"location":"labs/05-dataflows-gen2/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> </ol> <p>When your new workspace opens, it should be empty.</p> <p></p>"},{"location":"labs/05-dataflows-gen2/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-dataflow-gen2-to-ingest-data","title":"Create a Dataflow (Gen2) to ingest data","text":"<p>Now that you have a lakehouse, you need to ingest some data into it. One way to do this is to define a dataflow that encapsulates an extract, transform, and load (ETL) process.</p> <ol> <li> <p>In the home page for your lakehouse, select Get data &gt; New Dataflow Gen2</p> <p></p> <p>Click Create, and after a few seconds, the Power Query editor for your new dataflow opens as shown here:</p> <p></p> </li> <li> <p>Select Import from a Text/CSV file, and create a new data source with the following settings:</p> <ul> <li>Link to file: Selected</li> <li>File path or URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv</li> <li>Connection: Create new connection</li> <li>Connection Name: default value ~ or orders.csv if name already exists</li> <li>data gateway: (none)</li> <li>Authentication kind: Anonymous</li> <li>Privacy Level: None</li> </ul> </li> <li> <p>Select Next to preview the file data, and then Create the data source.</p> <p>The Power Query editor shows the data source and an initial set of query steps to format the data, as shown here:</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Add column tab. Then select Custom column and create a new column.</p> <p></p> </li> <li> <p>Set the New column name to <code>MonthNo</code>, set the Data type to Whole Number and then add the following formula: <code>Date.Month([OrderDate])</code> - as shown here:</p> <p></p> </li> <li> <p>Select OK to create the column. Notice how the step to add the custom column is added to the query.</p> <p>The resulting column is displayed in the data pane:</p> <p></p> <p>Info</p> <ul> <li>In the Query Settings pane on the right side, notice the Applied Steps include each transformation step.</li> <li>At the bottom, you can also toggle the Diagram view button to turn on the Visual Diagram of the steps.</li> </ul> <p>Info</p> <p>Steps can be moved up or down, edited by selecting the gear icon, and you can select each step to see the transformations apply in the preview pane.</p> </li> <li> <p>Check and confirm that the data type for the OrderDate column is set to Date and the data type for the newly created column MonthNo is set to Whole Number.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#add-data-destination-for-dataflow","title":"Add data destination for Dataflow","text":"<ol> <li> <p>On the toolbar ribbon, select the Home tab. Then in the Add data destination drop-down menu, select Lakehouse.</p> <p>Note</p> <ul> <li>If this option is grayed out, you may already have a data destination set.</li> <li>Check the data destination at the bottom of the Query settings pane on the right side of the Power Query editor.</li> <li>If a default destination is already set, you can remove it and add a new one.</li> </ul> </li> <li> <p>In the Connect to data destination dialog box, use the existing connection credentials:</p> <p></p> </li> <li> <p>Select Next and in the list of available workspaces, find your workspace and select the lakehouse you created in it at the start of this exercise. Then specify a new table named orders:</p> <p></p> </li> <li> <p>Select Next and on the Choose destination settings page:</p> <ul> <li>Disable the Use automatic settings option, select Append, and then Save settings.</li> </ul> <p></p> </li> <li> <p>On the Menu bar, open View and select Diagram view. Notice the Lakehouse destination is indicated as an icon in the query in the Power Query editor.</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Home tab. Then select Save &amp; run and wait for the Dataflow 1 dataflow to be created in your workspace.</p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#add-a-dataflow-to-a-pipeline","title":"Add a dataflow to a pipeline","text":"<p>You can include a dataflow as an activity in a pipeline. Pipelines are used to orchestrate data ingestion and processing activities, enabling you to combine dataflows with other kinds of operation in a single, scheduled process. Pipelines can be created in a few different experiences, including Data Factory experience.</p> <ol> <li> <p>From your Fabric-enabled workspace, select + New item &gt; Data pipeline</p> <ul> <li>When prompted, create a new pipeline named: Load data</li> </ul> <p>Click Create, and the pipeline editor will open:</p> <p></p> <p>If the Copy Data wizard opens automatically, you can just close it.</p> </li> <li> <p>Select Pipeline activity, and add a Dataflow activity to the pipeline.</p> </li> <li> <p>With the new Dataflow1 activity selected, on the Settings tab, in the Dataflow drop-down list, select Dataflow 1 (the data flow you created previously)</p> <p></p> </li> <li> <p>On the Home tab, save the pipeline using the  (Save) icon.</p> </li> <li> <p>Use the  Run button to run the pipeline, and wait for it to complete. It may take a few minutes.</p> <p></p> </li> <li> <p>In the menu bar on the left edge, select your lakehouse.</p> </li> <li> <p>In the <code>...</code> menu for Tables, select refresh. </p> <p>Then expand Tables and select the orders table, which has been created by your dataflow.</p> <p></p> </li> </ol> Tip for Power Bi Desktop users: <ul> <li>In Power BI Desktop, you can connect directly to the data transformations done with your dataflow by using the Power BI dataflows (Legacy) connector.</li> <li>You can also make additional transformations, publish as a new dataset, and distribute with intended audience for specialized datasets.</li> </ul> <p></p>"},{"location":"labs/05-dataflows-gen2/#clean-up-resources","title":"Clean up resources","text":"<p>If you've finished exploring dataflows in Microsoft Fabric, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>You can now close the incognito/private mode browser tab, and click Submit on the Microsoft Fabric Playground hands-on lab.</p>"},{"location":"labs/05-dataflows-gen2/#source-httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs05-dataflows-gen2html","title":"Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/05-dataflows-gen2.html","text":""},{"location":"labs/06c-monitor-data-warehouse/","title":"Lab: Monitor a data warehouse in Microsoft Fabric","text":"<p>In Microsoft Fabric, a data warehouse provides a relational database for large-scale analytics. Data warehouses in Microsoft Fabric include dynamic management views that you can use to monitor activity and queries.</p> <p>This lab will take approximately 30 minutes to complete.</p> <p>You need a Microsoft Fabric trial to complete this exercise.</p>"},{"location":"labs/06c-monitor-data-warehouse/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page at https://app.fabric.microsoft.com/home?experience=fabric in a browser, and sign in with your Fabric credentials.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a new workspace with a name of your choice, selecting a licensing mode that includes Fabric capacity (Trial, Premium, or Fabric).</p> </li> </ol> <p>When your new workspace opens, it should be empty.</p> <p>![Screenshot of an empty workspace in Fabric.]</p>"},{"location":"labs/06c-monitor-data-warehouse/#create-a-sample-data-warehouse","title":"Create a sample data warehouse","text":"<p>Now that you have a workspace, it\u2019s time to create a data warehouse.</p> <ol> <li>On the menu bar on the left, select Create. In the New page, under the Data Warehouse section, select Sample warehouse and create a new data warehouse named <code>sample-dw</code></li> </ol> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new warehouse will be created and populated with sample data for a taxi ride analysis scenario.</p> <p>![Screenshot of a new warehouse.]</p>"},{"location":"labs/06c-monitor-data-warehouse/#explore-dynamic-management-views","title":"Explore dynamic management views","text":"<p>Microsoft Fabric data warehouses include dynamic management views (DMVs), which you can use to identify current activity in the data warehouse instance.</p> <ol> <li> <p>In the sample-dw data warehouse page, in the New SQL query drop-down list, select New SQL query.</p> </li> <li> <p>In the new blank query pane, enter the following Transact-SQL code to query the <code>sys.dm_exec_connections</code> DMV:</p> </li> </ol> <pre><code>SELECT * FROM sys.dm_exec_connections;\n</code></pre> <ol> <li> <p>Use the \u25b7 Run button to run the SQL script and view the results, which include details of all connections to the data warehouse.</p> </li> <li> <p>Modify the SQL code to query the <code>sys.dm_exec_sessions</code> DMV, like this:</p> </li> </ol> <pre><code>SELECT * FROM sys.dm_exec_sessions;\n</code></pre> <ol> <li> <p>Run the modified query and view the results, which show details of all authenticated sessions.</p> </li> <li> <p>Modify the SQL code to query the sys.dm_exec_requests DMV, like this:</p> </li> </ol> <pre><code>SELECT * FROM sys.dm_exec_requests;\n</code></pre> <ol> <li> <p>Run the modified query and view the results, which show details of all requests being executed in the data warehouse.</p> </li> <li> <p>Modify the SQL code to join the DMVs and return information about currently running requests in the same database, like this:</p> </li> </ol> <p><pre><code>SELECT connections.connection_id,\n sessions.session_id, sessions.login_name, sessions.login_time,\n requests.command, requests.start_time, requests.total_elapsed_time\nFROM sys.dm_exec_connections AS connections\nINNER JOIN sys.dm_exec_sessions AS sessions\n    ON connections.session_id=sessions.session_id\nINNER JOIN sys.dm_exec_requests AS requests\n    ON requests.session_id = sessions.session_id\nWHERE requests.status = 'running'\n    AND requests.database_id = DB_ID()\nORDER BY requests.total_elapsed_time DESC;\n</code></pre> 9. Run the modified query and view the results, which show details of all running queries in the database (including this one).</p> <ol> <li>In the New SQL query drop-down list, select New SQL query to add a second query tab. Then in the new empty query tab, run the following code:</li> </ol> <pre><code>WHILE 1 = 1\n    SELECT * FROM Trip;\n</code></pre> <ol> <li> <p>Leave the query running, and return to the tab containing the code to query the DMVs and re-run it. This time, the results should include the second query that is running in the other tab. Note the elapsed time for that query.</p> </li> <li> <p>Wait a few seconds and re-run the code to query the DMVs again. The elapsed time for the query in the other tab should have increased.</p> </li> <li> <p>Return to the second query tab where the query is still running and select Cancel to cancel it.</p> </li> <li> <p>Back on the tab with the code to query the DMVs, re-run the query to confirm that the second query is no longer running.</p> </li> <li> <p>Close all query tabs.</p> </li> </ol> <p>Further Information</p> <p>See Monitor connections, sessions, and requests using DMVs in the Microsoft Fabric documentation for more information about using DMVs.</p>"},{"location":"labs/06c-monitor-data-warehouse/#explore-query-insights","title":"Explore query insights","text":"<p>Microsoft Fabric data warehouses provide query insights - a special set of views that provide details about the queries being run in your data warehouse.</p> <ol> <li> <p>In the sample-dw data warehouse page, in the New SQL query drop-down list, select New SQL query.</p> </li> <li> <p>In the new blank query pane, enter the following Transact-SQL code to query the exec_requests_history view:</p> </li> </ol> <pre><code>SELECT * FROM queryinsights.exec_requests_history;\n</code></pre> <ol> <li> <p>Use the \u25b7 Run button to run the SQL script and view the results, which include details of previously executed queries.</p> </li> <li> <p>Modify the SQL code to query the frequently_run_queries view, like this:</p> </li> </ol> <pre><code>SELECT * FROM queryinsights.frequently_run_queries;\n</code></pre> <ol> <li> <p>Run the modified query and view the results, which show details of frequently run queries.</p> </li> <li> <p>Modify the SQL code to query the long_running_queries view, like this:</p> </li> </ol> <pre><code>SELECT * FROM queryinsights.long_running_queries;\n</code></pre> <ol> <li>Run the modified query and view the results, which show details of all queries and their durations.</li> </ol> <p>Further Information</p> <p>See Query insights in Fabric data warehousing in the Microsoft Fabric documentation for more information about using query insights.</p>"},{"location":"labs/06c-monitor-data-warehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have used dynamic management views and query insights to monitor activity in a Microsoft Fabric data warehouse.</p> <p>If you\u2019ve finished exploring your data warehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol>"},{"location":"labs/06c-monitor-data-warehouse/#httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs06c-monitor-data-warehousehtml","title":"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/06c-monitor-data-warehouse.html","text":""},{"location":"labs/11-data-activator/","title":"Lab: Use Data Activator in Fabric","text":"<p>Data Activator in Microsoft Fabric takes action based on what\u2019s happening in your data. Data Activator lets you monitor your data and create triggers to react to your data changes.</p> <p>https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-tutorial</p> <p>IMPORTANT!</p> <p>This exercise is deprecated, and will be removed or updated soon. The instructions are no longer accurate, and the exercise is unsupported.</p> <p>https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/11-data-activator.html</p>"},{"location":"labs/18-monitor-hub/","title":"Lab: Monitor Fabric Activity in the Monitoring Hub","text":"<p>Monitor Fabric activity in the monitoring hub The monitoring hub in Microsoft Fabric provides a central place where you can monitor activity. You can use the monitoring hub to review events related to items you have permission to view.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>Note: You need access to a Microsoft Fabric tenant to complete this exercise.</p> <p>Create a workspace Before working with data in Fabric, create a workspace in a tenant with the Fabric capacity enabled.</p> <p>Navigate to the Microsoft Fabric home page at https://app.fabric.microsoft.com/home?experience=fabric-developer in a browser and sign in with your Fabric credentials. In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7). Create a new workspace with a name of your choice, selecting a licensing mode in the Advanced section that includes Fabric capacity (Trial, Premium, or Fabric). When your new workspace opens, it should be empty.</p> <p>Screenshot of an empty workspace in Fabric.</p> <p>Create a lakehouse Now that you have a workspace, it\u2019s time to create a data lakehouse for your data.</p> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse. Give it a unique name of your choice.</p> <p>Note: If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new lakehouse will be created:</p> <p>Screenshot of a new lakehouse.</p> <p>View the new lakehouse, and note that the Lakehouse explorer pane on the left enables you to browse tables and files in the lakehouse:</p> <p>Currently, there are no tables or files in the lakehouse.</p> <p>Create and monitor a Dataflow In Microsoft Fabric, you can use a Dataflow (Gen2) to ingest data from a wide range of sources. In this exercise, you\u2019ll use a dataflow to get data from a CSV file and load it into a table in your lakehouse.</p> <p>On the Home page for your lakehouse, in the Get data menu, select New Dataflow Gen2. Name the new dataflow Get Product Data and select Create.</p> <p>Screenshot of a new dataflow.</p> <p>In the dataflow designer, select Import from a Text/CSV file. Then complete the Get Data wizard to create a data connection by linking to https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/products.csv using anonymous authentication. When you have completed the wizard, a preview of the data will be shown in the dataflow designer like this:</p> <p>Screenshot of a dataflow query.</p> <p>Publish the dataflow. In the navigation bar on the left, select Monitor to view the monitoring hub and observe that your dataflow is in-progress (if not, refresh the view until you see it).</p> <p>Screenshot of the monitoring hub with a dataflow in-progress.</p> <p>Wait for a few seconds, and then refresh the page until the status of the dataflow is Succeeded. In the navigation pane, select your lakehouse. Then expand the Tables folder to verify that a table named products has been created and loaded by the dataflow (you may need to refresh the Tables folder).</p> <p>Screenshot of the products table in the lakehouse page.</p> <p>Create and monitor a Spark notebook In Microsoft Fabric, you can use notebooks to run Spark code.</p> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Notebook.</p> <p>A new notebook named Notebook 1 is created and opened.</p> <p>Screenshot of a new notebook.</p> <p>At the top left of the notebook, select Notebook 1 to view its details, and change its name to Query Products. In the notebook editor, in the Explorer pane, select Add data items and then select Existing data sources. Add the lakehouse you created previously. Expand the lakehouse item until you reach the products table. In the \u2026 menu for the products table, select Load data &gt; Spark. This adds a new code cell to the notebook as shown here:</p> <p>Screenshot of a notebook with code to query a table.</p> <p>Use the \u25b7 Run all button to run all cells in the notebook. It will take a moment or so to start the Spark session, and then the results of the query will be shown under the code cell.</p> <p>Screenshot of a notebook with query results.</p> <p>On the toolbar, use the \u25fb (Stop session) button to stop the Spark session. In the navigation bar, select Monitor to view the monitoring hub, and note that the notebook activity is listed.</p> <p>Screenshot of the monitoring hub with a notebook activity.</p> <p>Monitor history for an item Some items in a workspace might be run multiple times. You can use the monitoring hub to view their run history.</p> <p>In the navigation bar, return to the page for your workspace. Then use the \u21bb (Refresh now) button for your Get Product Data dataflow to re-run it. In the navigation pane, select the Monitor page to view the monitoring hub and verify that the dataflow is in-progress. In the \u2026 menu for the Get Product Data dataflow, select Historical runs to view the run history for the dataflow:</p> <p>Screenshot of the monitoring hub historical runs view.</p> <p>In the \u2026 menu for any of the historical runs select View detail to see details of the run. Close the Details pane and use the Back to main view button to return to the main monitoring hub page. Customize monitoring hub views In this exercise you\u2019ve only run a few activities, so it should be fairly easy to find events in the monitoring hub. However, in a real environment you may need to search through a large number of events. Using filters and other view customizations can make this easier.</p> <p>In the monitoring hub, use the Filter button to apply the following filter: Status: Succeeeded Item type: Dataflow Gen2 With the filter applied, only successful runs of dataflows are listed.</p> <p>Screenshot of the monitoring hub with a filter applied.</p> <p>Use the Column Options button to include the following columns in the view (use the Apply button to apply the changes): Activity name Status Item type Start time Submitted by Location End time Duration Refresh type You may need to scroll horizontally to see all of the columns:</p> <p>Screenshot of the monitoring hub with custom columns.</p> <p>Clean up resources In this exercise, you have created a lakehouse, a dataflow, and a Spark notebook; and you\u2019ve used the monitoring hub to view item activity.</p> <p>If you\u2019ve finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains. In the \u2026 menu on the toolbar, select Workspace settings. In the General section, select Remove this workspace.</p>"},{"location":"labs/18-monitor-hub/#httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs18-monitor-hubhtml","title":"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/18-monitor-hub.html","text":""},{"location":"labs/19-secure-data-access/","title":"Lab: Secure Data Access","text":"<p>Secure data access in Microsoft Fabric Microsoft Fabric has a multi-layer security model for managing data access. Security can be set for an entire workspace, for individual items, or through granular permissions in each Fabric engine. In this exercise, you secure data using workspace, and item access controls and OneLake data access roles.</p> <p>This lab takes approximately 45 minutes to complete.</p> <p>Create a workspace Before working with data in Fabric, create a workspace with the Fabric trial enabled.</p> <p>Navigate to the Microsoft Fabric home page at https://app.fabric.microsoft.com/home?experience=fabric in a browser and sign in with your Fabric credentials. In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7). Create a new workspace with a name of your choice, selecting a licensing mode that includes Fabric capacity (Trial, Premium, or Fabric). When your new workspace opens, it should be empty.</p> <p>Screenshot of an empty workspace in Fabric.</p> <p>Note: When you create a workspace, you automatically become a member of the Workspace Admin role.</p> <p>Create a data warehouse Next, create a data warehouse in the workspace you created:</p> <p>Click + New Item. On the New item page, under the Store Data section, select Sample warehouse and create a new data warehouse with a name of your choice.</p> <p>After a minute or so, a new warehouse will be created:</p> <p>Screenshot of a new warehouse.</p> <p>Create a lakehouse Next, create a lakehouse in the workspace you created.</p> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7). Select the workspace you created. In the workspace, select the + New Item button and then select Lakehouse. Create a new Lakehouse with the name of your choice.</p> <p>After a minute or so, a new Lakehouse will be created:</p> <p>Screenshot of a new lakehouse in Fabric.</p> <p>Select the Start with sample data tile and then select the Public holidays sample. After a minute or so, the lakehouse will be populated with data. Apply workspace access controls Workspace roles are used to control access to workspaces and the content within them. Workspace roles can be assigned when users need to see all items in a workspace, when they need to manage workspace access, or create new Fabric items, or when they need specific permissions to view, modify or share content in the workspace.</p> <p>In this exercise, you add a user to a workspace role, apply permissions and, see what is viewable when each set of permissions is applied. You open two browsers and sign-in as different users. In one browser, you\u2019ll be a Workspace Admin and in the other, you\u2019ll sign-in as a second, less privileged user. In one browser, the Workspace Admin changes permissions for the second user and in the second browser, you\u2019re able to see the effects of changing permissions.</p> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7). Next select the workspace you created. Select on Manage access on the top of the screen. Note: You\u2019ll see the user you\u2019re logged, who is a a member of the Workspace Admin role because you created the workspace. No other users are assigned access to the workspace yet.</p> <p>Next, you\u2019ll see what a user without permissions on the workspace can view. In your browser, open an InPrivate window. In the Microsoft Edge browser, select the ellipse at the top right corner and select New InPrivate Window. Enter https://app.fabric.microsoft.com and sign-in as the second user you\u2019re using for testing. On the bottom left corner of your screen, select Microsoft Fabric and then select Data Warehouse. Next select Workspaces (the icon looks similar to \ud83d\uddc7). Note: The second user doesn\u2019t have access to the workspace, so it\u2019s not viewable.</p> <p>Next, you assign the Workspace Viewer role to the second user and see that the role grants read access to the warehouse in the workspace. Return to the browser window where you\u2019re logged in as the Workspace Admin. Ensure you\u2019re still on the page that shows the workspace you created. It should have your new workspace items, and the sample warehouse and lakehouse, listed at the bottom of the page. Select Manage access at the top right of the screen. Select Add people or groups. Enter the email of the second user you\u2019re testing with. Select Add to assign the user to the workspace Viewer role. Return to the InPrivate browser window where you\u2019re logged in as the second user and select refresh button on the browser to refresh session permissions assigned to the second user. Select the Workspaces icon on the left menu bar (the icon looks similar to \ud83d\uddc7) and select on the workspace name you created as the Workspace Admin user. The second user can now see all of the items in the workspace because they were assigned the Workspace Viewer role.</p> <p>Screenshot of workspace items in Fabric.</p> <p>Select the warehouse and open it. Select the Date table and wait for the rows to be loaded. You can see the rows because as a member of the Workspace Viewer role, you have CONNECT and ReadData permission on tables in the warehouse. For more information on permissions granted to the Workspace Viewer role, see Workspace roles. Next, select the Workspaces icon on the left menu bar, then select the lakehouse. When the lakehouse opens, click on the dropdown box at the top right corner of the screen that says Lakehouse and select SQL analytics endpoint. Select the publicholidays table and wait for the data to be displayed. Data in the lakehouse table is readable from the SQL analytics endpoint because the user is a member of the Workspace Viewer role that grants read permissions on the SQL analytics endpoint. Apply item access control Item permissions control access to individual Fabric items within a workspace, like warehouses, lakehouses and semantic models. In this exercise, you remove the Workspace Viewer permissions applied in the previous exercise and then apply item level permissions on the warehouse so a less privileged user can only view the warehouse data, not the lakehouse data.</p> <p>Return to the browser window where you\u2019re logged in as the Workspace Admin. Select Workspaces from the left navigation pane. Select the workspace that you created to open it. Select Manage access from the top of the screen. Select the word Viewer under the name of the second user. On the menu that appears, select Remove.</p> <p>Screenshot of workspace access dropdown in Fabric.</p> <p>Close the Manage access section. In the workspace, hover over the name of your warehouse and an ellipse (\u2026) will appear. Select the ellipse and select Manage permissions</p> <p>Select Add user and enter the name of the second user. In the box that appears, under Additional permissions check Read all data using SQL (ReadData) and uncheck all other boxes.</p> <p>Screenshot of warehouse permissions being granted in Fabric.</p> <p>Select Grant</p> <p>Return to the browser window where you\u2019re logged in as the second user. Refresh the browser view.</p> <p>The second user no longer has access to the workspace and instead has access to only the warehouse. You can no longer browse workspaces on the left navigation pane to find the warehouse. Select OneLake on the left navigation menu to find the warehouse.</p> <p>Select the warehouse. On the screen that appears, select Open from the top menu bar.</p> <p>When the warehouse view appears, select the Date table to view table data. The rows are viewable because the user still has read access to the warehouse because ReadData permissions were applied by using item permissions on the warehouse. Apply OneLake data access roles in a Lakehouse OneLake data access roles let you create custom roles within a Lakehouse and grant read permissions to folders you specify. OneLake data access roles is currently a Preview feature.</p> <p>In this exercise, you assign an item permission and create a OneLake data access role and experiment with how they work together to restrict access to data in a Lakehouse.</p> <p>Stay in the browser where you\u2019re logged in as the second user. Select OneLake on the left navigation bar. The second user doesn\u2019t see the lakehouse. Return to the browser where you\u2019re logged in as the Workspace Admin. Select Workspaces on the left menu and select your workspace. Hover over the name of the lakehouse. Select on the ellipse (\u2026) to the right of the ellipse and select Manage permissions</p> <p>Screenshot of setting permissions on a lakehouse in Fabric.</p> <p>On the screen that appears, select Add user. Assign the second user to the lakehouse and ensure none of the checkboxes on the Grant People Access window are checked.</p> <p>Screenshot of the grant access lakehouse window in Fabric.</p> <p>Select Grant. The second user now has read permissions on the lakehouse. Read permission only allows the user to see metadata for the lakehouse but not the underlying data. Next we\u2019ll validate this. Return to the browser where you\u2019re logged in as the second user. Refresh the browser. Select OneLake in the left navigation pane. Select the lakehouse and open it. Select Open on the top menu bar. You\u2019re unable to expand the tables or files even though read permission was granted. Next, you grant the second user access to a specific folder using OneLake data access permissions. Return to the browser where you\u2019re logged in as the workspace administrator. Select Workspaces from the left navigation bar. Select your workspace name. Select the lakehouse. When the lakehouse opens, select Manage OneLake data access on the top menu bar and enable the feature by clicking the Continue button.</p> <p>Screenshot of the Manage OneLake data access (preview) feature on the menu bar in Fabric.</p> <p>Select new role on the Manage OneLake data access (preview) screen that appears.</p> <p>Screenshot of the new role functionality in the manage OneLake data access feature.</p> <p>Create a new role called publicholidays that can only access the publicholidays folder as shown in the screenshot below.</p> <p>Screenshot of the folder assignment in the manage OneLake data access feature.</p> <p>When the role finishes creating, select Assign role and assign the role to your second user, select Add and, select Save.</p> <p>Screenshot of the folder assignment in the manage OneLake data access feature.</p> <p>Return to the browser where you\u2019re logged in as the second user. Ensure you\u2019re still on the page where the lakehouse is open. Refresh the browser. Select the publicholidays table and wait for the data to load. Only the data in the publicholidays table is accessible to the user because the user was assigned to the custom OneLake data access role. The role permits them to see only the data in the publicholidays table, not data in any of the other tables, files, or folders. Clean up resources In this exercise, you secured data using workspace access controls, item access controls and, OneLake data access roles.</p> <p>In the left navigation bar, select the icon for your workspace to view all of the items it contains. In the menu on the top toolbar, select Workspace settings. In the General section, select Remove this workspace.</p>"},{"location":"labs/19-secure-data-access/#httpsdocsmicrosoftcomlearnmodulessecure-data-access-fabric","title":"https://docs.microsoft.com/learn/modules/secure-data-access-fabric/","text":""},{"location":"labs/19-secure-data-access/#httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs19-secure-data-accesshtml","title":"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/19-secure-data-access.html","text":""},{"location":"labs/21-deployment-pipelines/","title":"Lab: Implement Deployment Pipelines","text":"<p>Implement deployment pipelines in Microsoft Fabric Deployment pipelines in Microsoft Fabric let you automate the process of copying changes made to the content in Fabric items between environments like development, test, and production. You can use deployment pipelines to develop and test content before it reaches end users. In this exercise, you create a deployment pipeline, and assign stages to the pipeline. Then you create some content in a development workspace and use deployment pipelines to deploy it between the Development, Test and Production pipeline stages.</p> <p>Note: To complete this exercise, you need to be an member of the Fabric workspace admin role. To assign roles see Roles in workspaces in Microsoft Fabric.</p> <p>This lab takes approximately 20 minutes to complete.</p> <p>Create workspaces Create three workspaces with the Fabric trial enabled.</p> <p>Navigate to the Microsoft Fabric home page at https://app.fabric.microsoft.com/home?experience=fabric in a browser and sign in with your Fabric credentials. In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7). Create a new workspace named Development, selecting a licensing mode that includes Fabric capacity (Trial, Premium, or Fabric). Repeat steps 1 &amp; 2, creating two more workspaces named Test, and Production. Your workspaces are: Development, Test, and Production. Select the Workspaces icon on the menu bar on the left and confirm that there are three workspaces named: Development, Test, and Production Note: If you are prompted to enter a unique name for the workspaces, append one or more random numbers to the words: Development, Test, or Production.</p> <p>Create a deployment pipeline Next, create a deployment pipeline.</p> <p>In the menu bar on the left, select Workspaces. Select Deployment Pipelines, then New pipeline. In the Add a new deployment pipeline window, give the pipeline a unique name and select Next. In the new pipeline window, select Create and continue. Assign workspaces to stages of a deployment pipeline Assign workspaces to the stages of the deployment pipeline.</p> <p>On the left menu bar, select the pipeline you created. In the window that appears, expand the options under Assign a workspace on each deployment stage and select the name of the workspace that matches the name of the stage. Select the check mark Assign for each deployment stage. Screenshot of deployment pipeline.</p> <p>Create content Fabric items haven\u2019t been created in your workspaces yet. Next, create a lakehouse in the development workspace.</p> <p>In the menu bar on the left, select Workspaces. Select the Development workspace. Select New Item. In the window that appears, select Lakehouse and in the New lakehouse window, name the lakehouse, LabLakehouse. Select Create. In the Lakehouse Explorer window, select Start with sample data to populate the new lakehouse with data. Screenshot of Lakehouse Explorer.</p> <p>Select the sample NYCTaxi. In the menu bar on the left, select the pipeline you created. Select the Development stage, and under the deployment pipeline canvas you can see the lakehouse you created as a stage item. In the left edge of the Test stage, there\u2019s an X within a circle. The X indicates that the Development and Test stages aren\u2019t synchronized. Select the Test stage and under the deployment pipeline canvas you can see that the lakehouse you created is only a stage item in the source, which in this case refers to the Development stage. Screenshot the deployment pipeline showing content mismatches between stages.</p> <p>Deploy content between stages Deploy the lakehouse from the Development stage to the Test and Production stages.</p> <p>Select the Test stage in the deployment pipeline canvas. Under the deployment pipeline canvas, select the checkbox next to the Lakehouse item. Then select the Deploy button to copy the lakehouse in its current state to the Test stage. In the Deploy to next stage window that appears, select Deploy. There is now an X in a circle in the Production stage in the deployment pipeline canvas. The lakehouse exists in the Development and Test stages but not yet in the Production stage. Select the Production stage in the deployment canvas. Under the deployment pipeline canvas, select the checkbox next to the Lakehouse item. Then select the Deploy button to copy the lakehouse in its current state to the Production stage. In the Deploy to next stage window that appears, select Deploy. The green check marks between the stages indicates that all stages in sync and contain the same content. Using deployment pipelines to deploy between stages also updates the content in the workspaces corresponding to the deployment stage. Let\u2019s confirm. In the menu bar on the left, select Workspaces. Select the Test workspace. The lakehouse was copied there. Open the Production workspace from the Workspaces icon on the left menu. The lakehouse was copied to the Production workspace too. Clean up In this exercise, you created a deployment pipeline, and assigned stages to the pipeline. Then you created content in a development workspace and deployed it between pipeline stages using deployment pipelines.</p> <p>In the left navigation bar, select the icon for each workspace to view all of the items it contains. In the menu on the top toolbar, select Workspace settings. In the General section, select Remove this workspace.</p>"},{"location":"labs/21-deployment-pipelines/#httpsmicrosoftlearninggithubiomslearn-fabricinstructionslabs21-implement-cicdhtml","title":"https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/21-implement-cicd.html","text":""},{"location":"tne/day1-orig/","title":"Day 1: Operational Monitoring &amp; Performance","text":""},{"location":"tne/day1-orig/#930-950-vm-setup-welcome-20-mins","title":"9:30-9:50 ~ VM Setup &amp; Welcome (20 mins)","text":"<ul> <li>VM allocation</li> <li>Day overview and learning objectives</li> </ul>"},{"location":"tne/day1-orig/#950-1040-foundation-purpose-50-mins","title":"9:50-10:40 ~ Foundation &amp; Purpose (50 mins)","text":"<ul> <li>Discussion: \"When did a system failure impact you personally?\"</li> <li>Acquisition: Why monitor? From personal pain to business impact</li> <li>Investigation: Status page analysis: \"What do companies communicate when things break?\"</li> </ul>"},{"location":"tne/day1-orig/#1040-1100-break-20-mins","title":"10:40-11:00 ~ Break (20 mins)","text":""},{"location":"tne/day1-orig/#1100-1215-fabric-introduction-first-labs-75-mins","title":"11:00-12:15 ~ Fabric Introduction &amp; First Labs (75 mins)","text":"<ul> <li>Introduction to Microsoft Fabric</li> <li>Practice: Lab 5 (Dataflows Gen2) - 30 mins - Fabric intro</li> <li>Practice: Lab 4 (Pipeline) - 30 mins - baseline for monitoring</li> <li>Discussion: \"What could go wrong with what we just built?\"</li> </ul>"},{"location":"tne/day1-orig/#1215-1315-lunch-60-mins","title":"12:15-13:15 ~ Lunch (60 mins)","text":""},{"location":"tne/day1-orig/#1315-1430-monitoring-in-practice-75-mins","title":"13:15-14:30 ~ Monitoring in Practice (75 mins)","text":"<ul> <li>Practice: Lab 18 (Monitor hub) - 25 mins</li> <li>Practice: Lab 6c (Monitor warehouse) - 25 mins</li> <li>Discussion: \"What surprised you about the monitoring tools?\"</li> </ul>"},{"location":"tne/day1-orig/#1430-1450-break-20-mins","title":"14:30-14:50 ~ Break (20 mins)","text":""},{"location":"tne/day1-orig/#1450-1550-investigation-production-60-mins","title":"14:50-15:50 ~ Investigation &amp; Production (60 mins)","text":"<ul> <li>Investigation: Cloud monitoring comparison (AWS/GCP/Azure) - 25 mins</li> <li>Production: Pairs create \"monitoring strategy\" - 30 mins</li> <li>Gallery walk: Share monitoring strategies - 5 mins</li> </ul>"},{"location":"tne/day1-orig/#1550-1600-collaboration-wrap-up-10-mins","title":"15:50-16:00 ~ Collaboration &amp; Wrap-up (10 mins)","text":"<ul> <li>Day reflection and tomorrow's preview</li> </ul>"},{"location":"tne/day1/","title":"Day 1: Day 1 - Monitoring &amp; Performance","text":""},{"location":"tne/day1/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> VM Setup &amp; Welcome (20 mins)</li> <li><code>09:50</code> Discussion: System failure impact (20 mins)</li> <li><code>10:10</code> Why monitor? (10 mins)</li> <li><code>10:20</code> Investigation: Status page analysis (20 mins)</li> </ul>"},{"location":"tne/day1/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"tne/day1/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Introduction to Microsoft Fabric (10 mins)</li> <li><code>11:10</code> Practice: Lab 1.1 ~ 05 Dataflows Gen2 (30 mins)</li> <li><code>11:40</code> Practice: Lab 1.2 ~ 04 Ingest Pipeline (30 mins)</li> <li><code>12:10</code> Discussion: What could go wrong with what we just built? (10 mins)</li> </ul>"},{"location":"tne/day1/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"tne/day1/#session-3","title":"Session 3","text":"<ul> <li><code>13:20</code> Practice: Lab 1.3 ~ 18 Monitor Hub (30 mins)</li> <li><code>13:50</code> Practice: Lab 1.4 ~ 06c Monitor Warehouse (30 mins)</li> <li><code>14:20</code> Discussion: What surprised you about monitoring tools? (10 mins)</li> </ul>"},{"location":"tne/day1/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"tne/day1/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> Investigation: Cloud monitoring comparison (30 mins)</li> <li><code>15:20</code> Activity: Create monitoring strategy (20 mins)</li> <li><code>15:40</code> Report-Back: Share monitoring strategies (10 mins)</li> </ul>"},{"location":"tne/day1/#wrap-up","title":"Wrap up","text":""},{"location":"tne/day2-orig/","title":"Day 2:","text":""},{"location":"tne/day2-orig/#930-950-welcome-context-setting-20-mins","title":"9:30-9:50 ~ Welcome &amp; Context Setting (20 mins)","text":"<ul> <li>Discussion: \"Share a time when something broke at work - what happened?\" </li> </ul>"},{"location":"tne/day2-orig/#950-1030-breaking-things-40-mins","title":"9:50-10:30 ~ Breaking Things (40 mins)","text":"<ul> <li>Practice: Re-run Lab 4 (working version) - 15 mins</li> <li>Practice: Break it systematically - 25 mins</li> </ul>"},{"location":"tne/day2-orig/#1030-1050-break-20-mins","title":"10:30-10:50 ~ Break (20 mins)","text":""},{"location":"tne/day2-orig/#1050-1215-investigation-discussion-85-mins","title":"10:50-12:15 ~  Investigation + Discussion (85 mins)","text":"<ul> <li>Practice: Lab 11 (Data Activator) - 35 mins - set up alerts for failures</li> <li>Discussion: Share findings - 10 mins </li> <li>Practice: More complex breaks - 30 mins </li> <li>Discussion: Share findings - 10 mins </li> </ul>"},{"location":"tne/day2-orig/#1215-1315-lunch-60-mins","title":"12:15-13:15 ~ Lunch (60 mins)","text":""},{"location":"tne/day2-orig/#1315-1430-incident-response-75-mins","title":"13:15-14:30 ~ Incident Response (75 mins)","text":"<ul> <li>Fire Drill Preparation - 25 mins</li> <li>Scenario 1 Briefing - 5 mins</li> <li>Fire Drill Round 1 - 25 mins</li> <li>Discussion: Debrief - 20 mins</li> </ul>"},{"location":"tne/day2-orig/#1430-1450-break-20-mins","title":"14:30-14:50 ~ Break (20 mins)","text":""},{"location":"tne/day2-orig/#1450-1550-session-4-60-mins","title":"14:50-15:50 ~ Session 4 (60 mins)","text":"<ul> <li>Fire Drill Round 2 - 25 mins</li> <li>Discussion: Fire Drill Debrief - 15 mins</li> <li>Discussion: Workplace Connections - 20 mins</li> </ul>"},{"location":"tne/day2-orig/#1550-1600-wrap-up-10-mins","title":"15:50-16:00 ~ Wrap-up (10 mins)","text":"<ul> <li>Day reflection and tomorrow's preview</li> </ul>"},{"location":"tne/day2-orig/#parked","title":"Parked:","text":"<ul> <li>Investigation: Major outage case studies - 20 mins </li> <li>Discussion: Share findings - 15 mins</li> <li>Workplace Procedures</li> <li>Checklist Creation</li> </ul>"},{"location":"tne/day2/","title":"Day 2: Day 2 - Incident Response","text":""},{"location":"tne/day2/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome (10 mins)</li> <li><code>09:40</code> Discussion: When a system broke at work (10 mins)</li> <li><code>09:50</code> Practice: Re-run Day 1 Lab: 04 Ingest Pipeline (10 mins)</li> <li><code>10:00</code> Practice: Lab 2.1 ~ Break it systematically (20 mins)</li> <li><code>10:20</code> Incident vs Problem - whats the difference? (10 mins)</li> </ul>"},{"location":"tne/day2/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"tne/day2/#session-2","title":"Session 2","text":"<ul> <li><code>10:50</code> Practice: More complex breaks (30 mins)</li> <li><code>11:20</code> Discussion: Share findings (10 mins)</li> <li><code>11:30</code> Demo: Lab ~ 11 Data Activator (10 mins)</li> <li><code>11:40</code> Discussion: AI in Data Engineering (30 mins)</li> </ul>"},{"location":"tne/day2/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"tne/day2/#session-3","title":"Session 3","text":"<ul> <li><code>13:10</code> Investigation: Fire Drill preparation (20 mins)</li> <li><code>13:30</code> Scenario 1 briefing (10 mins)</li> <li><code>13:40</code> Fire Drill Round 1 (30 mins)</li> <li><code>14:10</code> Discussion: Fire drill debrief (20 mins)</li> </ul>"},{"location":"tne/day2/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"tne/day2/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> Fire Drill Round 2 (30 mins)</li> <li><code>15:20</code> Discussion: Fire drill debrief (10 mins)</li> <li><code>15:30</code> Discussion: Workplace connections (20 mins)</li> </ul>"},{"location":"tne/day2/#wrap-up","title":"Wrap up","text":""},{"location":"tne/day3-orig/","title":"Day 3: Operational Quality &amp; Governance","text":"<p>9:30-9:50 ~ Welcome &amp; Quality Foundation (20 mins) - Discussion: \"What does 'good data' mean to you?\" - Acquisition: Introduction to Data Quality - why it matters operationally</p> <p>9:50-10:40 ~ DMBOK Quality Investigation (50 mins) - Investigation: DMBOK's 6 data quality dimensions - 35 mins - Discussion: Report back findings - 15 mins</p> <p>10:40-11:00 ~ Break (20 mins)</p> <p>11:00-12:15 ~ Medallion Architecture &amp; Quality (75 mins) - Practice: Lab 3b (Medallion architecture) - 45 mins - Discussion: Quality Patterns Analysis - 20 mins - Investigation: Quality Tools Investigation - 10 mins</p> <p>12:15-13:15 ~ Lunch (60 mins)</p> <p>13:15-14:30: Governance &amp; Security (75 mins) - Practice: Lab 19 (Secure data access) - 45 mins - Discussion: Security vs Accessibility trade-offs - 20 mins - Investigation: Governance Approaches Investigation - 10 mins</p> <p>14:30-14:50 ~ Break (20 mins)</p> <p>14:50-15:50 ~ Deployment Pipelines &amp; Governance Integration (60 mins) - Practice: Lab 21 Deployment Pipelines (20 minutes) - Discussion: Deployment Governance Discussion (20 minutes) - Investigation: Governance Integration Workshop (20 minutes)</p> <p>15:50-16:00 ~ Wrap-up (10 mins) - Day reflection and tomorrow's preview</p> <p>Parked: - Investigation: Their workplace governance policies - 25 mins - Production: Design data governance framework (pairs) - 30 mins - 14:50-15:45: Real-World Governance (55 mins)</p>"},{"location":"tne/day3/","title":"Day 3: Day 3 - Quality &amp; Governance","text":""},{"location":"tne/day3/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Discussion: What does good data mean to you? (10 mins)</li> <li><code>09:40</code> Data Quality - why it matters operationally (10 mins)</li> <li><code>09:50</code> Investigation: DMBOKs 6 data quality dimensions (40 mins)</li> <li><code>10:30</code> Report-Back: DMBOK report back (10 mins)</li> </ul>"},{"location":"tne/day3/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"tne/day3/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Practice: Lab 3.1 ~ 03b Medallion Architecture (40 mins)</li> <li><code>11:40</code> Discussion: Quality patterns analysis (20 mins)</li> <li><code>12:00</code> Investigation: Quality tools (20 mins)</li> </ul>"},{"location":"tne/day3/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"tne/day3/#session-3","title":"Session 3","text":"<ul> <li><code>13:30</code> Practice: Lab 3.2 ~ 19 Secure Data Access (40 mins)</li> <li><code>14:10</code> Discussion: Security vs Accessibility trade-offs (10 mins)</li> <li><code>14:20</code> Investigation: Governance Approaches Investigation (10 mins)</li> </ul>"},{"location":"tne/day3/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"tne/day3/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> Practice: Lab 3.3 ~ 21 Deployment Pipelines (20 mins)</li> <li><code>15:10</code> Discussion: Deployment governance (20 mins)</li> <li><code>15:30</code> Workshop: Governance integration (20 mins)</li> </ul>"},{"location":"tne/day3/#wrap-up","title":"Wrap up","text":""},{"location":"tne/day4-orig/","title":"Day 4 with Joint Standups:","text":""},{"location":"tne/day4-orig/#930950-welcome-product-backlog-introduction-20-mins","title":"9:30\u20139:50 \u2014 Welcome &amp; Product Backlog Introduction (20 mins)","text":"<ul> <li>Challenge scenario + Product Backlog Review</li> <li>Team formation</li> </ul>"},{"location":"tne/day4-orig/#9501030-sprint-1-40-mins","title":"9:50\u201310:30 \u2014 Sprint 1 (40 mins)","text":"<ul> <li>Sprint Planning (10 mins): Select items, commit to sprint goal</li> <li>Sprint Work (30 mins): Development work</li> </ul>"},{"location":"tne/day4-orig/#10301045-break-15-mins","title":"10:30\u201310:45 \u2014 Break (15 mins)","text":""},{"location":"tne/day4-orig/#10451100-joint-standup-break-15-mins","title":"10:45\u201311:00 \u2014 Joint Standup &amp; Break (15 mins)","text":"<ul> <li>Joint Standup Each person shares - What did I complete? What am I working on next? Any blockers?</li> <li>Learning opportunity: Teams hear what others accomplished, can offer help</li> </ul>"},{"location":"tne/day4-orig/#11001215-sprint-2-75-mins","title":"11:00\u201312:15 \u2014 Sprint 2 (75 mins)","text":"<ul> <li>Sprint Planning (10 mins): Plan second sprint</li> <li>Sprint Work (65 mins): Development work</li> </ul>"},{"location":"tne/day4-orig/#12151315-lunch-60-mins","title":"12:15\u201313:15 \u2014 Lunch (60 mins)","text":""},{"location":"tne/day4-orig/#13151330-joint-standup-15-mins","title":"13:15\u201313:30 \u2014 Joint Standup (15 mins)","text":"<ul> <li>Same format - keeps momentum, shows progress across teams</li> </ul>"},{"location":"tne/day4-orig/#13301430-sprint-3-60-mins","title":"13:30\u201314:30 \u2014 Sprint 3 (60 mins)","text":"<ul> <li>Sprint Planning (5 mins): Quick final sprint plan</li> <li>Sprint Work (55 mins): Final development</li> </ul>"},{"location":"tne/day4-orig/#14301450-break-20-mins","title":"14:30\u201314:50 \u2014 Break (20 mins)","text":""},{"location":"tne/day4-orig/#14501550-demo-retrospective-60-mins","title":"14:50\u201315:50 \u2014 Demo &amp; Retrospective (60 mins)","text":"<ul> <li>Team Demos (40 mins)</li> <li>Retrospective (20 mins)</li> </ul>"},{"location":"tne/day4/","title":"Day 4: Day 4 - Improvement &amp; Value","text":""},{"location":"tne/day4/#session-1","title":"Session 1","text":""},{"location":"tne/day4/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"tne/day4/#session-2","title":"Session 2","text":""},{"location":"tne/day4/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"tne/day4/#session-3","title":"Session 3","text":""},{"location":"tne/day4/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"tne/day4/#session-4","title":"Session 4","text":""},{"location":"tne/day4/#wrap-up","title":"Wrap up","text":""},{"location":"tne/day5/","title":"Day5","text":"<p>No content found for day DE5M6D5 Make sure you've specified the --day-code parameter correctly.</p>"}]}