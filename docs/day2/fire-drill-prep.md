# Day 2: Fire Drill Preparation

---

## What's Coming Next
In **20 minutes**, you'll participate in **incident response fire drills** simulating real data platform failures. You'll work in teams to coordinate technical responses and stakeholder communication under time pressure.

## Your Preparation Tasks

### 1. Form Your Incident Response Team
**Team Size:** 3-4 people depending on class size

**Assign Roles:**

- **Incident Commander:** Coordinates overall response, makes key decisions
- **Technical Lead:** Investigates root cause, determines fix options  
- **Communications Lead:** Handles stakeholder updates and messaging
- **Business Impact Analyst:** Assesses downstream effects *(if 4 people)*

### 2. Real Incident Examples
Scan these major incidents to understand what works (and what doesn't) in incident response:

**Cloudflare Global DNS Outage - June 21, 2022**

- *27-minute global DNS failure affecting millions of websites*
- **Link:** https://blog.cloudflare.com/cloudflare-outage-on-june-21-2022/
- **Key lesson:** Speed vs. accuracy in technical communication

**AWS us-east-1 Outage - December 7, 2021**  

- *5+ hour power-related outage affecting thousands of services*
- **Search:** "AWS us-east-1 outage December 2021 post-mortem"
- **Key lesson:** Cascading failures and dependency management

**GitHub Database Incident - October 21, 2018**

- *24+ hour database cluster failure during maintenance*
- **Search:** "GitHub October 2018 incident post-mortem database"
- **Key lesson:** Incident escalation and technical decision-making

**Meta/Facebook Global Outage - October 4, 2021**

- *6+ hour BGP configuration error taking down all Meta services*
- **Search:** "Facebook outage October 2021 BGP routing"
- **Key lesson:** When monitoring systems fail too

### 3. Extract Key Principles
*Focus your research on these questions*

**Detection & Assessment:**

- How quickly did they recognize the problem?
- What information did they gather before acting?
- How did they assess business impact?

**Technical Response:**

- Did they implement quick fixes or wait for proper solutions?
- How did they coordinate multiple teams?
- What tools and processes helped/hindered them?

**Communication Strategy:**

- When did they first communicate publicly?
- How detailed were their updates?
- How did they handle uncertainty in their messaging?
- What different messages did they send to different audiences?

**Coordination & Decision-Making:**

- Who made key decisions?
- How did they balance speed vs. accuracy?
- What escalation triggers did they use?

---

## Research Strategy Tips

**Don't try to read everything!** Focus on:

- **Executive summaries** and key timeline points
- **Communication examples** - actual status updates they published
- **Lessons learned** sections in post-mortems
- **Decision points** - when they chose one approach over another

**Look for patterns:**

- What's common across different incidents?
- What approaches consistently work or fail?
- How do companies handle uncertainty in their communication?

### Team Coordination
- **Share your findings** with your team members
- **Agree on communication approach** - who will handle what during the drill
- **Establish team coordination method** - how will you make quick decisions together?

!!! note "The goal isn't deep expertise - it's rapid preparation for practical application!"

---
